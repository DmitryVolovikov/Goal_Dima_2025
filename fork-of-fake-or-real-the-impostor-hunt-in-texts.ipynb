{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":105874,"databundleVersionId":12964783,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:11:17.922077Z","iopub.execute_input":"2025-07-28T23:11:17.922356Z","iopub.status.idle":"2025-07-28T23:11:24.267553Z","shell.execute_reply.started":"2025-07-28T23:11:17.922333Z","shell.execute_reply":"2025-07-28T23:11:24.266912Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Сиды","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\n\ntry:\n    # Hugging Face convenience fn; sets Python/Rand, NumPy, Torch seeds\n    from transformers import set_seed  \nexcept ImportError:\n    set_seed = None\n\ndef seed_everything(seed: int = 42):\n    # 1. Python built‑ins\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n\n    # 2. NumPy\n    np.random.seed(seed)\n\n    # 3. PyTorch (CPU & GPU)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    # 4. CuDNN: make deterministic, but may slow you down\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # 5. Transformers (if installed)\n    if set_seed is not None:\n        set_seed(seed)\n\n# call it!\nseed_everything(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:47:30.790655Z","iopub.execute_input":"2025-07-29T10:47:30.791073Z","iopub.status.idle":"2025-07-29T10:47:52.104944Z","shell.execute_reply.started":"2025-07-29T10:47:30.791041Z","shell.execute_reply":"2025-07-29T10:47:52.104171Z"}},"outputs":[{"name":"stderr","text":"2025-07-29 10:47:38.968224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753786059.201253      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753786059.270242      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Импорты","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:47:52.106016Z","iopub.execute_input":"2025-07-29T10:47:52.106504Z","iopub.status.idle":"2025-07-29T10:48:01.624817Z","shell.execute_reply.started":"2025-07-29T10:47:52.106486Z","shell.execute_reply":"2025-07-29T10:48:01.624126Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\nfrom torch.utils.data import Dataset\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:48:01.625669Z","iopub.execute_input":"2025-07-29T10:48:01.626289Z","iopub.status.idle":"2025-07-29T10:48:01.630266Z","shell.execute_reply.started":"2025-07-29T10:48:01.626261Z","shell.execute_reply":"2025-07-29T10:48:01.629636Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Загрузка данных","metadata":{}},{"cell_type":"code","source":"def read_texts_from_dir(dir_path):\n  \"\"\"\n  Reads the texts from a given directory and saves them in the pd.DataFrame with columns ['id', 'file_1', 'file_2'].\n\n  Params:\n    dir_path (str): path to the directory with data\n  \"\"\"\n  # Count number of directories in the provided path\n  dir_count = sum(os.path.isdir(os.path.join(root, d)) for root, dirs, _ in os.walk(dir_path) for d in dirs)\n  data=[0 for _ in range(dir_count)]\n  print(f\"Number of directories: {dir_count}\")\n\n  # For each directory, read both file_1.txt and file_2.txt and save results to the list\n  i=0\n  for folder_name in sorted(os.listdir(dir_path)):\n    folder_path = os.path.join(dir_path, folder_name)\n    if os.path.isdir(folder_path):\n      try:\n        with open(os.path.join(folder_path, 'file_1.txt'), 'r', encoding='utf-8') as f1:\n          text1 = f1.read().strip()\n        with open(os.path.join(folder_path, 'file_2.txt'), 'r', encoding='utf-8') as f2:\n          text2 = f2.read().strip()\n        index = int(folder_name[-4:])\n        data[i]=(index, text1, text2)\n        i+=1\n      except Exception as e:\n        print(f\"Error reading directory {folder_name}: {e}\")\n\n  # Change list with results into pandas DataFrame\n  df = pd.DataFrame(data, columns=['id', 'file_1', 'file_2']).set_index('id')\n  return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:48:01.631522Z","iopub.execute_input":"2025-07-29T10:48:01.631760Z","iopub.status.idle":"2025-07-29T10:48:01.693386Z","shell.execute_reply.started":"2025-07-29T10:48:01.631738Z","shell.execute_reply":"2025-07-29T10:48:01.692616Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_path=\"/kaggle/input/fake-or-real-the-impostor-hunt/data/train\"\ntrain=read_texts_from_dir(train_path)\ntest_path=\"/kaggle/input/fake-or-real-the-impostor-hunt/data/test\"\ntest=read_texts_from_dir(test_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:48:01.694210Z","iopub.execute_input":"2025-07-29T10:48:01.694471Z","iopub.status.idle":"2025-07-29T10:48:12.642443Z","shell.execute_reply.started":"2025-07-29T10:48:01.694447Z","shell.execute_reply":"2025-07-29T10:48:12.641671Z"}},"outputs":[{"name":"stdout","text":"Number of directories: 95\nNumber of directories: 1068\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_labels=pd.read_csv('/kaggle/input/fake-or-real-the-impostor-hunt/data/train.csv')\n#train_repository='/kaggle/input/fake-or-real-the-impostor-hunt/data/train/'\n#test_repository='/kaggle/input/fake-or-real-the-impostor-hunt/data/test/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:21:36.384639Z","iopub.execute_input":"2025-07-23T17:21:36.384993Z","iopub.status.idle":"2025-07-23T17:21:36.398065Z","shell.execute_reply.started":"2025-07-23T17:21:36.384962Z","shell.execute_reply":"2025-07-23T17:21:36.397152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_labels = pd.read_csv('/kaggle/input/fake-or-real-the-impostor-hunt/data/train.csv')\ntrain = (\n    train.reset_index()                 # bring 'id' back as a column\n         .merge(train_labels, on='id', how='inner')\n         .set_index('id')\n)\n\n# sanity check\nassert train['real_text_id'].notna().all(), \"Labels have NaNs after merge.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:48:12.643266Z","iopub.execute_input":"2025-07-29T10:48:12.643473Z","iopub.status.idle":"2025-07-29T10:48:12.663554Z","shell.execute_reply.started":"2025-07-29T10:48:12.643455Z","shell.execute_reply":"2025-07-29T10:48:12.662819Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train = train.join(train_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:21:36.398941Z","iopub.execute_input":"2025-07-23T17:21:36.399277Z","iopub.status.idle":"2025-07-23T17:21:36.410459Z","shell.execute_reply.started":"2025-07-23T17:21:36.399247Z","shell.execute_reply":"2025-07-23T17:21:36.409755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Чистка текста","metadata":{}},{"cell_type":"code","source":"import re ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:21:36.411068Z","iopub.execute_input":"2025-07-23T17:21:36.411295Z","iopub.status.idle":"2025-07-23T17:21:36.428092Z","shell.execute_reply.started":"2025-07-23T17:21:36.411279Z","shell.execute_reply":"2025-07-23T17:21:36.427446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def clean():\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:21:36.430715Z","iopub.execute_input":"2025-07-23T17:21:36.430921Z","iopub.status.idle":"2025-07-23T17:21:36.44543Z","shell.execute_reply.started":"2025-07-23T17:21:36.430906Z","shell.execute_reply":"2025-07-23T17:21:36.444686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_size = 0.1\nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train, test_size=test_size, stratify=train['real_text_id'], random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:48:12.664244Z","iopub.execute_input":"2025-07-29T10:48:12.664488Z","iopub.status.idle":"2025-07-29T10:48:12.688712Z","shell.execute_reply.started":"2025-07-29T10:48:12.664465Z","shell.execute_reply":"2025-07-29T10:48:12.687812Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Датасет","metadata":{}},{"cell_type":"code","source":"#class ImposterDataset(Dataset):\n#    def __init__(self, ):\n#        \n#    def __len__(self):\n#        \n#    def __getitem__(self, idx):\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:21:36.481306Z","iopub.execute_input":"2025-07-23T17:21:36.48154Z","iopub.status.idle":"2025-07-23T17:21:36.485082Z","shell.execute_reply.started":"2025-07-23T17:21:36.481519Z","shell.execute_reply":"2025-07-23T17:21:36.484433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImposterDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=512):\n        self.texts1 = df['file_1'].tolist()\n        self.texts2 = df['file_2'].tolist()\n        self.labels = df['real_text_id'].tolist() if 'real_text_id' in df.columns else None\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts1)\n\n    def __getitem__(self, idx):\n        # Prepare texts\n        text1 = self.texts1[idx]\n        text2 = self.texts2[idx]\n        encoding = self.tokenizer(\n            text1,\n            text2,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        item = {k: v.squeeze(0) for k, v in encoding.items()}\n        if self.labels is not None:\n            lbl_orig = self.labels[idx]\n            # lbl_orig in {1,2} -> shift to 0/1 for model\n            assert lbl_orig in [1,2], f'Unexpected label {lbl_orig}, expected 1 or 2'\n            lbl = lbl_orig - 1\n            #item['labels'] = torch.tensor(lbl, dtype=torch.float)\n            item['labels'] = torch.tensor([lbl], dtype=torch.float)\n        return item\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T09:51:55.730072Z","iopub.execute_input":"2025-07-28T09:51:55.730612Z","iopub.status.idle":"2025-07-28T09:51:55.737239Z","shell.execute_reply.started":"2025-07-28T09:51:55.730587Z","shell.execute_reply":"2025-07-28T09:51:55.736419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Модель","metadata":{}},{"cell_type":"markdown","source":"candidate_models = [\n    'intfloat/e5-small-v2',\n    'distilbert-base-uncased',\n    'roberta-base',\n    'sentence-transformers/all-MiniLM-L6-v2'\n]","metadata":{}},{"cell_type":"code","source":"model_name = 'cross-encoder/ms-marco-MiniLM-L-6-v2'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T09:51:59.863016Z","iopub.execute_input":"2025-07-28T09:51:59.86328Z","iopub.status.idle":"2025-07-28T09:51:59.867079Z","shell.execute_reply.started":"2025-07-28T09:51:59.863262Z","shell.execute_reply":"2025-07-28T09:51:59.866144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    model_name # разрешаем пересоздать слой\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T09:52:01.796297Z","iopub.execute_input":"2025-07-28T09:52:01.796997Z","iopub.status.idle":"2025-07-28T09:52:03.706101Z","shell.execute_reply.started":"2025-07-28T09:52:01.796974Z","shell.execute_reply":"2025-07-28T09:52:03.705545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model=AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:21:38.908529Z","iopub.execute_input":"2025-07-23T17:21:38.908772Z","iopub.status.idle":"2025-07-23T17:21:38.91237Z","shell.execute_reply.started":"2025-07-23T17:21:38.90875Z","shell.execute_reply":"2025-07-23T17:21:38.911569Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Токенайзер","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T09:52:06.525098Z","iopub.execute_input":"2025-07-28T09:52:06.525387Z","iopub.status.idle":"2025-07-28T09:52:07.540443Z","shell.execute_reply.started":"2025-07-28T09:52:06.525365Z","shell.execute_reply":"2025-07-28T09:52:07.539681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = ImposterDataset(train_df, tokenizer)\nval_dataset   = ImposterDataset(val_df,   tokenizer)\ntest_dataset  = ImposterDataset(test.reset_index(drop=True), tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T09:52:09.82712Z","iopub.execute_input":"2025-07-28T09:52:09.827805Z","iopub.status.idle":"2025-07-28T09:52:09.83347Z","shell.execute_reply.started":"2025-07-28T09:52:09.827773Z","shell.execute_reply":"2025-07-28T09:52:09.832727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Метрика","metadata":{}},{"cell_type":"code","source":"'''def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    logits = logits.squeeze()  # (N,)\n    probs = 1 / (1 + np.exp(-logits))\n    preds = (probs > 0.5).astype(int)\n    from sklearn.metrics import accuracy_score, f1_score\n    return {'accuracy': accuracy_score(labels, preds),\n            'f1': f1_score(labels, preds)}'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:21:40.408169Z","iopub.execute_input":"2025-07-23T17:21:40.408489Z","iopub.status.idle":"2025-07-23T17:21:40.427285Z","shell.execute_reply.started":"2025-07-23T17:21:40.408469Z","shell.execute_reply":"2025-07-23T17:21:40.426729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    # preds shape: (N, 1) or (N,)\n    logits = preds.squeeze()\n    probs = 1 / (1 + np.exp(-logits))\n    y_pred = (probs > 0.5).astype(int)\n    y_true = labels.astype(int)\n    from sklearn.metrics import accuracy_score, f1_score\n    return {'accuracy': accuracy_score(y_true, y_pred),\n            'f1': f1_score(y_true, y_pred)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T10:12:25.262064Z","iopub.execute_input":"2025-07-28T10:12:25.262276Z","iopub.status.idle":"2025-07-28T10:12:25.273086Z","shell.execute_reply.started":"2025-07-28T10:12:25.26226Z","shell.execute_reply":"2025-07-28T10:12:25.272331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Обучение","metadata":{}},{"cell_type":"markdown","source":"## Training arguments","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"args=TrainingArguments(\n    output_dir='gol',\n    num_train_epochs=10,\n    learning_rate=3e-5,\n    report_to='none',\n    lr_scheduler_type='linear',\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    \n    eval_strategy='steps',\n    eval_steps=10,\n    warmup_ratio=0.1015,\n    save_strategy='epoch',\n    greater_is_better=True,\n    \n\n    \n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:21:40.427929Z","iopub.execute_input":"2025-07-23T17:21:40.428128Z","iopub.status.idle":"2025-07-23T17:21:40.469804Z","shell.execute_reply.started":"2025-07-23T17:21:40.428113Z","shell.execute_reply":"2025-07-23T17:21:40.469051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir='gol',\n    num_train_epochs=10,\n    #learning_rate=5e-6,\n    learning_rate=3e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    eval_strategy='steps',\n    eval_steps=20,\n    logging_dir='logs',\n    logging_steps=20,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n    greater_is_better=True,\n    report_to='none',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T09:52:16.594771Z","iopub.execute_input":"2025-07-28T09:52:16.595042Z","iopub.status.idle":"2025-07-28T09:52:16.625067Z","shell.execute_reply.started":"2025-07-28T09:52:16.595021Z","shell.execute_reply":"2025-07-28T09:52:16.624268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Trainer","metadata":{}},{"cell_type":"code","source":"trainer=Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    args=args,\n\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T09:52:19.262345Z","iopub.execute_input":"2025-07-28T09:52:19.262989Z","iopub.status.idle":"2025-07-28T09:52:19.459119Z","shell.execute_reply.started":"2025-07-28T09:52:19.262956Z","shell.execute_reply":"2025-07-28T09:52:19.458313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T09:52:21.721604Z","iopub.execute_input":"2025-07-28T09:52:21.722178Z","iopub.status.idle":"2025-07-28T09:52:42.418783Z","shell.execute_reply.started":"2025-07-28T09:52:21.722146Z","shell.execute_reply":"2025-07-28T09:52:42.418146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================\n# Pairwise Ranking: setup\n# ============================\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_NAME = \"microsoft/deberta-v3-base\"   # good default; switch to -large if VRAM allows\nMAX_LEN = 384                               # tweak (384–512). You can auto-tune by length quantile if needed.\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# ----------------------------\n# Dataset: returns paired inputs and a label in {+1, -1}\n# swap_prob adds order augmentation during training\n# ----------------------------\nclass PairwiseDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=384, with_labels=True, swap_prob=0.5):\n        self.text1 = df[\"file_1\"].tolist()\n        self.text2 = df[\"file_2\"].tolist()\n        self.with_labels = with_labels\n        self.swap_prob = swap_prob if with_labels else 0.0\n        if with_labels:\n            # label: +1 if file_1 is real, -1 if file_2 is real\n            y = (df[\"real_text_id\"].values == 1).astype(np.int64)\n            self.labels = torch.from_numpy(2*y - 1).float()  # {0,1} -> {-1,+1}\n        else:\n            self.labels = None\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self): \n        return len(self.text1)\n\n    def _encode(self, t):\n        enc = self.tokenizer(\n            t,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        return {k: v.squeeze(0) for k, v in enc.items()}\n\n    def __getitem__(self, i):\n        t1, t2 = self.text1[i], self.text2[i]\n        label = None if self.labels is None else self.labels[i]\n\n        # Randomly swap order during training (label flips sign)\n        if self.swap_prob > 0 and np.random.rand() < self.swap_prob:\n            t1, t2 = t2, t1\n            if label is not None:\n                label = -label\n\n        item = {}\n        enc1 = self._encode(t1)\n        enc2 = self._encode(t2)\n        # we namespace keys: *1 and *2\n        for k, v in enc1.items():\n            item[f\"{k}1\"] = v\n        for k, v in enc2.items():\n            item[f\"{k}2\"] = v\n        if label is not None:\n            item[\"labels\"] = label\n        return item\n\n# ----------------------------\n# Model: wraps a single-text scorer (num_labels=1) and computes pairwise loss\n# ----------------------------\nclass PairwiseRanker(nn.Module):\n    def __init__(self, base_model_name):\n        super().__init__()\n        # Single‑logit scorer\n        self.scorer = AutoModelForSequenceClassification.from_pretrained(\n            base_model_name, num_labels=1\n        )\n        self.loss_fn = nn.MarginRankingLoss(margin=0.2)\n\n    def forward(self, **batch):\n        # split inputs for side 1 and 2\n        def gather(prefix):\n            out = {}\n            for k in list(batch.keys()):\n                if k.endswith(prefix):\n                    out[k[:-1]] = batch[k]   # strip trailing '1' or '2'\n            return out\n\n        b1 = gather(\"1\")  # input_ids1 -> input_ids, attention_mask1 -> attention_mask, etc.\n        b2 = gather(\"2\")\n\n        # Some tokenizers don't provide token_type_ids; make sure we don't pass missing keys\n        # (Hugging Face handles absent keys fine, so no extra work needed)\n\n        s1 = self.scorer(**b1).logits.squeeze(-1)  # (B,)\n        s2 = self.scorer(**b2).logits.squeeze(-1)  # (B,)\n\n        logits_diff = (s1 - s2)                    # (B,)\n\n        if \"labels\" in batch and batch[\"labels\"] is not None:\n            # labels in {+1, -1}\n            y = batch[\"labels\"].view_as(logits_diff)\n            loss = self.loss_fn(s1, s2, y)\n            return {\"loss\": loss, \"logits\": logits_diff}\n\n        return {\"logits\": logits_diff}\n\n# ----------------------------\n# Metrics: pairwise accuracy\n# ----------------------------\ndef compute_metrics(eval_pred):\n    # Trainer passes predictions (logits) and labels\n    preds = eval_pred.predictions.squeeze()\n    labels = eval_pred.label_ids.squeeze()\n    # correct if sign matches (positive -> text1 real)\n    acc = ( (preds > 0) == (labels > 0) ).mean().item()\n    return {\"pairwise_accuracy\": acc}\n\n# ============================\n# Build datasets\n# ============================\ntrain_dataset = PairwiseDataset(train_df, tokenizer, max_length=MAX_LEN, with_labels=True,  swap_prob=0.5)\nval_dataset   = PairwiseDataset(val_df,   tokenizer, max_length=MAX_LEN, with_labels=True,  swap_prob=0.0)\ntest_dataset  = PairwiseDataset(test,     tokenizer, max_length=MAX_LEN, with_labels=False, swap_prob=0.0)\n\n# ============================\n# Trainer & training\n# ============================\nargs = TrainingArguments(\n    output_dir=\"pair_rank_gol\",\n    num_train_epochs=3,\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.05,\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    logging_steps=200,\n    save_strategy=\"steps\",\n    save_steps=200,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"pairwise_accuracy\",\n    greater_is_better=True,\n    report_to=\"none\",\n    gradient_accumulation_steps=1,\n)\n\nmodel = PairwiseRanker(MODEL_NAME)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n# ============================\n# Validation sanity check (pairwise accuracy)\n# ============================\nval_out = trainer.predict(val_dataset)\nval_logits = val_out.predictions.squeeze()\nval_preds = (val_logits > 0).astype(int)        # 1 -> file_1 real, 0 -> file_2 real\nval_true  = (val_out.label_ids.squeeze() > 0).astype(int)\nval_pair_acc = (val_preds == val_true).mean()\nprint(f\"Validation pairwise accuracy: {val_pair_acc:.4f}\")\n\n# ============================\n# Inference & submission\n# ============================\ntest_out = trainer.predict(test_dataset)\nlogits_diff = test_out.predictions.squeeze()\nwhich_real = np.where(logits_diff > 0, 1, 2)   # sign decides which side is real\n\nsubmission = pd.DataFrame({\n    \"id\": test.index, \n    \"real_text_id\": which_real\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T10:12:38.585088Z","iopub.execute_input":"2025-07-28T10:12:38.585365Z","iopub.status.idle":"2025-07-28T10:12:45.680323Z","shell.execute_reply.started":"2025-07-28T10:12:38.585344Z","shell.execute_reply":"2025-07-28T10:12:45.679252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) Flatten pairs into single-text rows\ndef make_rowwise(df):  # df has columns: file_1, file_2, real_text_id, index 'id'\n    rows = []\n    for pid, row in df.reset_index().iterrows():\n        rows.append({'pair_id': row['id'], 'text': row['file_1'], 'label': int(row['real_text_id']==1)})\n        rows.append({'pair_id': row['id'], 'text': row['file_2'], 'label': int(row['real_text_id']==2)})\n    return pd.DataFrame(rows)\n\ntrain_row = make_rowwise(train_df)\nval_row   = make_rowwise(val_df)\n\n# 2) Dataset for single text\nclass RealnessDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=512, with_labels=True):\n        self.texts  = df['text'].tolist()\n        self.labels = df['label'].tolist() if with_labels else None\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tokenizer(self.texts[i], truncation=True, padding='max_length',\n                             max_length=self.max_length, return_tensors='pt')\n        item = {k: v.squeeze(0) for k,v in enc.items()}\n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels[i], dtype=torch.long)\n        return item\n\n# 3) Model: 2-class head\n#model_name = 'microsoft/deberta-v3-base'  # or 'roberta-base' deberta-v3-large\nmodel_name = 'microsoft/deberta-v3-base'\ntokenizer  = AutoTokenizer.from_pretrained(model_name)\nmodel      = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\ntrain_ds = RealnessDataset(train_row, tokenizer)\nval_ds   = RealnessDataset(val_row,   tokenizer)\n\n# 4) Metrics aligned to Kaggle (accuracy)\ndef compute_metrics(p):\n    from sklearn.metrics import accuracy_score, f1_score\n    y_pred = p.predictions.argmax(-1)\n    y_true = p.label_ids\n    return {\"accuracy\": accuracy_score(y_true, y_pred),\n            \"f1\": f1_score(y_true, y_pred)}\n\nargs = TrainingArguments(\n    output_dir='gol',\n    num_train_epochs=3,\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    eval_strategy='steps',\n    eval_steps=200,\n    logging_steps=200,\n    save_strategy='steps',\n    save_steps=200,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model='accuracy',\n    greater_is_better=True,\n    report_to='none',\n)\n\ntrainer = Trainer(model=model, tokenizer=tokenizer,\n                  train_dataset=train_ds, eval_dataset=val_ds,\n                  compute_metrics=compute_metrics, args=args)\ntrainer.train()\n\n# 5) Inference: score both texts, choose the higher \"real\" prob within each pair\n# build test as two rows per pair\ndef make_rowwise_test(df):\n    rows=[]\n    for pid, row in df.reset_index().iterrows():\n        rows.append({'pair_id': row['id'], 'which':1, 'text': row['file_1']})\n        rows.append({'pair_id': row['id'], 'which':2, 'text': row['file_2']})\n    return pd.DataFrame(rows)\n\ntest_row = make_rowwise_test(test)\ntest_ds  = RealnessDataset(test_row, tokenizer, with_labels=False)\nout = trainer.predict(test_ds)\n\nimport numpy as np, pandas as pd\nprobs = np.exp(out.predictions) / np.exp(out.predictions).sum(-1, keepdims=True)\ntest_row = test_row.assign(p_real=probs[:,1])\nchoice = (test_row.sort_values(['pair_id','which'])\n                 .groupby('pair_id', as_index=False)\n                 .apply(lambda g: pd.Series({'real_text_id': int(g.loc[g['p_real'].idxmax(), 'which'])})))\nsubmission = choice.rename(columns={'pair_id':'id'})[['id','real_text_id']]\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T22:56:21.837493Z","iopub.execute_input":"2025-07-28T22:56:21.838208Z","iopub.status.idle":"2025-07-28T22:56:33.380224Z","shell.execute_reply.started":"2025-07-28T22:56:21.838179Z","shell.execute_reply":"2025-07-28T22:56:33.379274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List, Any\nimport torch\n\n@dataclass\nclass PairwiseCollator:\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        batch: Dict[str, torch.Tensor] = {}\n        keys = features[0].keys()\n        for k in keys:\n            vals = [f[k] for f in features]\n            if isinstance(vals[0], torch.Tensor):\n                batch[k] = torch.stack(vals)\n            else:\n                batch[k] = torch.tensor(vals)\n        return batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:50:43.232138Z","iopub.execute_input":"2025-07-29T10:50:43.232467Z","iopub.status.idle":"2025-07-29T10:50:43.239370Z","shell.execute_reply.started":"2025-07-29T10:50:43.232443Z","shell.execute_reply":"2025-07-29T10:50:43.238745Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ============================\n# Pairwise Ranking: setup\n# ============================\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#MODEL_NAME = \"microsoft/deberta-v3-base\"\nMODEL_NAME = \"google/electra-large-discriminator\"\n# good default; switch to -large if VRAM allows\nMAX_LEN = 384                               # tweak (384–512). You can auto-tune by length quantile if needed.\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# ----------------------------\n# Dataset: returns paired inputs and a label in {+1, -1}\n# swap_prob adds order augmentation during training\n# ----------------------------\nclass PairwiseDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=384, with_labels=True, swap_prob=0.5):\n        self.text1 = df[\"file_1\"].tolist()\n        self.text2 = df[\"file_2\"].tolist()\n        self.with_labels = with_labels\n        self.swap_prob = swap_prob if with_labels else 0.0\n        if with_labels:\n            # label: +1 if file_1 is real, -1 if file_2 is real\n            y = (df[\"real_text_id\"].values == 1).astype(np.int64)\n            self.labels = torch.from_numpy(2*y - 1).float()  # {0,1} -> {-1,+1}\n        else:\n            self.labels = None\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self): \n        return len(self.text1)\n\n    def _encode(self, t):\n        enc = self.tokenizer(\n            t,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        return {k: v.squeeze(0) for k, v in enc.items()}\n\n    def __getitem__(self, i):\n        t1, t2 = self.text1[i], self.text2[i]\n        label = None if self.labels is None else self.labels[i]\n\n        # Randomly swap order during training (label flips sign)\n        if self.swap_prob > 0 and np.random.rand() < self.swap_prob:\n            t1, t2 = t2, t1\n            if label is not None:\n                label = -label\n\n        item = {}\n        enc1 = self._encode(t1)\n        enc2 = self._encode(t2)\n        # we namespace keys: *1 and *2\n        for k, v in enc1.items():\n            item[f\"{k}1\"] = v\n        for k, v in enc2.items():\n            item[f\"{k}2\"] = v\n        if label is not None:\n            item[\"labels\"] = label\n        return item\n\n# ----------------------------\n# Model: wraps a single-text scorer (num_labels=1) and computes pairwise loss\n# ----------------------------\nclass PairwiseRanker(nn.Module):\n    def __init__(self, base_model_name):\n        super().__init__()\n        # Single‑logit scorer\n        self.scorer = AutoModelForSequenceClassification.from_pretrained(\n            base_model_name, num_labels=1\n        )\n        self.loss_fn = nn.MarginRankingLoss(margin=0.2)\n\n    def forward(self, **batch):\n        # split inputs for side 1 and 2\n        def gather(prefix):\n            out = {}\n            for k in list(batch.keys()):\n                if k.endswith(prefix):\n                    out[k[:-1]] = batch[k]   # strip trailing '1' or '2'\n            return out\n\n        b1 = gather(\"1\")  # input_ids1 -> input_ids, attention_mask1 -> attention_mask, etc.\n        b2 = gather(\"2\")\n\n        # Some tokenizers don't provide token_type_ids; make sure we don't pass missing keys\n        # (Hugging Face handles absent keys fine, so no extra work needed)\n\n        s1 = self.scorer(**b1).logits.squeeze(-1)  # (B,)\n        s2 = self.scorer(**b2).logits.squeeze(-1)  # (B,)\n\n        logits_diff = (s1 - s2)                    # (B,)\n\n        if \"labels\" in batch and batch[\"labels\"] is not None:\n            # labels in {+1, -1}\n            y = batch[\"labels\"].view_as(logits_diff)\n            loss = self.loss_fn(s1, s2, y)\n            return {\"loss\": loss, \"logits\": logits_diff}\n\n        return {\"logits\": logits_diff}\n\n# ----------------------------\n# Metrics: pairwise accuracy\n# ----------------------------\ndef compute_metrics(eval_pred):\n    # Trainer passes predictions (logits) and labels\n    preds = eval_pred.predictions.squeeze()\n    labels = eval_pred.label_ids.squeeze()\n    # correct if sign matches (positive -> text1 real)\n    acc = ( (preds > 0) == (labels > 0) ).mean().item()\n    return {\"pairwise_accuracy\": acc}\n\n# ============================\n# Build datasets\n# ============================\ntrain_dataset = PairwiseDataset(train_df, tokenizer, max_length=MAX_LEN, with_labels=True,  swap_prob=0.5)\nval_dataset   = PairwiseDataset(val_df,   tokenizer, max_length=MAX_LEN, with_labels=True,  swap_prob=0.0)\ntest_dataset  = PairwiseDataset(test,     tokenizer, max_length=MAX_LEN, with_labels=False, swap_prob=0.0)\n\n# ============================\n# Trainer & training\n# ============================\nargs = TrainingArguments(\n    output_dir=\"pair_rank_gol\",\n    num_train_epochs=6,\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.05,\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    logging_steps=200,\n    save_strategy=\"steps\",\n    save_steps=200,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"pairwise_accuracy\",\n    greater_is_better=True,\n    report_to=\"none\",\n    gradient_accumulation_steps=1,\n    remove_unused_columns=False,   # <-- keep custom keys\n)\nmodel = PairwiseRanker(MODEL_NAME)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    data_collator=PairwiseCollator(),  # <-- custom collator\n)\n\n\n\ntrainer.train()\n\n# ============================\n# Validation sanity check (pairwise accuracy)\n# ============================\nval_out = trainer.predict(val_dataset)\nval_logits = val_out.predictions.squeeze()\nval_preds = (val_logits > 0).astype(int)        # 1 -> file_1 real, 0 -> file_2 real\nval_true  = (val_out.label_ids.squeeze() > 0).astype(int)\nval_pair_acc = (val_preds == val_true).mean()\nprint(f\"Validation pairwise accuracy: {val_pair_acc:.4f}\")\n\n# ============================\n# Inference & submission\n# ============================\ntest_out = trainer.predict(test_dataset)\nlogits_diff = test_out.predictions.squeeze()\nwhich_real = np.where(logits_diff > 0, 1, 2)   # sign decides which side is real\n\nsubmission = pd.DataFrame({\n    \"id\": test.index, \n    \"real_text_id\": which_real\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:50:45.572121Z","iopub.execute_input":"2025-07-29T10:50:45.572894Z","iopub.status.idle":"2025-07-29T10:50:51.895462Z","shell.execute_reply.started":"2025-07-29T10:50:45.572844Z","shell.execute_reply":"2025-07-29T10:50:51.894023Z"}},"outputs":[{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/215824418.py:155: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/215824418.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;31m# ============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3811\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/215824418.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **batch)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mlogits_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# (B,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         discriminator_hidden_states = self.electra(\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_project\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         hidden_states = self.encoder(\n\u001b[0m\u001b[1;32m    813\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    575\u001b[0m                 )\n\u001b[1;32m    576\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    578\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 393\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     return (\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m     )\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 29.12 MiB is free. Process 2375 has 15.86 GiB memory in use. Of the allocated memory 15.56 GiB is allocated by PyTorch, and 7.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 29.12 MiB is free. Process 2375 has 15.86 GiB memory in use. Of the allocated memory 15.56 GiB is allocated by PyTorch, and 7.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"test_out = trainer.predict(test_dataset)\nlogits_diff = test_out.predictions.squeeze()\nwhich_real = np.where(logits_diff > 0, 1, 2)   # sign decides which side is real\n\nsubmission = pd.DataFrame({\n    \"id\": test.index, \n    \"real_text_id\": which_real\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:13:28.188735Z","iopub.execute_input":"2025-07-28T23:13:28.18944Z","iopub.status.idle":"2025-07-28T23:14:12.92236Z","shell.execute_reply.started":"2025-07-28T23:13:28.189415Z","shell.execute_reply":"2025-07-28T23:14:12.921786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:04:50.382478Z","iopub.execute_input":"2025-07-28T23:04:50.383039Z","iopub.status.idle":"2025-07-28T23:04:50.399414Z","shell.execute_reply.started":"2025-07-28T23:04:50.383019Z","shell.execute_reply":"2025-07-28T23:04:50.398776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"preds_out = trainer.predict(test_dataset)\nlogits = preds_out.predictions.squeeze()\nprobs  = 1 / (1 + np.exp(-logits))\npreds  = (probs > 0.5).astype(int)\nsubmission = pd.DataFrame({'id': test.index, 'label': preds + 1})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''preds_out = trainer.predict(test_dataset)\npreds     = np.argmax(preds_out.predictions, axis=1)\nsubmission = pd.DataFrame({'id': test.index.tolist(), 'label': preds + 1})  # shift back to {1,2}\nsubmission.to_csv('submission.csv', index=False)\nprint('Saved submission.csv')'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:22:00.968323Z","iopub.execute_input":"2025-07-23T17:22:00.9685Z","iopub.status.idle":"2025-07-23T17:22:08.517557Z","shell.execute_reply.started":"2025-07-23T17:22:00.968485Z","shell.execute_reply":"2025-07-23T17:22:08.51688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_thr=0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:25:54.59409Z","iopub.execute_input":"2025-07-23T17:25:54.594594Z","iopub.status.idle":"2025-07-23T17:25:54.598025Z","shell.execute_reply.started":"2025-07-23T17:25:54.594567Z","shell.execute_reply":"2025-07-23T17:25:54.597324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logits = preds_out.predictions.squeeze()          # (N,)\nprobs  = 1 / (1 + np.exp(-logits))                # sigmoid\npreds  = (probs > best_thr).astype(int)           # 0/1\nsubmission = pd.DataFrame({'id': test.index, 'label': preds + 1})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:25:55.934209Z","iopub.execute_input":"2025-07-23T17:25:55.934858Z","iopub.status.idle":"2025-07-23T17:25:55.939334Z","shell.execute_reply.started":"2025-07-23T17:25:55.934837Z","shell.execute_reply":"2025-07-23T17:25:55.938631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('eeeee.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:26:29.91837Z","iopub.execute_input":"2025-07-23T17:26:29.91865Z","iopub.status.idle":"2025-07-23T17:26:29.924461Z","shell.execute_reply.started":"2025-07-23T17:26:29.918629Z","shell.execute_reply":"2025-07-23T17:26:29.923753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:25:58.156528Z","iopub.execute_input":"2025-07-23T17:25:58.157195Z","iopub.status.idle":"2025-07-23T17:25:58.165126Z","shell.execute_reply.started":"2025-07-23T17:25:58.15717Z","shell.execute_reply":"2025-07-23T17:25:58.164468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference_and_submission(trainer, test_dataset, test_index):\n    preds_out = trainer.predict(test_dataset)\n    preds     = np.argmax(preds_out.predictions, axis=1)\n    return pd.DataFrame({'id': test_index, 'label': preds + 1})  # shift back to {1,2}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:22:08.53921Z","iopub.execute_input":"2025-07-23T17:22:08.539461Z","iopub.status.idle":"2025-07-23T17:22:08.544003Z","shell.execute_reply.started":"2025-07-23T17:22:08.539442Z","shell.execute_reply":"2025-07-23T17:22:08.543404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\ndef evaluate_models(model_names, train_df, val_df, tokenizer_class, model_class):\n    results = []\n    for model_name in model_names:\n        print(f\"=== Evaluating {model_name} ===\")\n        tok = tokenizer_class.from_pretrained(model_name)\n        mdl = model_class.from_pretrained(model_name, num_labels=2)\n        # Freeze encoder\n        for n,p in mdl.named_parameters():\n            if 'classifier' not in n:\n                p.requires_grad = False\n        train_ds = ImposterDataset(train_df, tok)\n        val_ds   = ImposterDataset(val_df,   tok)\n        args = TrainingArguments(\n            output_dir=f'gol_{model_name.replace(\"/\",\"_\")}',\n            num_train_epochs=5,\n            learning_rate=5e-6,\n            per_device_train_batch_size=8,\n            per_device_eval_batch_size=8,\n            weight_decay=0.01,\n            eval_strategy='epoch',\n            save_strategy='no',\n            logging_strategy='no',\n            load_best_model_at_end=False\n        )\n        tr = Trainer(\n            model=mdl,\n            args=args,\n            train_dataset=train_ds,\n            eval_dataset=val_ds,\n            compute_metrics=compute_metrics,\n            tokenizer=tok,\n            \n        )\n        tr.train()\n        metrics = tr.evaluate()\n        results.append({'model': model_name, 'eval_accuracy': metrics['eval_accuracy'], 'eval_f1': metrics['eval_f1']})\n    return pd.DataFrame(results)\n\n# Define candidate models\ncandidate_models = [\n    'intfloat/e5-small-v2',\n    'distilbert-base-uncased',\n    'roberta-base',\n    'sentence-transformers/all-MiniLM-L6-v2'\n]\n\n# Run evaluation\ncomparison_df = evaluate_models(\n    candidate_models,\n    train_df, val_df,\n    \n    AutoTokenizer, AutoModelForSequenceClassification\n)\n\n\nprint(comparison_df)\n\n# Pick best model and retrain full pipeline if desired\nbest_model = comparison_df.sort_values('eval_f1', ascending=False).iloc[0]['model']\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:22:08.544641Z","iopub.execute_input":"2025-07-23T17:22:08.544848Z","iopub.status.idle":"2025-07-23T17:25:44.93105Z","shell.execute_reply.started":"2025-07-23T17:22:08.544831Z","shell.execute_reply":"2025-07-23T17:25:44.929874Z"}},"outputs":[],"execution_count":null}]}