{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":24286,"databundleVersionId":1878097,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================== Shopee: ViT + DeBERTa + Text Decoder (ArcFace) ==================\n## ================== Shopee: ViT + DeBERTa + Sub-Center ArcFace (fixed training) ==================\n# ================== Shopee: ViT + DeBERTa + Sub-Center ArcFace + Fusion (fixed) ==================\nimport os, math, random, gc, sys\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# torchvision v2 transforms (fallback to old API)\ntry:\n    from torchvision.transforms import v2\nexcept Exception:\n    from torchvision import transforms as v2\n\nimport timm\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel\n\n# ----------------- Seeds / Determinism -----------------\nseed = 42\nos.environ['PYTHONHASHSEED'] = str(seed)\nrandom.seed(seed); np.random.seed(seed)\ntorch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ----------------- Config -----------------\nDATA_DIR = '/kaggle/input/shopee-product-matching'\nTRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\nTEST_CSV  = os.path.join(DATA_DIR, 'test.csv')\nTRAIN_IMG_DIR = os.path.join(DATA_DIR, 'train_images')\nTEST_IMG_DIR  = os.path.join(DATA_DIR, 'test_images')\n\nIMSIZE = 224\nBATCH_SIZE = 32\nNUM_WORKERS = 4\nEPOCHS = 5\n\n# Warmup/AMP/Clip\nFREEZE_EPOCHS   = 1           # freeze backbone for first N epochs\nAMP_OFF_STEPS   = 1000        # no AMP for the first N optimizer steps\nCLIP_GRAD_NORM  = 5.0         # gradient clipping (None to disable)\n\n# Retrieval / fusion\nKQ = 100\nMUTUAL_AT_VAL = True\nTEXT_MAX_LEN = 64\nTEXT_ARC_W_FINAL = 0.20       # weight of text-branch CE in total loss (linearly ramps)\n\n# image / text encoders\nIMG_BACKBONE = 'eca_nfnet_l1'\nTEXT_MODEL_NAME = 'xlm-roberta-base'\n\n# ArcFace S (scale) schedule\nS_START, S_END = 16.0, 45.0\nSUB_K = 3   # sub-centers per class\n\n# --------- Margin strategy (pick one) ----------\nUSE_ADAPTIVE_MARGIN = True   # True: class_size^-p ; False: fixed final margins (1.0 img / 0.8 txt)\nM_START = 0.20               # start margin for warm-up (both branches)\n# Adaptive exponents\nIMG_MARGIN_POW = -0.10\nTXT_MARGIN_POW = -0.20\n# Fixed final margins (ignored when USE_ADAPTIVE_MARGIN=True)\nM_IMG_FIXED_FINAL = 1.00\nM_TXT_FIXED_FINAL = 0.80\n\n# --------- LRs ----------\nLR_BACKBONE = 2e-4\nLR_HEAD     = 1e-3\nLR_TEXTHEAD = 1e-3\nLR_ARC      = 3e-3           # larger LR for ArcFace \"cosinehead\"\n\n# --------- LR schedule (per-step) ----------\nWARMUP_RATIO = 0.30          # large warmup as suggested\n\n# ----------------- Data -----------------\ntrain = pd.read_csv(TRAIN_CSV)\ntest  = pd.read_csv(TEST_CSV)\n\n# id mapping\nlabel2id = {lg: i for i, lg in enumerate(sorted(train['label_group'].unique()))}\nid2label = {i: lg for lg, i in label2id.items()}\ntrain['class_id'] = train['label_group'].map(label2id)\n\n# simple random split\ntrain_df, val_df = train_test_split(train, test_size=0.2, random_state=seed, shuffle=True)\n\n# ----------------- Dataset -----------------\nclass ShopeeDataset(Dataset):\n    def __init__(self, df, img_root, transform, train=True):\n        self.df = df.reset_index(drop=True)\n        self.img_root = img_root\n        self.transform = transform\n        self.train = train\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_root, str(row['image']))\n        image = Image.open(img_path).convert('RGB')\n        if self.transform is not None:\n            image = self.transform(image)\n        sample = {'image': image,\n                  'posting_id': row['posting_id'],\n                  'title': row.get('title', '')}\n        if self.train:\n            sample['label'] = torch.tensor(int(row['class_id']), dtype=torch.long)\n        return sample\n\n# ----------------- Transforms -----------------\ntransforms_train = v2.Compose([\n    v2.Resize(256, antialias=True),\n    v2.RandomResizedCrop(IMSIZE, scale=(0.8, 1.0), antialias=True),\n    v2.RandomHorizontalFlip(),\n    v2.ToTensor(),\n    v2.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n])\ntransforms_eval = v2.Compose([\n    v2.Resize(256, antialias=True),\n    v2.CenterCrop(IMSIZE),\n    v2.ToTensor(),\n    v2.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n])\n\ntrain_ds = ShopeeDataset(train_df, TRAIN_IMG_DIR, transforms_train, train=True)\nval_ds   = ShopeeDataset(val_df,   TRAIN_IMG_DIR, transforms_eval,  train=True)\ntest_ds  = ShopeeDataset(test,     TEST_IMG_DIR,  transforms_eval,  train=False)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True)\n\n# ----------------- Models -----------------\n# Image backbone (ViT)\nimg_backbone = timm.create_model(IMG_BACKBONE, pretrained=True, num_classes=0).to(device)\nfeat_dim = img_backbone.num_features   # ViT-B/16 -> 768\n\n# Heads: BN -> L2 norm (no extra FC)\nembedding_head = nn.Sequential(\n    nn.Linear(feat_dim, 512, bias=False),\n    nn.BatchNorm1d(512)\n).to(device)\n\n# Text encoder (frozen) + projection head BN -> L2\ntext_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME, use_fast=True)\ntext_encoder   = AutoModel.from_pretrained(TEXT_MODEL_NAME).to(device)\nfor p in text_encoder.parameters():\n    p.requires_grad = False\ntext_hidden = text_encoder.config.hidden_size\n\ntext_head = nn.Sequential(\n    nn.Linear(text_hidden, 512, bias=False),\n    nn.BatchNorm1d(512)\n).to(device)\n\ndef mean_pooling(last_hidden_state, attention_mask):\n    mask = attention_mask.unsqueeze(-1).float()\n    summed = (last_hidden_state * mask).sum(dim=1)\n    denom = mask.sum(dim=1).clamp(min=1e-6)\n    return summed / denom\n\n# ----------------- Sub-Center ArcFace with per-sample margin -----------------\nclass SubCenterArcFace(nn.Module):\n    \"\"\"\n    Sub-Center ArcFace supporting per-sample margins:\n      - K sub-centers per class, take max across sub-centers\n      - optional margin_per_sample: tensor (B,) of margins for true labels\n    \"\"\"\n    def __init__(self, in_features, out_classes, k_sub=3, s=30.0, m=0.5, easy_margin=False):\n        super().__init__()\n        self.in_features  = in_features\n        self.out_classes  = out_classes\n        self.k_sub        = k_sub\n        self.s            = s\n        self.m            = float(m)\n        self.easy_margin  = easy_margin\n\n        self.weight = nn.Parameter(torch.FloatTensor(out_classes * k_sub, in_features))\n        nn.init.xavier_uniform_(self.weight)\n        self._refresh_trig()\n\n    def _refresh_trig(self):\n        self.cos_m = math.cos(self.m)\n        self.sin_m = math.sin(self.m)\n        self.th    = math.cos(math.pi - self.m)\n        self.mm    = math.sin(math.pi - self.m) * self.m\n\n    def set_margin(self, m: float):\n        self.m = float(m)\n        self._refresh_trig()\n\n    def forward(self, emb, labels, margin_per_sample: torch.Tensor = None):\n    # emb: (B,D) L2-normalized; labels: (B,)\n        W = F.normalize(self.weight)                           # (C*K, D)\n        cosine_all = F.linear(emb, W)                          # (B, C*K)\n        cosine_all = cosine_all.view(emb.size(0), self.out_classes, self.k_sub)\n        cosine, _  = torch.max(cosine_all, dim=2)              # (B, C)\n        dtype = cosine.dtype\n        device = cosine.device\n\n        if margin_per_sample is None:\n        # fixed-margin path – keep everything in `dtype`\n            cos_m = torch.tensor(self.cos_m, device=device, dtype=dtype)\n            sin_m = torch.tensor(self.sin_m, device=device, dtype=dtype)\n            th    = torch.tensor(self.th,    device=device, dtype=dtype)\n            mm    = torch.tensor(self.mm,    device=device, dtype=dtype)\n\n            sine = torch.sqrt(torch.clamp(1.0 - cosine**2, min=1e-6))\n            phi  = cosine * cos_m - sine * sin_m\n            if self.easy_margin:\n                phi = torch.where(cosine > 0, phi, cosine)\n            else:\n                phi = torch.where(cosine > th, phi, cosine - mm)\n\n            one_hot = torch.zeros_like(cosine)\n            one_hot.scatter_(1, labels.view(-1,1), 1.0)\n            logits = one_hot * phi + (1.0 - one_hot) * cosine\n            return logits * self.s\n\n    # per-sample margin path – compute in `dtype`\n        m  = margin_per_sample.view(-1, 1).to(dtype)\n        cos_m = torch.cos(m)\n        sin_m = torch.sin(m)\n        th    = torch.cos(torch.pi - m)\n        mm    = torch.sin(torch.pi - m) * m\n\n        idx   = torch.arange(cosine.size(0), device=device)\n        cos_y = cosine[idx, labels].view(-1, 1)                # (B,1)\n        sin_y = torch.sqrt(torch.clamp(1.0 - cos_y**2, min=1e-6))\n        phi_y = cos_y * cos_m - sin_y * sin_m\n        if not self.easy_margin:\n            phi_y = torch.where(cos_y > th, phi_y, cos_y - mm)\n\n        logits = cosine.clone()\n        logits[idx, labels] = phi_y.view(-1).to(dtype)         # <-- key cast\n        return logits * self.s\n\nNUM_CLASSES = train['class_id'].nunique()\narcface = SubCenterArcFace(512, NUM_CLASSES, k_sub=SUB_K, s=S_START, m=0.0, easy_margin=False).to(device)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n\n# ----------------- Param groups (BN/LayerNorm no WD) -----------------\ndef split_params_by_wd(module):\n    wd, no_wd = [], []\n    for m in module.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):\n            for p in m.parameters(recurse=False):\n                if p.requires_grad: no_wd.append(p)\n        else:\n            for p in m.parameters(recurse=False):\n                if p.requires_grad:\n                    (no_wd if p.ndim==1 else wd).append(p)\n    return wd, no_wd\n\nwd_backbone, no_wd_backbone = split_params_by_wd(img_backbone)\nwd_head,     no_wd_head     = split_params_by_wd(embedding_head)\nwd_text,     no_wd_text     = split_params_by_wd(text_head)\nwd_arc,      no_wd_arc      = split_params_by_wd(arcface)\n\nparam_groups = [\n    {'params': wd_backbone, 'lr': LR_BACKBONE, 'weight_decay': 0.05},\n    {'params': no_wd_backbone, 'lr': LR_BACKBONE, 'weight_decay': 0.0},\n    {'params': wd_head, 'lr': LR_HEAD, 'weight_decay': 0.05},\n    {'params': no_wd_head, 'lr': LR_HEAD, 'weight_decay': 0.0},\n    {'params': wd_text, 'lr': LR_TEXTHEAD, 'weight_decay': 0.05},\n    {'params': no_wd_text, 'lr': LR_TEXTHEAD, 'weight_decay': 0.0},\n    {'params': wd_arc, 'lr': LR_ARC, 'weight_decay': 0.05},       # larger LR for ArcFace\n    {'params': no_wd_arc, 'lr': LR_ARC, 'weight_decay': 0.0},\n]\noptimizer = torch.optim.AdamW(param_groups)\nscaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n\n# ----------------- Build margin schedules -----------------\nNUM_CLASSES = int(NUM_CLASSES)\nclass_sizes = np.ones(NUM_CLASSES, dtype=np.float32)\nfor cid, cnt in train['class_id'].value_counts().items():\n    class_sizes[int(cid)] = float(cnt)\n\nif USE_ADAPTIVE_MARGIN:\n    # target margins from class size: size^pow\n    m_img_target_np = np.power(class_sizes, IMG_MARGIN_POW).astype('float32')   # ~[1.0 .. 0.63]\n    m_txt_target_np = np.power(class_sizes, TXT_MARGIN_POW).astype('float32')   # ~[1.0 .. 0.40]\nelse:\n    m_img_target_np = np.full(NUM_CLASSES, M_IMG_FIXED_FINAL, dtype=np.float32)\n    m_txt_target_np = np.full(NUM_CLASSES, M_TXT_FIXED_FINAL, dtype=np.float32)\n\nm_img_target = torch.tensor(m_img_target_np, device=device)\nm_txt_target = torch.tensor(m_txt_target_np, device=device)\n\ndef s_at_epoch(ep, total=EPOCHS):\n    if total==1: return S_END\n    return S_START + (S_END - S_START) * (ep-1) / (total-1)\n\ndef margin_vec_at_epoch(ep, total, target_vec, m_start=M_START):\n    if total==1: \n        return target_vec\n    t = float(ep-1) / float(total-1)\n    return (1.0 - t) * m_start + t * target_vec\n\ndef text_weight_at_epoch(ep, w_end=TEXT_ARC_W_FINAL, total=EPOCHS):\n    if total==1: return w_end\n    return (w_end * (ep-1)) / (total-1)\n\ndef set_backbone_trainable(flag: bool):\n    for p in img_backbone.parameters():\n        p.requires_grad = flag\n\n# ----------------- Per-step warmup + cosine scheduler -----------------\nsteps_per_epoch = len(train_loader)\ntotal_steps = steps_per_epoch * EPOCHS\nwarmup_steps = max(100, int(WARMUP_RATIO * total_steps))\n\ndef lr_lambda(current_step: int):\n    if current_step < warmup_steps:\n        return float(current_step) / float(max(1, warmup_steps))\n    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n    return 0.5 * (1.0 + math.cos(math.pi * progress))\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n# ----------------- Train -----------------\nglobal_step = 0\nfor epoch in range(1, EPOCHS+1):\n    # schedules\n    arcface.s = s_at_epoch(epoch)\n    TW = text_weight_at_epoch(epoch)\n    m_img_epoch_vec = margin_vec_at_epoch(epoch, EPOCHS, m_img_target)  # (C,)\n    m_txt_epoch_vec = margin_vec_at_epoch(epoch, EPOCHS, m_txt_target)  # (C,)\n\n    # warmup freeze\n    set_backbone_trainable(epoch > FREEZE_EPOCHS)\n    img_backbone.train(epoch > FREEZE_EPOCHS)\n    embedding_head.train(); text_head.train(); arcface.train()\n\n    running = 0.0\n    pbar = tqdm(train_loader, desc=f\"train {epoch}/{EPOCHS} (s={arcface.s:.1f}, tw={TW:.2f})\", leave=False)\n    for batch in pbar:\n        optimizer.zero_grad(set_to_none=True)\n\n        use_amp = (device.type=='cuda') and (global_step >= AMP_OFF_STEPS)\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            X = batch['image'].to(device, non_blocking=True)\n            y = batch['label'].to(device, non_blocking=True)\n\n            # image branch\n            f_img = img_backbone(X)\n            e_img = F.normalize(embedding_head(f_img), dim=1)\n            # per-sample margins for image\n            m_img_ps = m_img_epoch_vec[y]                         # (B,)\n            logits_img = arcface(e_img, y, margin_per_sample=m_img_ps)\n            loss_img = criterion(logits_img, y)\n\n            # text branch (encoder frozen)\n            titles = batch['title']\n            tok = text_tokenizer(list(titles), padding=True, truncation=True,\n                                 max_length=TEXT_MAX_LEN, return_tensors='pt')\n            tok = {k: v.to(device, non_blocking=True) for k,v in tok.items()}\n            with torch.no_grad():\n                out_txt = text_encoder(**tok)\n                pooled  = mean_pooling(out_txt.last_hidden_state, tok['attention_mask'])\n            e_txt = F.normalize(text_head(pooled), dim=1)\n            m_txt_ps = m_txt_epoch_vec[y]                         # (B,)\n            logits_txt = arcface(e_txt, y, margin_per_sample=m_txt_ps)\n            loss_txt = criterion(logits_txt, y)\n\n            loss = loss_img + TW * loss_txt\n\n        if use_amp:\n            scaler.scale(loss).backward()\n            if CLIP_GRAD_NORM:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(arcface.parameters(), CLIP_GRAD_NORM)\n                torch.nn.utils.clip_grad_norm_(embedding_head.parameters(), CLIP_GRAD_NORM)\n                if epoch > FREEZE_EPOCHS:\n                    torch.nn.utils.clip_grad_norm_(img_backbone.parameters(), CLIP_GRAD_NORM)\n            scaler.step(optimizer); scaler.update()\n        else:\n            loss.backward()\n            if CLIP_GRAD_NORM:\n                torch.nn.utils.clip_grad_norm_(arcface.parameters(), CLIP_GRAD_NORM)\n                torch.nn.utils.clip_grad_norm_(embedding_head.parameters(), CLIP_GRAD_NORM)\n                if epoch > FREEZE_EPOCHS:\n                    torch.nn.utils.clip_grad_norm_(img_backbone.parameters(), CLIP_GRAD_NORM)\n            optimizer.step()\n\n        scheduler.step()\n        running += loss.item() * X.size(0)\n        global_step += 1\n        if global_step % 100 == 0:\n            try:\n                gnorm = arcface.weight.grad.norm().item()\n            except:\n                gnorm = float('nan')\n            pbar.set_postfix(loss=running/((pbar.n+1)*X.size(0)), g_arc=f\"{gnorm:.2f}\")\n\n# ----------------- Validation (embeddings + fusion) -----------------\n@torch.no_grad()\ndef build_img_embs(loader):\n    img_backbone.eval(); embedding_head.eval()\n    embs, ids, labels = [], [], []\n    for b in tqdm(loader, desc=\"Embed/val(img)\"):\n        x = b['image'].to(device, non_blocking=True)\n        e = F.normalize(embedding_head(img_backbone(x)), dim=1)\n        embs.append(e.cpu())\n        ids.extend(b['posting_id'])\n        labels.extend(b['label'].cpu().numpy().tolist())\n    embs = torch.cat(embs, dim=0).numpy().astype('float32')\n    embs = embs / (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-8)\n    return embs, ids, np.array(labels)\n\n@torch.no_grad()\ndef build_text_embs_for_df(df, batch_size=512, max_len=TEXT_MAX_LEN):\n    text_encoder.eval(); text_head.eval()\n    titles = df['title'].fillna('').astype(str).tolist()\n    outs = []\n    for i in tqdm(range(0, len(titles), batch_size), desc=\"Embed/val(txt)\"):\n        b = titles[i:i+batch_size]\n        tok = text_tokenizer(b, padding=True, truncation=True,\n                             max_length=max_len, return_tensors='pt')\n        tok = {k: v.to(device, non_blocking=True) for k,v in tok.items()}\n        out = text_encoder(**tok)\n        pooled = mean_pooling(out.last_hidden_state, tok['attention_mask'])\n        e = F.normalize(text_head(pooled), dim=1)\n        outs.append(e.cpu())\n    embs = torch.cat(outs, dim=0).numpy().astype('float32')\n    embs = embs / (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-8)\n    return embs\n\ndef topk_chunked_cos(embs_f32: np.ndarray, K: int, qbs: int = 128):\n    N, D = embs_f32.shape\n    device_t = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    db = torch.from_numpy(embs_f32.astype('float32', copy=False)).to(device_t, non_blocking=True)\n    K = min(K, N)\n    idxs_list, sims_list = [], []\n    for start in tqdm(range(0, N, qbs), desc=\"TopK (torch-chunk)\"):\n        q = db[start:start+qbs]\n        S = torch.matmul(q, db.T)\n        vals, ids = torch.topk(S, k=K, dim=1, largest=True, sorted=True)\n        idxs_list.append(ids.cpu().numpy().astype('int32'))\n        sims_list.append(vals.cpu().numpy().astype('float32'))\n        del S, vals, ids\n        if device_t.type == 'cuda':\n            torch.cuda.empty_cache()\n    idxs = np.vstack(idxs_list); sims = np.vstack(sims_list)\n    del db\n    return sims, idxs\n\ndef build_preds_fused_union(ids, idxs_img, sims_img, idxs_txt, sims_txt,\n                            tau, alpha=0.7, K_cap=50, mutual=True):\n    N = len(ids)\n    out = {}\n    for i in range(N):\n        cand_idx = set(idxs_img[i]).union(set(idxs_txt[i]))\n        map_img = {int(j): float(s) for j, s in zip(idxs_img[i], sims_img[i])}\n        map_txt = {int(j): float(s) for j, s in zip(idxs_txt[i], sims_txt[i])}\n        fused = []\n        for j in cand_idx:\n            si = (map_img.get(int(j), 0.0) + 1.0) / 2.0\n            st = (map_txt.get(int(j), 0.0) + 1.0) / 2.0\n            s  = alpha*si + (1.0-alpha)*st\n            fused.append((j, s))\n        fused.sort(key=lambda x: -x[1])\n        keep = []\n        for j, s in fused:\n            if s < tau:\n                continue\n            if (not mutual) or (np.any(idxs_img[j] == i)):\n                keep.append(ids[j])\n        if ids[i] not in keep:\n            keep = [ids[i]] + keep\n        out[ids[i]] = set(keep[:50])\n    return out\n\ndef f1_matches(ids, labels, preds):\n    truth={}\n    for pid, g in zip(ids, labels):\n        truth.setdefault(g, set()).add(pid)\n    f1s=[]\n    for pid, g in zip(ids, labels):\n        T = truth[g]; P = preds[pid]\n        inter = len(T & P); denom = len(T)+len(P)\n        f1s.append(2*inter/denom if denom>0 else 0.0)\n    return float(np.mean(f1s))\n\n# --- build embeddings ---\nval_img, val_ids, val_labels = build_img_embs(val_loader)\nval_txt = build_text_embs_for_df(val_ds.df, batch_size=512, max_len=TEXT_MAX_LEN)\n\n# --- topK ---\nsims_img, idxs_img = topk_chunked_cos(val_img, K=KQ, qbs=128)\nsims_txt, idxs_txt = topk_chunked_cos(val_txt, K=KQ, qbs=256)\n\n# --- grid search for (alpha, tau), MUTUAL=True ---\nalphas = np.linspace(0.4, 0.9, 6)\ntaus   = np.linspace(0.20, 0.80, 31)\nbest_f1, best_tau, best_alpha, best_preds = -1.0, None, None, None\nfor a in alphas:\n    for t in taus:\n        preds = build_preds_fused_union(val_ids, idxs_img, sims_img, idxs_txt, sims_txt,\n                                        tau=float(t), alpha=float(a), K_cap=50, mutual=MUTUAL_AT_VAL)\n        f1 = f1_matches(val_ids, val_labels, preds)\n        if f1 > best_f1:\n            best_f1, best_tau, best_alpha, best_preds = f1, float(t), float(a), preds\nprint(f\"[VAL FUSION] Best F1={best_f1:.4f} at tau={best_tau:.2f}, alpha={best_alpha:.2f}\")\nprint(f\"[VAL] Avg predicted group size: {np.mean([len(v) for v in best_preds.values()]):.2f}\")\n\n# ----------------- Save -----------------\nSAVE_DIR = '/kaggle/working'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\nckpt = {\n    'backbone_name': IMG_BACKBONE,\n    'text_model_name': TEXT_MODEL_NAME,\n    'feat_dim': int(feat_dim),\n    'emb_dim': 512,\n    'num_classes': NUM_CLASSES,\n    'arcface_type': 'subcenter',\n    'arcface_cfg': {'s': float(arcface.s), 'k_sub': int(SUB_K), 'easy_margin': False},\n    'state_dict': {\n        'backbone': img_backbone.state_dict(),\n        'embedding_head': embedding_head.state_dict(),\n        'text_head': text_head.state_dict(),\n        'arcface': arcface.state_dict(),\n    },\n    'label2id': label2id,\n    'best_tau': float(best_tau),\n    'best_alpha': float(best_alpha),\n    'mutual_used': bool(MUTUAL_AT_VAL),\n    'val_f1_fusion': float(best_f1),\n    'epoch': EPOCHS,\n    'use_adaptive_margin': bool(USE_ADAPTIVE_MARGIN),\n    'img_margin_pow': float(IMG_MARGIN_POW),\n    'txt_margin_pow': float(TXT_MARGIN_POW),\n}\ntorch.save(ckpt, os.path.join(SAVE_DIR, 'vit_deberta_subarcface_ckpt.pth'))\nprint('[SAVE] Full checkpoint ->', os.path.join(SAVE_DIR, 'vit_deberta_subarcface_ckpt.pth'))\n\nembed_pkg = {\n    'backbone_name': IMG_BACKBONE,\n    'text_model_name': TEXT_MODEL_NAME,\n    'feat_dim': int(feat_dim),\n    'emb_dim': 512,\n    'state_dict': {\n        'backbone': img_backbone.state_dict(),\n        'embedding_head': embedding_head.state_dict(),\n        'text_head': text_head.state_dict(),\n    },\n    'best_tau': float(best_tau),\n    'best_alpha': float(best_alpha),\n    'mutual_used': bool(MUTUAL_AT_VAL),\n}\ntorch.save(embed_pkg, os.path.join(SAVE_DIR, 'embedding_extractor_vit_deberta_subarcface.pth'))\nprint('[SAVE] Embedding extractor ->', os.path.join(SAVE_DIR, 'embedding_extractor_vit_deberta_subarcface.pth'))\n\n# ================== Inference / Submission ==================\n@torch.no_grad()\ndef embed_test_images(ds, batch_size=64):\n    loader = DataLoader(ds, batch_size=batch_size, shuffle=False,\n                        num_workers=NUM_WORKERS, pin_memory=True)\n    img_backbone.eval(); embedding_head.eval()\n    embs, ids = [], []\n    for b in tqdm(loader, desc=\"Embed/test(img)\"):\n        x = b['image'].to(device, non_blocking=True)\n        e = F.normalize(embedding_head(img_backbone(x)), dim=1)\n        embs.append(e.cpu())\n        ids.extend(b['posting_id'])\n    embs = torch.cat(embs, dim=0).numpy().astype('float32')\n    embs = embs / (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-8)\n    return embs, ids\n\n@torch.no_grad()\ndef embed_test_text(df, batch_size=512, max_len=TEXT_MAX_LEN):\n    text_encoder.eval(); text_head.eval()\n    titles = df['title'].fillna('').astype(str).tolist()\n    outs=[]\n    for i in tqdm(range(0, len(titles), batch_size), desc=\"Embed/test(txt)\"):\n        b = titles[i:i+batch_size]\n        tok = text_tokenizer(b, padding=True, truncation=True,\n                             max_length=max_len, return_tensors='pt')\n        tok = {k: v.to(device, non_blocking=True) for k,v in tok.items()}\n        out = text_encoder(**tok)\n        pooled = mean_pooling(out.last_hidden_state, tok['attention_mask'])\n        e = F.normalize(text_head(pooled), dim=1)\n        outs.append(e.cpu())\n    embs = torch.cat(outs, dim=0).numpy().astype('float32')\n    embs = embs / (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-8)\n    return embs\n\ndef predict_fused(ids, img_embs, txt_embs, alpha, tau, mutual=True, K_cap=50, ksearch=KQ):\n    sims_img, idxs_img = topk_chunked_cos(img_embs, K=ksearch, qbs=128)\n    sims_txt, idxs_txt = topk_chunked_cos(txt_embs, K=ksearch, qbs=256)\n    preds = build_preds_fused_union(ids, idxs_img, sims_img, idxs_txt, sims_txt,\n                                    tau=float(tau), alpha=float(alpha), K_cap=K_cap, mutual=mutual)\n    posting_ids = []\n    matches = []\n    for pid in ids:\n        posting_ids.append(pid)\n        matches.append(\" \".join(list(preds[pid])))\n    return pd.DataFrame({'posting_id': posting_ids, 'matches': matches})\n\n# --- Build test embeddings & predict ---\ntest_img_embs, test_ids = embed_test_images(test_ds, batch_size=64)\ntest_txt_embs = embed_test_text(test_ds.df, batch_size=512, max_len=TEXT_MAX_LEN)\n\nsub_df = predict_fused(test_ids, test_img_embs, test_txt_embs,\n                       alpha=best_alpha, tau=best_tau, mutual=MUTUAL_AT_VAL, K_cap=50, ksearch=KQ)\n\nOUT_PATH = '/kaggle/working/submission.csv'\nsub_df.to_csv(OUT_PATH, index=False)\nprint(\"[SAVE] submission ->\", OUT_PATH)\nprint(sub_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T11:51:47.184144Z","iopub.execute_input":"2025-09-21T11:51:47.184474Z","iopub.status.idle":"2025-09-21T12:27:45.343333Z","shell.execute_reply.started":"2025-09-21T11:51:47.184451Z","shell.execute_reply":"2025-09-21T12:27:45.342429Z"}},"outputs":[],"execution_count":null}]}