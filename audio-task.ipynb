{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13367096,"sourceType":"datasetVersion","datasetId":8479594}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q sentence-transformers==2.7.0 transformers==4.45.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:39:20.459627Z","iopub.execute_input":"2025-10-20T19:39:20.460359Z","iopub.status.idle":"2025-10-20T19:41:00.689404Z","shell.execute_reply.started":"2025-10-20T19:39:20.460329Z","shell.execute_reply":"2025-10-20T19:41:00.688173Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, math, json, random, warnings, time\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict, Union\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\n\nimport torchaudio\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoConfig, AutoModel, TrainingArguments, Trainer\nfrom tqdm.auto import tqdm\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:00.691390Z","iopub.execute_input":"2025-10-20T19:41:00.691690Z","iopub.status.idle":"2025-10-20T19:41:32.935286Z","shell.execute_reply.started":"2025-10-20T19:41:00.691663Z","shell.execute_reply":"2025-10-20T19:41:32.934286Z"}},"outputs":[{"name":"stderr","text":"2025-10-20 19:41:15.055014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760989275.275823      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760989275.340010      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"TRAIN_DIR = \"/kaggle/input/vseros-audio-task/train_data\"\nTEST_DIR  = \"/kaggle/input/vseros-audio-task/test_data\"\nOUT_DIR   = \"/kaggle/working/out_kws\"\n\nBACKBONE = \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:32.936373Z","iopub.execute_input":"2025-10-20T19:41:32.937117Z","iopub.status.idle":"2025-10-20T19:41:32.941625Z","shell.execute_reply.started":"2025-10-20T19:41:32.937089Z","shell.execute_reply":"2025-10-20T19:41:32.940707Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"EPOCHS       = 15\nBATCH_SIZE   = 32\nLR           = 1e-5\nSEG_DUR      = 2.0\nPOS_MARGIN   = 0.2\nNEG_RATIO    = 1.0\nVAL_SPLIT    = 0.15\nWINDOW_SEC   = 1.5\nHOP_SEC      = 0.25\nSEED         = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:32.944235Z","iopub.execute_input":"2025-10-20T19:41:32.944632Z","iopub.status.idle":"2025-10-20T19:41:32.968994Z","shell.execute_reply.started":"2025-10-20T19:41:32.944593Z","shell.execute_reply":"2025-10-20T19:41:32.967813Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FORCE_CPU = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:32.969969Z","iopub.execute_input":"2025-10-20T19:41:32.970314Z","iopub.status.idle":"2025-10-20T19:41:32.985829Z","shell.execute_reply.started":"2025-10-20T19:41:32.970283Z","shell.execute_reply":"2025-10-20T19:41:32.984704Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"if FORCE_CPU:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() and not FORCE_CPU else \"cpu\")\nAMP_ENABLED = (DEVICE.type == \"cuda\")\nPIN_MEMORY = AMP_ENABLED","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:32.986760Z","iopub.execute_input":"2025-10-20T19:41:32.987102Z","iopub.status.idle":"2025-10-20T19:41:33.004924Z","shell.execute_reply.started":"2025-10-20T19:41:32.987066Z","shell.execute_reply":"2025-10-20T19:41:33.003867Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def autocast_ctx():\n    return torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=AMP_ENABLED)\n\nprint(\"Device:\", DEVICE, \"| AMP:\", AMP_ENABLED, \"| pin_memory:\", PIN_MEMORY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:33.005965Z","iopub.execute_input":"2025-10-20T19:41:33.006261Z","iopub.status.idle":"2025-10-20T19:41:33.023778Z","shell.execute_reply.started":"2025-10-20T19:41:33.006237Z","shell.execute_reply":"2025-10-20T19:41:33.022949Z"}},"outputs":[{"name":"stdout","text":"Device: cpu | AMP: False | pin_memory: False\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def set_seed(seed=SEED):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nset_seed(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:33.024979Z","iopub.execute_input":"2025-10-20T19:41:33.025339Z","iopub.status.idle":"2025-10-20T19:41:33.049078Z","shell.execute_reply.started":"2025-10-20T19:41:33.025306Z","shell.execute_reply":"2025-10-20T19:41:33.048122Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _find_audio_root(p: Union[str, Path]) -> Path:\n    p = Path(p)\n    if (p/\"audio\").exists() and list((p/\"audio\").glob(\"*.opus\")):\n        return p\n    for sub in p.rglob(\"audio\"):\n        if list(sub.glob(\"*.opus\")):\n            return sub.parent\n    raise FileNotFoundError(f\"Не нашёл audio/*.opus внутри {p}\")\n\ndef list_valid_opus_fast(root_dir: Path, check_size: bool = False, min_bytes: int = 2048):\n    audio_dir = Path(root_dir) / \"audio\"\n    keep, skipped = [], 0\n    for name in os.listdir(audio_dir):\n        if not name.endswith(\".opus\"):\n            continue\n        if name.startswith(\"._\"):\n            skipped += 1\n            continue\n        if check_size:\n            p = audio_dir / name\n            try:\n                if p.stat().st_size < min_bytes:\n                    skipped += 1\n                    continue\n            except Exception:\n                skipped += 1\n                continue\n        keep.append(str(audio_dir / name))\n    keep.sort()\n    print(f\"{audio_dir}: kept {len(keep)} files, skipped {skipped} (._*{', size<'+str(min_bytes) if check_size else ''})\")\n    return keep\n\ndef safe_load_audio_16k(path: Union[str, Path], sr_target: int = 16000) -> torch.Tensor:\n    path = str(path)\n    try:\n        wav, sr = torchaudio.load(path)\n        if wav.dim()==2 and wav.size(0)>1:\n            wav = wav.mean(dim=0, keepdim=True)\n        wav = wav.squeeze(0)\n        if sr != sr_target:\n            wav = torchaudio.functional.resample(wav, sr, sr_target)\n        wav = wav.float()\n    except Exception:\n        import soundfile as sf, librosa\n        data, sr = sf.read(path, dtype=\"float32\", always_2d=False)\n        if isinstance(data, np.ndarray) and data.ndim > 1:\n            data = data.mean(axis=1)\n        if sr != sr_target:\n            data = librosa.resample(data, orig_sr=sr, target_sr=sr_target)\n        wav = torch.from_numpy(data.astype(np.float32))\n    peak = float(wav.abs().max())\n    if peak > 0:\n        wav = wav / peak\n    return wav\n\ndef build_pos_neg_lists(train_files: List[str], word_bounds: Dict[str, List[float]]):\n    pos, neg = [], []\n    for f in train_files:\n        fid = Path(f).stem\n        if fid in word_bounds:\n            s, e = word_bounds[fid]\n            pos.append((f, (float(s), float(e))))\n        else:\n            neg.append(f)\n    return pos, neg\n\ndef get_conv_params_from_config(cfg):\n    strides = list(getattr(cfg, \"conv_stride\", [5,2,2,2,2,2,2]))\n    kernels = list(getattr(cfg, \"conv_kernel\", [10,3,3,3,3,2,2]))\n    return strides, kernels\n\ndef feat_len_from_samples(n_samples: int, strides, kernels) -> int:\n    L = n_samples\n    for k, s in zip(kernels, strides):\n        L = math.floor((L - k) / s + 1)\n        if L <= 0:\n            return 0\n    return L\n\ndef ensure_length(wav: torch.Tensor, target_len: int) -> torch.Tensor:\n    T = wav.numel()\n    if T == target_len: return wav\n    if T > target_len:  return wav[:target_len]\n    return F.pad(wav, (0, target_len - T))\n\ndef pick_positive_window(T, sr, seg_size, bounds, context_frac=0.5):\n    t0, t1 = bounds\n    p0 = max(0, min(T, int(round(t0 * sr))))\n    p1 = max(0, min(T, int(round(t1 * sr))))\n    pos_len = max(1, p1 - p0)\n    if pos_len >= seg_size:\n        c = (p0 + p1) // 2\n        left = max(0, c - seg_size // 2)\n        right = min(T, left + seg_size)\n        left = right - seg_size\n        return left, right\n    free = seg_size - pos_len\n    alpha = float(np.clip(np.random.normal(loc=context_frac, scale=0.15), 0, 1))\n    left_ctx = int(alpha * free); right_ctx = free - left_ctx\n    left = p0 - left_ctx; right = p1 + right_ctx\n    if left < 0:\n        shift = -left; left = 0; right = min(T, right + shift)\n    if right > T:\n        shift = right - T; right = T; left = max(0, left - shift)\n    if right - left != seg_size:\n        right = min(T, left + seg_size); left = max(0, right - seg_size)\n    return left, right\n\ndef pick_negative_window(T, seg_size):\n    if T <= seg_size: return 0, min(T, seg_size)\n    L = int(np.random.randint(0, T - seg_size + 1))\n    return L, L + seg_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:33.050050Z","iopub.execute_input":"2025-10-20T19:41:33.050334Z","iopub.status.idle":"2025-10-20T19:41:33.073268Z","shell.execute_reply.started":"2025-10-20T19:41:33.050311Z","shell.execute_reply":"2025-10-20T19:41:33.072101Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class SegmentKWSDataset(Dataset):\n    def __init__(self, pos_items, neg_items, seg_size_samples, sr, conv_strides, conv_kernels,\n                 pos_margin_sec=0.2, neg_ratio=1.0, seed=SEED):\n        super().__init__()\n        self.pos_items = pos_items\n        self.neg_items = neg_items\n        self.sr = sr\n        self.seg = int(seg_size_samples)\n        self.margin = float(pos_margin_sec)\n        self.strides = conv_strides\n        self.kernels = conv_kernels\n\n        rng = np.random.RandomState(seed)\n        n_pos = len(pos_items)\n        n_neg = int(math.ceil(n_pos * neg_ratio))\n        if len(neg_items) == 0:\n            raise RuntimeError(\"Нет отрицательных примеров.\")\n        neg_idx = rng.randint(0, len(neg_items), size=n_neg).tolist()\n        self.index = [(\"pos\", i) for i in range(n_pos)] + [(\"neg\", j) for j in neg_idx]\n        rng.shuffle(self.index)\n\n        self.frames = feat_len_from_samples(self.seg, self.strides, self.kernels)\n        if self.frames <= 0:\n            raise RuntimeError(\"frames<=0 — проверь SEG_DUR или conv-параметры\")\n\n    def __len__(self): return len(self.index)\n\n    def __getitem__(self, i):\n        kind, idx = self.index[i]\n        if kind == \"pos\":\n            path, (t0, t1) = self.pos_items[idx]\n            wav = safe_load_audio_16k(path); T = wav.numel()\n            L, R = pick_positive_window(T, self.sr, self.seg, (t0, t1))\n            seg = ensure_length(wav[L:R], self.seg)\n\n            seg_start = L / float(self.sr)\n            a = max(0.0, t0 - self.margin - seg_start)\n            b = max(0.0, t1 + self.margin - seg_start)\n            sa = max(0, min(self.seg - 1, int(np.floor(a * self.sr))))\n            sb = max(0, min(self.seg,     int(np.ceil (b * self.sr))))\n            def s2f(n): return feat_len_from_samples(n, self.strides, self.kernels)\n            A = max(0, min(self.frames, s2f(sa)))\n            B = max(0, min(self.frames, s2f(sb)))\n            if B <= A: B = min(self.frames, A + 1)\n\n            y = torch.zeros(self.frames, dtype=torch.float32)\n            y[A:B] = 1.0\n            m = torch.ones(self.frames, dtype=torch.bool)\n        else:\n            path = self.neg_items[idx]\n            wav = safe_load_audio_16k(path); T = wav.numel()\n            L, R = pick_negative_window(T, self.seg)\n            seg = ensure_length(wav[L:R], self.seg)\n            y = torch.zeros(self.frames, dtype=torch.float32)\n            m = torch.ones(self.frames, dtype=torch.bool)\n\n        return {\"input_values\": seg, \"labels\": y, \"frame_mask\": m}\n\ndef collate_segments(batch):\n    return {\n        \"input_values\": torch.stack([b[\"input_values\"] for b in batch], 0),\n        \"labels\":       torch.stack([b[\"labels\"]       for b in batch], 0),\n        \"frame_mask\":   torch.stack([b[\"frame_mask\"]   for b in batch], 0),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:33.075986Z","iopub.execute_input":"2025-10-20T19:41:33.076391Z","iopub.status.idle":"2025-10-20T19:41:33.099249Z","shell.execute_reply.started":"2025-10-20T19:41:33.076354Z","shell.execute_reply":"2025-10-20T19:41:33.098229Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Wav2Vec2KWS(nn.Module):\n    def __init__(self, backbone_name=BACKBONE, dropout=0.1):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(backbone_name)\n        self.backbone = AutoModel.from_pretrained(backbone_name)\n        H = self.config.hidden_size\n        self.head = nn.Sequential(\n            nn.Conv1d(H, H, 3, padding=1),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Conv1d(H, 1, 1),\n        )\n        if hasattr(self.backbone, \"feature_extractor\"):\n            for p in self.backbone.feature_extractor.parameters():\n                p.requires_grad = False\n\n    def forward(self, input_values):\n        out = self.backbone(input_values=input_values, output_hidden_states=False)\n        x = out.last_hidden_state.transpose(1, 2)\n        logits = self.head(x).squeeze(1)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:33.100057Z","iopub.execute_input":"2025-10-20T19:41:33.100317Z","iopub.status.idle":"2025-10-20T19:41:33.122731Z","shell.execute_reply.started":"2025-10-20T19:41:33.100296Z","shell.execute_reply":"2025-10-20T19:41:33.121887Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class KwsTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        mask   = inputs.pop(\"frame_mask\")\n        logits = model(**inputs)\n        loss = F.binary_cross_entropy_with_logits(\n            logits[mask], labels[mask],\n            pos_weight=torch.tensor(2.0, device=logits.device)\n        )\n        return (loss, {\"logits\": logits}) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:33.123660Z","iopub.execute_input":"2025-10-20T19:41:33.124012Z","iopub.status.idle":"2025-10-20T19:41:33.141536Z","shell.execute_reply.started":"2025-10-20T19:41:33.123989Z","shell.execute_reply":"2025-10-20T19:41:33.140579Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_root = _find_audio_root(TRAIN_DIR)\ntest_root  = _find_audio_root(TEST_DIR)\n\ntrain_files = list_valid_opus_fast(train_root, check_size=False)\ntest_files  = list_valid_opus_fast(test_root,  check_size=False)\n\nwith open(train_root / \"word_bounds.json\", \"r\", encoding=\"utf-8\") as f:\n    word_bounds = json.load(f)\n\npos_items, neg_items = build_pos_neg_lists(train_files, word_bounds)\npos_tr, pos_dev = train_test_split(pos_items, test_size=VAL_SPLIT, random_state=SEED)\nneg_tr, neg_dev = train_test_split(neg_items, test_size=VAL_SPLIT, random_state=SEED)\n\nsr = 16000\nseg_len = int(round(SEG_DUR * sr))\n\n_tmp_cfg = AutoConfig.from_pretrained(BACKBONE)\nstrides, kernels = get_conv_params_from_config(_tmp_cfg)\n\ntrain_ds = SegmentKWSDataset(pos_tr, neg_tr, seg_len, sr, strides, kernels,\n                             pos_margin_sec=POS_MARGIN, neg_ratio=NEG_RATIO, seed=SEED)\ndev_ds   = SegmentKWSDataset(pos_dev, neg_dev, seg_len, sr, strides, kernels,\n                             pos_margin_sec=POS_MARGIN, neg_ratio=1.0, seed=SEED+1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:33.142513Z","iopub.execute_input":"2025-10-20T19:41:33.143295Z","iopub.status.idle":"2025-10-20T19:41:43.802707Z","shell.execute_reply.started":"2025-10-20T19:41:33.143266Z","shell.execute_reply":"2025-10-20T19:41:43.801876Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vseros-audio-task/train_data/train_opus/audio: kept 90000 files, skipped 90000 (._*)\n/kaggle/input/vseros-audio-task/test_data/test_opus/audio: kept 27000 files, skipped 27000 (._*)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384dc2962fc84db0bd8ff0fad51500e0"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"model = Wav2Vec2KWS(BACKBONE).to(DEVICE)\n\nargs = TrainingArguments(\n    output_dir=OUT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LR,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.06,\n    save_total_limit=1,\n    fp16=AMP_ENABLED,\n    dataloader_pin_memory=PIN_MEMORY,\n    dataloader_num_workers=4,\n    logging_steps=50,\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    remove_unused_columns=False,\n    label_names=[\"labels\",\"frame_mask\"],\n    weight_decay=0.01,\n    seed=SEED,\n)\n\ntrainer = KwsTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    eval_dataset=dev_ds,\n    data_collator=collate_segments,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:41:43.803556Z","iopub.execute_input":"2025-10-20T19:41:43.803845Z","execution_failed":"2025-10-20T19:48:02.390Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edbe5c64878548d6815949104df4fac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='7173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   5/7173 03:59 < 158:40:59, 0.01 it/s, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"OUT_PATH = Path(OUT_DIR); OUT_PATH.mkdir(parents=True, exist_ok=True)\nWIN_BATCH = 128 if DEVICE.type == \"cuda\" else 64\n\n@torch.no_grad()\ndef score_file_batched(model: nn.Module, wav: torch.Tensor,\n                       win_sec=WINDOW_SEC, hop_sec=HOP_SEC, sr=16000,\n                       batch_windows: int = WIN_BATCH) -> float:\n    \"\"\"Максимальная вероятность по всем окнам записи.\"\"\"\n    model.eval()\n    T = wav.numel()\n    win = int(round(win_sec * sr))\n    hop = int(round(hop_sec * sr))\n    starts = list(range(0, max(1, T - win + 1), hop))\n    if T <= win:\n        starts = [0]\n\n    device = next(model.parameters()).device\n    best = 0.0\n    for i in range(0, len(starts), batch_windows):\n        b_st = starts[i:i + batch_windows]\n        xb = torch.stack([ensure_length(wav[s:s+win], win) for s in b_st], 0).to(device)\n        with autocast_ctx():\n            logits = trainer.model(xb)\n            probs  = torch.sigmoid(logits).amax(dim=1)\n        b_max = float(probs.max().item())\n        if b_max > best:\n            best = b_max\n    return best\n\n@torch.no_grad()\ndef collect_dev_scores_fast(model, pos_files, neg_files):\n    pos_scores, neg_scores = [], []\n    for path, _ in tqdm(pos_files, desc=f\"DEV pos (W={WINDOW_SEC})\"):\n        pos_scores.append(score_file_batched(model, safe_load_audio_16k(path)))\n    for path in tqdm(neg_files, desc=f\"DEV neg (W={WINDOW_SEC})\"):\n        neg_scores.append(score_file_batched(model, safe_load_audio_16k(path)))\n    return np.asarray(pos_scores, np.float32), np.asarray(neg_scores, np.float32)\n\ndef threshold_search_fast(pos_scores, neg_scores, far_cap=None,\n                          lo=0.05, hi=0.995, steps=191):\n    thr_grid = np.linspace(lo, hi, steps, dtype=np.float32)\n    Ppos = (pos_scores[None, :] >= thr_grid[:, None])\n    Pneg = (neg_scores[None, :] >= thr_grid[:, None])\n    npos, nneg = len(pos_scores), len(neg_scores)\n\n    TP = Ppos.sum(1); FN = npos - TP\n    FP = Pneg.sum(1); TN = nneg - FP\n    FAR = FP / max(1, nneg)\n    FRR = FN / max(1, npos)\n    TPR = 1.0 - FRR; TNR = 1.0 - FAR\n    score = 2.0 / (1.0/np.clip(TPR,1e-9,None) + 1.0/np.clip(TNR,1e-9,None))\n\n    if far_cap is not None:\n        mask = FAR <= float(far_cap)\n        if mask.any():\n            i = int(np.argmax(score[mask])); idx = int(np.flatnonzero(mask)[i])\n        else:\n            idx = int(np.argmax(score))\n    else:\n        idx = int(np.argmax(score))\n\n    thr = float(thr_grid[idx])\n    best = dict(score=float(score[idx]), FAR=float(FAR[idx]), FRR=float(FRR[idx]))\n    return thr, best","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-20T19:48:02.390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n→ scoring DEV (batched windows)…\")\npos_scores, neg_scores = collect_dev_scores_fast(trainer.model, pos_dev, neg_dev)\nnp.save(OUT_PATH/\"dev_pos_scores.npy\", pos_scores)\nnp.save(OUT_PATH/\"dev_neg_scores.npy\", neg_scores)\n\nthr, best = threshold_search_fast(pos_scores, neg_scores, far_cap=None)\nprint(f\"[DEV] thr={thr:.6f}  score={best['score']:.4f}  FAR={best['FAR']:.4f}  FRR={best['FRR']:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-20T19:48:02.391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(OUT_PATH/\"dev_thresholds.json\",\"w\") as w:\n    json.dump({\"thr\": thr, \"stats\": best,\n               \"window_sec\": WINDOW_SEC, \"hop_sec\": HOP_SEC}, w, indent=2)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-20T19:48:02.391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def infer_test_and_cache(model, files, cache_dir: Path = OUT_PATH, reuse_if_exists: bool = True):\n    ids_path    = cache_dir / \"test_ids.npy\"\n    scores_path = cache_dir / \"test_scores.npy\"\n\n    if reuse_if_exists and ids_path.exists() and scores_path.exists():\n        ids    = np.load(ids_path, allow_pickle=True)\n        scores = np.load(scores_path).astype(np.float32)\n        print(\"✓ loaded test cache:\", ids_path.name, \"|\", scores_path.name)\n        return ids.tolist(), scores\n\n    ids, scores = [], []\n    t0 = time.time()\n    for f in tqdm(files, desc=f\"TEST (batched, W={WINDOW_SEC})\"):\n        ids.append(Path(f).stem)\n        scores.append(score_file_batched(trainer.model, safe_load_audio_16k(f)))\n    tps = time.time() - t0\n    print(f\"✓ TEST throughput: {len(files)/max(1e-6,tps):.2f} files/sec\")\n\n    scores = np.asarray(scores, np.float32)\n    np.save(ids_path, np.asarray(ids, dtype=object))\n    np.save(scores_path, scores)\n    print(\"✓ saved test cache:\", ids_path.name, \"|\", scores_path.name)\n    return ids, scores","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-20T19:48:02.391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids, test_scores = infer_test_and_cache(trainer.model, test_files, OUT_PATH, reuse_if_exists=True)\nlabels = (test_scores >= thr).astype(np.int32)\n\nsub_path = OUT_PATH / f\"submission_thr_{thr:.6f}.csv\"\npd.DataFrame({\"id\": ids, \"label\": labels}).to_csv(sub_path, index=False)\nprint(\"✓ saved submission:\", sub_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-20T19:48:02.391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_submission_from_cache(new_thr: float,\n                               ids_path=OUT_PATH/\"test_ids.npy\",\n                               scores_path=OUT_PATH/\"test_scores.npy\",\n                               out_path=None):\n    ids = np.load(ids_path, allow_pickle=True)\n    scores = np.load(scores_path).astype(np.float32)\n    out = Path(out_path or (OUT_PATH / f\"submission_from_cache_thr_{float(new_thr):.6f}.csv\"))\n    pd.DataFrame({\"id\": ids, \"label\": (scores >= float(new_thr)).astype(np.int32)}).to_csv(out, index=False)\n    print(\"✓ saved:\", out)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-20T19:48:02.391Z"}},"outputs":[],"execution_count":null}]}