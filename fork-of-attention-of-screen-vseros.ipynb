{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13506919,"sourceType":"datasetVersion","datasetId":8575640}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TEST MODE","metadata":{}},{"cell_type":"code","source":"# 30 min + 30 + 10 + ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:00.530947Z","iopub.execute_input":"2025-10-27T20:38:00.531211Z","iopub.status.idle":"2025-10-27T20:38:00.535557Z","shell.execute_reply.started":"2025-10-27T20:38:00.531190Z","shell.execute_reply":"2025-10-27T20:38:00.534873Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"TEST_MODE=True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:00.540156Z","iopub.execute_input":"2025-10-27T20:38:00.540336Z","iopub.status.idle":"2025-10-27T20:38:00.549639Z","shell.execute_reply.started":"2025-10-27T20:38:00.540323Z","shell.execute_reply":"2025-10-27T20:38:00.548959Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Сиды","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nimport os\n\nseed=105\n\nos.environ['PYTHONHASHSEED']=str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:00.552028Z","iopub.execute_input":"2025-10-27T20:38:00.552268Z","iopub.status.idle":"2025-10-27T20:38:00.565622Z","shell.execute_reply.started":"2025-10-27T20:38:00.552254Z","shell.execute_reply":"2025-10-27T20:38:00.564933Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Импорты","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import StratifiedKFold, train_test_split, KFold\nfrom catboost import CatBoostClassifier, Pool, CatBoostRegressor\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:00.566745Z","iopub.execute_input":"2025-10-27T20:38:00.567274Z","iopub.status.idle":"2025-10-27T20:38:13.110512Z","shell.execute_reply.started":"2025-10-27T20:38:00.567256Z","shell.execute_reply":"2025-10-27T20:38:13.109857Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Загрузка данных","metadata":{}},{"cell_type":"code","source":"train_stat=pd.read_parquet('/kaggle/input/attention-on-screen/video_stat (1).parquet')\ntrain=pd.read_parquet('/kaggle/input/attention-on-screen/train (6).parquet')\ntest=pd.read_parquet('/kaggle/input/attention-on-screen/test (1).parquet')\nsample=pd.read_csv('/kaggle/input/attention-on-screen/sample_submission (14).csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:13.111516Z","iopub.execute_input":"2025-10-27T20:38:13.111869Z","iopub.status.idle":"2025-10-27T20:38:38.619760Z","shell.execute_reply.started":"2025-10-27T20:38:13.111851Z","shell.execute_reply":"2025-10-27T20:38:38.619165Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"num_feats=train.select_dtypes(include='number').columns.tolist()\n\ncat_feats=train.select_dtypes(include='object').columns.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:38.620451Z","iopub.execute_input":"2025-10-27T20:38:38.620643Z","iopub.status.idle":"2025-10-27T20:38:39.909050Z","shell.execute_reply.started":"2025-10-27T20:38:38.620628Z","shell.execute_reply":"2025-10-27T20:38:39.908439Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"num2_feats=train_stat.select_dtypes(include='number').columns.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:39.910414Z","iopub.execute_input":"2025-10-27T20:38:39.910662Z","iopub.status.idle":"2025-10-27T20:38:39.953075Z","shell.execute_reply.started":"2025-10-27T20:38:39.910644Z","shell.execute_reply":"2025-10-27T20:38:39.952366Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"if TEST_MODE:\n    train=train[:3_000_000]\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:39.953776Z","iopub.execute_input":"2025-10-27T20:38:39.954016Z","iopub.status.idle":"2025-10-27T20:38:39.957947Z","shell.execute_reply.started":"2025-10-27T20:38:39.953999Z","shell.execute_reply":"2025-10-27T20:38:39.957263Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:39.958771Z","iopub.execute_input":"2025-10-27T20:38:39.958995Z","iopub.status.idle":"2025-10-27T20:38:39.993341Z","shell.execute_reply.started":"2025-10-27T20:38:39.958971Z","shell.execute_reply":"2025-10-27T20:38:39.992797Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3000000 entries, 0 to 2999999\nData columns (total 6 columns):\n #   Column           Dtype                        \n---  ------           -----                        \n 0   event_timestamp  datetime64[ms, Europe/Moscow]\n 1   user_id          object                       \n 2   region           object                       \n 3   city             object                       \n 4   video_id         object                       \n 5   watchtime        int64                        \ndtypes: datetime64[ms, Europe/Moscow](1), int64(1), object(4)\nmemory usage: 137.3+ MB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Joining stuff","metadata":{}},{"cell_type":"code","source":"#train_full=train.merge(train_stat, on=['video_id'], how='left')\n\ntest_full=test.merge(train_stat, on=['video_id'], how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:39.994027Z","iopub.execute_input":"2025-10-27T20:38:39.994245Z","iopub.status.idle":"2025-10-27T20:38:41.242679Z","shell.execute_reply.started":"2025-10-27T20:38:39.994221Z","shell.execute_reply":"2025-10-27T20:38:41.241783Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#train_full.to_csv('for_analysis.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:41.243714Z","iopub.execute_input":"2025-10-27T20:38:41.244004Z","iopub.status.idle":"2025-10-27T20:38:41.247549Z","shell.execute_reply.started":"2025-10-27T20:38:41.243982Z","shell.execute_reply":"2025-10-27T20:38:41.246827Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Still EDA","metadata":{}},{"cell_type":"code","source":"#v_duration                               0.004272\n#v_likes                                  0.001689","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:41.248348Z","iopub.execute_input":"2025-10-27T20:38:41.249114Z","iopub.status.idle":"2025-10-27T20:38:41.272194Z","shell.execute_reply.started":"2025-10-27T20:38:41.249086Z","shell.execute_reply":"2025-10-27T20:38:41.271555Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# 1) Keep only train rows whose user is present in test\n#users_in_test = pd.Index(test['user_id'].dropna().unique())\n#train_new = train[train['user_id'].isin(users_in_test)].copy()\n\n#print(f\"train rows:     {len(train):,}\")\n#print(f\"train_new rows: {len(train_new):,} ({len(train_new)/len(train):.2%} of train)\\n\")\n\n# 2) Helper to report both unique-level and row-level coverage\ndef coverage_stats(train_series: pd.Series, test_series: pd.Series, name: str):\n    tr_u = pd.Index(train_series.dropna().unique())\n    te_u = pd.Index(test_series.dropna().unique())\n    n_common_u = tr_u.intersection(te_u).size\n    pct_common_u = n_common_u / te_u.size if te_u.size else np.nan\n\n    # Row-level: how many test rows have a value that exists in train_new\n    covered_rows = test_series.isin(tr_u).sum()\n    pct_rows = covered_rows / len(test_series)\n    unseen_rows = len(test_series) - covered_rows\n\n    print(f\"{name}:\")\n    print(f\"  test unique:          {te_u.size:,}\")\n    print(f\"  common unique:        {n_common_u:,}  ({pct_common_u:.2%})\")\n    print(f\"  test rows covered:    {covered_rows:,}  ({pct_rows:.2%})\")\n    print(f\"  unseen test rows:     {unseen_rows:,}  ({1 - pct_rows:.2%})\\n\")\n\n# 3) Re-check coverage after filtering train -> train_new\n#coverage_stats(train_new['event_timestamp'], test['event_timestamp'], \"event_timestamp\")\n#coverage_stats(train_new['video_id'],         test['video_id'],         \"video_id\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:41.275167Z","iopub.execute_input":"2025-10-27T20:38:41.275472Z","iopub.status.idle":"2025-10-27T20:38:41.286798Z","shell.execute_reply.started":"2025-10-27T20:38:41.275446Z","shell.execute_reply":"2025-10-27T20:38:41.286185Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"train_full_new=train.merge(train_stat, on=['video_id'], how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:41.287539Z","iopub.execute_input":"2025-10-27T20:38:41.287727Z","iopub.status.idle":"2025-10-27T20:38:43.932408Z","shell.execute_reply.started":"2025-10-27T20:38:41.287712Z","shell.execute_reply":"2025-10-27T20:38:43.931782Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"useless=['v_cr_click_dislike_7_days','v_cr_click_dislike_30_days',\n         'v_avg_watchtime_7_day','v_avg_watchtime_30_day',\n         'v_frac_avg_watchtime_7_day_duration','v_frac_avg_watchtime_30_day_duration',\n         'v_month_views','v_week_views',\n         'v_cr_click_like_7_days','v_cr_click_like_30_days',\n         'v_cr_click_comment_7_days','v_cr_click_comment_30_days',\n         'v_cr_click_long_view_7_days','v_cr_click_long_view_30_days',\n         'v_category_popularity_percent_30_days',\n         'v_long_views_7_days','v_long_views_30_days',\n         'v_is_deleted','v_is_hidden','row_number'\n         \n        # 'user_id','category_id','author_id', 'video_id',\n         #'description','title',\n        # 'region','city'\n        ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:43.933102Z","iopub.execute_input":"2025-10-27T20:38:43.933305Z","iopub.status.idle":"2025-10-27T20:38:43.937510Z","shell.execute_reply.started":"2025-10-27T20:38:43.933291Z","shell.execute_reply":"2025-10-27T20:38:43.936763Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_full_new1=train_full_new.drop(columns=useless)\n\ntest_full_new1=test_full.drop(columns=useless)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:43.938218Z","iopub.execute_input":"2025-10-27T20:38:43.938414Z","iopub.status.idle":"2025-10-27T20:38:44.939494Z","shell.execute_reply.started":"2025-10-27T20:38:43.938399Z","shell.execute_reply":"2025-10-27T20:38:44.938854Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport numpy as np\nimport pandas as pd\n\ndef make_long_watch_flag(df, duration_col='v_duration', watch_col='watchtime', strict=True, unit='s'):\n    # 1) to seconds (if needed) and numeric\n    dur = pd.to_numeric(df[duration_col], errors='coerce')\n    wt  = pd.to_numeric(df[watch_col],  errors='coerce')\n    if unit == 'ms':\n        dur = dur / 1000.0\n        wt  = wt  / 1000.0\n\n    # 2) thresholds\n    thr = np.where(dur > 300.0, dur * 0.25, 30.0)  # 300s = 5 min\n\n    # 3) validity mask and comparison\n    valid = dur.notna() & wt.notna() & (dur > 0) & (wt >= 0)\n    cmp = (wt > thr) if strict else (wt >= thr)\n\n    # 4) build nullable Int8 target safely (pandas Series + pd.NA)\n    y = pd.Series(cmp, index=df.index)\n    y.loc[~valid] = pd.NA\n    df['y_long'] = y.astype('Int8')  # 0/1 with NA\n\n    # optional: keep threshold for debugging\n    df['longwatch_threshold_sec'] = pd.Series(np.where(valid, thr, np.nan), index=df.index)\n\n    return df\n\n\n# Example:\ntrain_full_new1 = make_long_watch_flag(train_full_new1, 'v_duration', 'watchtime', strict=True)\n# If test has watchtime and you want the same flag there:\n# test = make_long_watch_flag(test, 'v_duration', 'watchtime', strict=True)\n\n# Quick sanity check\nprint(train_full_new1['y_long'].value_counts(dropna=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:44.940150Z","iopub.execute_input":"2025-10-27T20:38:44.940350Z","iopub.status.idle":"2025-10-27T20:38:45.602894Z","shell.execute_reply.started":"2025-10-27T20:38:44.940334Z","shell.execute_reply":"2025-10-27T20:38:45.602046Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/1262398379.py:24: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n  y.loc[~valid] = pd.NA\n","output_type":"stream"},{"name":"stdout","text":"y_long\n1       1474075\n0       1430935\n<NA>      94990\nName: count, dtype: Int64\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Вариант 1 (рекомендую)\nbefore = len(train_full_new1)\ntrain_full_new1 = train_full_new1.dropna(subset=['y_long']).copy()\ntrain_full_new1['y_long'] = train_full_new1['y_long'].astype('int8')\nprint(f\"Dropped {before - len(train_full_new1)} rows\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:45.603725Z","iopub.execute_input":"2025-10-27T20:38:45.603988Z","iopub.status.idle":"2025-10-27T20:38:48.770048Z","shell.execute_reply.started":"2025-10-27T20:38:45.603967Z","shell.execute_reply":"2025-10-27T20:38:48.769353Z"}},"outputs":[{"name":"stdout","text":"Dropped 94990 rows\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nВзять title и получить эмбеды \nВсякие лаг фичи, фурье фичи, по таймстемпу, цикличные фичи а также бинарные фичи по типу всякие праздники по типу нг \nпо id создателей сделать агреггированные фичи \n\nДля видео продолжительностью более 5 минут: просмотр считается долгим, если пользователь посмотрел больше 25% длительности видео.\nДля видео менее или равные 5 минут: просмотр считается долгим, если пользователь посмотрел более 30 секунд.\n\nРазница между event_timestamp и v_pub_timestamp\n\nНадо попробовать что Даня сказал с SSL и вот сделать тут также с текстом!\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:48.770845Z","iopub.execute_input":"2025-10-27T20:38:48.771092Z","iopub.status.idle":"2025-10-27T20:38:48.777024Z","shell.execute_reply.started":"2025-10-27T20:38:48.771072Z","shell.execute_reply":"2025-10-27T20:38:48.776441Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'\\nВзять title и получить эмбеды \\nВсякие лаг фичи, фурье фичи, по таймстемпу, цикличные фичи а также бинарные фичи по типу всякие праздники по типу нг \\nпо id создателей сделать агреггированные фичи \\n\\nДля видео продолжительностью более 5 минут: просмотр считается долгим, если пользователь посмотрел больше 25% длительности видео.\\nДля видео менее или равные 5 минут: просмотр считается долгим, если пользователь посмотрел более 30 секунд.\\n\\nРазница между event_timestamp и v_pub_timestamp\\n\\nНадо попробовать что Даня сказал с SSL и вот сделать тут также с текстом!\\n'"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"'''\nХорошие фичи\n\nv_duration                               0.004272\nv_likes                                  0.001689\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:48.777812Z","iopub.execute_input":"2025-10-27T20:38:48.778093Z","iopub.status.idle":"2025-10-27T20:38:48.814364Z","shell.execute_reply.started":"2025-10-27T20:38:48.778067Z","shell.execute_reply":"2025-10-27T20:38:48.813796Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'\\nХорошие фичи\\n\\nv_duration                               0.004272\\nv_likes                                  0.001689\\n'"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"'''\nТест и трейн один и тот же день. Надо об этом учитывать и делать фичи именно сильно дающие инфу о 2024-08-10 \nА также получается надо минутные цикличные или часовые(Надо построить спец график(есть имя ему) )\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:48.815122Z","iopub.execute_input":"2025-10-27T20:38:48.815513Z","iopub.status.idle":"2025-10-27T20:38:48.829750Z","shell.execute_reply.started":"2025-10-27T20:38:48.815491Z","shell.execute_reply":"2025-10-27T20:38:48.829220Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'\\nТест и трейн один и тот же день. Надо об этом учитывать и делать фичи именно сильно дающие инфу о 2024-08-10 \\nА также получается надо минутные цикличные или часовые(Надо построить спец график(есть имя ему) )\\n'"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"#Регрессия или классификация? Склоняюсь к регрессии и потом просто переводе в нули и единички ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:48.830330Z","iopub.execute_input":"2025-10-27T20:38:48.830554Z","iopub.status.idle":"2025-10-27T20:38:48.842454Z","shell.execute_reply.started":"2025-10-27T20:38:48.830539Z","shell.execute_reply":"2025-10-27T20:38:48.841834Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom typing import Tuple, Dict, Any, Optional\n\n# --------- helpers & regex ----------\n_RX_EMOJI = re.compile(\n    \"[\" \"\\U0001F300-\\U0001F64F\" \"\\U0001F680-\\U0001F6FF\" \"\\U0001F900-\\U0001F9FF\"\n    \"\\u2600-\\u26FF\\u2700-\\u27BF\" \"]+\"\n)\n\n# curated keyword hints (RU+EN)\n_HINT_SHORT  = r\"(?:\\bshorts?\\b|шортс|тизер|teaser|трейлер|trailer|highlights?|нарезка|clip\\b|клип\\b)\"\n_HINT_LONG   = r\"(?:стрим|прямой эфир|запись стрима|live|stream|подкаст|podcast|full(?:\\s+version|\\s+episode)?|полная верс|полный выпуск|longplay)\"\n_HINT_EDU    = r\"(?:\\bкак\\b|how to|tutorial|урок|гайд|инструкц|обзор|review)\"\n_HINT_OFFCL  = r\"(?:official|официальн)\"\n_RX_PART_EP  = re.compile(r\"(?:\\bpart\\s*\\d+\\b|\\bчаст[ьи]\\s*\\d+\\b|\\bs\\d{1,2}e\\d{1,2}\\b)\", re.I)\n_RX_YEAR     = re.compile(r\"\\b(19\\d{2}|20\\d{2}|210\\d)\\b\")\n_RX_DATE_DDMM = re.compile(r\"\\b([0-3]?\\d)[\\./\\-]([01]?\\d)\\b\")  # 12.08 or 12-8\n\n# time mentions in title\n_RX_HHMMSS = re.compile(r\"\\b(\\d{1,2}):([0-5]\\d)(?::([0-5]\\d))?\\b\")\n_RX_HOURS  = re.compile(r\"(\\d+)\\s*(?:час|часа|часов|h|hr|hour)s?\\b\", re.I)\n_RX_MIN    = re.compile(r\"(\\d+)\\s*(?:мин|минут|minute|m)\\b\", re.I)\n_RX_SEC    = re.compile(r\"(\\d+)\\s*(?:сек|секунд|second|s)\\b\", re.I)\n\ndef _normalize_title(s: pd.Series) -> pd.Series:\n    s = s.fillna(\"\").astype(str)\n    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n    return s\n\ndef _extract_title_time_sec(title: str) -> float:\n    \"\"\"Parse a rough duration from title; return seconds or np.nan.\"\"\"\n    if not title:\n        return np.nan\n    t = title.lower()\n\n    # HH:MM(:SS)\n    m = _RX_HHMMSS.search(t)\n    if m:\n        h = int(m.group(1))\n        mm = int(m.group(2))\n        ss = int(m.group(3)) if m.group(3) else 0\n        return h*3600 + mm*60 + ss\n\n    total_sec = 0.0\n    found = False\n    for g in _RX_HOURS.findall(t):\n        total_sec += float(g) * 3600; found = True\n    for g in _RX_MIN.findall(t):\n        total_sec += float(g) * 60;   found = True\n    for g in _RX_SEC.findall(t):\n        total_sec += float(g);        found = True\n    return total_sec if found else np.nan\n\n# --------- main builders ----------\ndef _fast_title_core(df: pd.DataFrame, title_col: str = \"title\") -> pd.DataFrame:\n    out = df.copy()\n    t_orig = out[title_col].fillna(\"\").astype(str)\n    t = _normalize_title(out[title_col]).str.lower()\n\n    # lengths\n    out[\"title_len_char\"] = t.str.len().astype(\"int32\")\n    words = t.str.split()\n    out[\"title_n_words\"]  = words.str.len().fillna(0).astype(\"int16\")\n    out[\"title_mean_word_len\"] = (\n        np.where(out[\"title_n_words\"]>0,\n                 out[\"title_len_char\"]/out[\"title_n_words\"].clip(lower=1),\n                 0).astype(\"float32\")\n    )\n\n    # counts/ratios (use original to preserve case/punct)\n    denom = out[\"title_len_char\"].replace(0, np.nan)\n    out[\"title_digits_ratio\"] = t_orig.str.count(r\"\\d\").div(denom).fillna(0).astype(\"float32\")\n    out[\"title_punct_ratio\"]  = t_orig.str.count(r\"[^\\w\\s]\", flags=re.UNICODE).div(denom).fillna(0).astype(\"float32\")\n    out[\"title_excl_cnt\"]     = t_orig.str.count(\"!\").astype(\"int16\")\n    out[\"title_quest_cnt\"]    = t_orig.str.count(r\"\\?\").astype(\"int16\")\n    out[\"title_hash_cnt\"]     = t_orig.str.count(\"#\").astype(\"int16\")\n    out[\"title_at_cnt\"]       = t_orig.str.count(\"@\").astype(\"int16\")\n    out[\"title_emoji_cnt\"]    = t_orig.apply(lambda x: len(_RX_EMOJI.findall(x))).astype(\"int16\")\n\n    # scripts / language hints\n    has_cyr = t_orig.str.contains(r\"[А-Яа-яЁё]\")\n    has_lat = t_orig.str.contains(r\"[A-Za-z]\")\n    out[\"title_has_cyr\"]  = has_cyr.astype(\"int8\")\n    out[\"title_has_lat\"]  = has_lat.astype(\"int8\")\n    out[\"title_has_mixed\"] = (has_cyr & has_lat).astype(\"int8\")\n\n    # curated keyword flags (compiled patterns are already fast)\n    out[\"title_is_shorts\"]   = t.str.contains(_HINT_SHORT,  regex=True).astype(\"int8\")\n    out[\"title_is_longform\"] = t.str.contains(_HINT_LONG,   regex=True).astype(\"int8\")\n    out[\"title_is_edu\"]      = t.str.contains(_HINT_EDU,    regex=True).astype(\"int8\")\n    out[\"title_is_official\"] = t.str_contains(_HINT_OFFCL,  regex=True).astype(\"int8\") if hasattr(t, 'str_contains') else t.str.contains(_HINT_OFFCL, regex=True).astype(\"int8\")\n\n    out[\"title_has_part_ep\"] = t.str.contains(_RX_PART_EP).astype(\"int8\")\n    out[\"title_has_year\"]    = t.str.contains(_RX_YEAR).astype(\"int8\")\n    out[\"title_has_date\"]    = t.str.contains(_RX_DATE_DDMM).astype(\"int8\")\n\n    # time mention from title\n    out[\"title_time_sec\"]    = t.apply(_extract_title_time_sec).astype(\"float32\")\n    out[\"title_has_time\"]    = out[\"title_time_sec\"].notna().astype(\"int8\")\n\n    # starts-with-number / all-caps token presence\n    out[\"title_starts_num\"]  = t.str.match(r\"^\\s*\\d\").fillna(False).astype(\"int8\")\n    out[\"title_has_topN\"]    = t.str.contains(r\"\\btop[- ]?\\d+\\b|\\bтоп[- ]?\\d+\\b\", regex=True).astype(\"int8\")\n\n    # binned lengths (cheap & strong for trees)\n    out[\"title_len_char_bin\"] = pd.cut(out[\"title_len_char\"], [-1,20,35,60,100,10_000], labels=[0,1,2,3,4]).astype(\"int8\")\n    out[\"title_n_words_bin\"]  = pd.cut(out[\"title_n_words\"],  [-1,3,6,12,20,10_000],   labels=[0,1,2,3,4]).astype(\"int8\")\n\n    return out\n\ndef _title_freq_from_train(train_norm_titles: pd.Series) -> pd.Series:\n    vc = train_norm_titles.value_counts()\n    return vc\n\ndef add_fast_title_features(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    title_col: str = \"title\",\n    v_duration_col: Optional[str] = \"v_duration\",  # seconds; set None if absent\n    add_tfidf_svd: bool = False,                   # optional tiny SVD\n    tfidf_max_features: int = 15000,\n    svd_components: int = 16,\n) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n    \"\"\"\n    Fast, high-signal title features with zero heavy deps; optional tiny TF-IDF+SVD.\n    Returns (train_out, test_out, artifacts).\n    \"\"\"\n    tr = _fast_title_core(train_df, title_col=title_col)\n    te = _fast_title_core(test_df,  title_col=title_col)\n\n    # title frequency (from train) -> map to both\n    tnorm_tr = _normalize_title(tr[title_col]).str.lower()\n    tnorm_te = _normalize_title(te[title_col]).str.lower()\n    freq = _title_freq_from_train(tnorm_tr)\n    tr[\"title_freq_train\"] = tnorm_tr.map(freq).fillna(0).astype(\"int32\")\n    te[\"title_freq_train\"] = tnorm_te.map(freq).fillna(0).astype(\"int32\")\n    tr[\"title_freq_log1p\"] = np.log1p(tr[\"title_freq_train\"]).astype(\"float32\")\n    te[\"title_freq_log1p\"] = np.log1p(te[\"title_freq_train\"]).astype(\"float32\")\n    tr[\"title_is_duplicated\"] = (tr[\"title_freq_train\"] > 1).astype(\"int8\")\n    te[\"title_is_duplicated\"] = (te[\"title_freq_train\"] > 1).astype(\"int8\")\n\n    # title vs meta duration (if you have v_duration in seconds)\n    if v_duration_col and (v_duration_col in tr.columns):\n        vd_tr = pd.to_numeric(tr[v_duration_col], errors=\"coerce\").astype(\"float32\")\n        vd_te = pd.to_numeric(te.get(v_duration_col, np.nan), errors=\"coerce\").astype(\"float32\") if v_duration_col in te.columns else pd.Series(np.nan, index=te.index, dtype=\"float32\")\n\n        # only if both present\n        for df, vd in ((tr, vd_tr), (te, vd_te)):\n            has_both = df[\"title_time_sec\"].notna() & vd.notna() & (vd > 0)\n            ratio = pd.Series(np.nan, index=df.index, dtype=\"float32\")\n            gap   = pd.Series(np.nan, index=df.index, dtype=\"float32\")\n            ratio[has_both] = (df.loc[has_both, \"title_time_sec\"] / vd.loc[has_both]).astype(\"float32\")\n            gap[has_both]   = (df.loc[has_both, \"title_time_sec\"] - vd.loc[has_both]).astype(\"float32\")\n            df[\"title_time_ratio_to_meta\"] = ratio\n            df[\"title_time_gap_to_meta\"]   = gap\n            # safe caps\n            df[\"title_time_ratio_to_meta_c\"] = df[\"title_time_ratio_to_meta\"].clip(0, 5).astype(\"float32\")\n            df[\"title_time_gap_to_meta_c\"]   = df[\"title_time_gap_to_meta\"].clip(-3*3600, 3*3600).astype(\"float32\")\n            df[\"title_time_consistent\"]      = has_both.astype(\"int8\")\n\n    artifacts: Dict[str, Any] = {\"title_freq\": freq}\n\n    # optional: tiny TF-IDF + SVD (still pretty fast, big boost)\n    if add_tfidf_svd:\n        from sklearn.feature_extraction.text import TfidfVectorizer\n        from sklearn.decomposition import TruncatedSVD\n\n        vec = TfidfVectorizer(\n            analyzer=\"char\",\n            ngram_range=(3,5),\n            max_features=tfidf_max_features,\n            min_df=2\n        )\n        X_tr = vec.fit_transform(tnorm_tr)\n        X_te = vec.transform(tnorm_te)\n\n        svd = TruncatedSVD(n_components=svd_components, random_state=42)\n        Z_tr = svd.fit_transform(X_tr).astype(\"float32\")\n        Z_te = svd.transform(X_te).astype(\"float32\")\n\n        for i in range(svd_components):\n            tr[f\"title_svd_{i:02d}\"] = Z_tr[:, i]\n            te[f\"title_svd_{i:02d}\"] = Z_te[:, i]\n\n        artifacts.update({\"tfidf_vectorizer\": vec, \"svd\": svd})\n\n    return tr, te, artifacts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:48.843241Z","iopub.execute_input":"2025-10-27T20:38:48.843466Z","iopub.status.idle":"2025-10-27T20:38:48.869260Z","shell.execute_reply.started":"2025-10-27T20:38:48.843449Z","shell.execute_reply":"2025-10-27T20:38:48.868735Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Minimal fast set (no heavy libs)\n'''\ntrain_title_fast, test_title_fast, title_arts = add_fast_title_features(\n    train_full_new1, test_full_new1,\n    title_col=\"title\",\n    v_duration_col=\"v_duration\",   # or None if not available\n    add_tfidf_svd=True            # set True for a small extra boost\n)\n\n# Peek\nprint(train_title_fast.filter(regex=\"^title_\").columns.tolist()[:30])\nprint(train_title_fast.filter(regex=\"^title_\").describe().T.head())\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:48.869843Z","iopub.execute_input":"2025-10-27T20:38:48.870000Z","iopub.status.idle":"2025-10-27T20:38:48.886536Z","shell.execute_reply.started":"2025-10-27T20:38:48.869989Z","shell.execute_reply":"2025-10-27T20:38:48.885794Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'\\ntrain_title_fast, test_title_fast, title_arts = add_fast_title_features(\\n    train_full_new1, test_full_new1,\\n    title_col=\"title\",\\n    v_duration_col=\"v_duration\",   # or None if not available\\n    add_tfidf_svd=True            # set True for a small extra boost\\n)\\n\\n# Peek\\nprint(train_title_fast.filter(regex=\"^title_\").columns.tolist()[:30])\\nprint(train_title_fast.filter(regex=\"^title_\").describe().T.head())\\n'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"import math\nfrom collections import defaultdict, deque\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional, Iterable, Tuple\n\nimport numpy as np\nimport pandas as pd\n\n\n# ============================ utils ============================\n\nEPS = 1e-6\n\ndef safe_div(a, b, default=np.nan):\n    return a / b if (b is not None and b != 0) else default\n\ndef logit(p: float) -> float:\n    p = min(max(p, EPS), 1 - EPS)\n    return math.log(p / (1 - p))\n\ndef invlogit(x: float) -> float:\n    return 1 / (1 + math.exp(-x))\n\ndef pct01(x: pd.Series) -> pd.Series:\n    \"\"\"Привести проценты к [0,1] если они заданы в [0,100].\"\"\"\n    if x.max(skipna=True) is None:\n        return x\n    mx = x.max(skipna=True)\n    return x/100.0 if (mx is not None and mx > 1.0) else x\n\ndef beta_smooth(success, total, alpha, p0):\n    return (success + alpha * p0) / (total + alpha + EPS)\n\ndef to_dt(s):\n    \"\"\"Поддержка int timestamp (sec) или уже datetime.\"\"\"\n    if np.issubdtype(s.dtype, np.integer) or np.issubdtype(s.dtype, np.floating):\n        return pd.to_datetime(s, unit=\"s\", utc=True)\n    elif np.issubdtype(s.dtype, np.datetime64):\n        return pd.to_datetime(s, utc=True)\n    else:\n        # строка\n        return pd.to_datetime(s, utc=True)\n\ndef cos_sim(a: np.ndarray, b: np.ndarray) -> float:\n    if a is None or b is None:\n        return np.nan\n    na = np.linalg.norm(a)\n    nb = np.linalg.norm(b)\n    if na < 1e-12 or nb < 1e-12:\n        return np.nan\n    return float(np.dot(a, b) / (na * nb))\n\ndef ema_update(vec_old: Optional[np.ndarray], vec_new: Optional[np.ndarray], beta: float) -> Optional[np.ndarray]:\n    if vec_new is None:\n        return vec_old\n    if vec_old is None:\n        return vec_new.copy()\n    return (1 - beta) * vec_old + beta * vec_new\n\ndef ensure_array(x):\n    if x is None:\n        return None\n    if isinstance(x, (list, tuple)):\n        return np.asarray(x, dtype=\"float32\")\n    if isinstance(x, np.ndarray):\n        return x.astype(\"float32\")\n    # если в колонке сериализованная строка — попробуем ast.literal_eval снаружи\n    return np.asarray(x, dtype=\"float32\")\n\n\n# ============================ config ============================\n\n@dataclass\nclass FeatConfig:\n    session_gap_s: int = 30 * 60  # 30 минут\n    user_L_list: Tuple[int, ...] = (1, 3, 5, 10, 30)\n    alpha_video: float = 50.0\n    alpha_author: float = 200.0\n    alpha_category: float = 500.0\n    alpha_online_small: float = 3.0     # сглаживание для user×pair/geo и т.п.\n    ema_beta: float = 0.2               # EMA для пользовательских центроидов\n    min_views_video: int = 50           # пороги backoff\n    min_views_author: int = 200\n    min_views_category: int = 1000\n    fresh_hours: int = 24               # что считаем \"свежим\" видео\n    recent_k_sim: int = 5               # k в \"user_recent_k_sim\"\n    make_time_bins: bool = True\n\n\n# ============================ main ============================\n\ndef featurize(\n    df_train: pd.DataFrame,\n    df_test: pd.DataFrame,\n    video_stat: pd.DataFrame,\n    video_emb: Optional[pd.DataFrame] = None,   # опционально: columns ['video_id','emb'] где emb—np.ndarray\n    cfg: FeatConfig = FeatConfig(),\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Возвращает (train_feat, test_feat) cо всеми перечисленными golden-фичами.\n    Ожидаемые колонки:\n      - df_train: ['event_timestamp','video_id','user_id','region','city','watchtime']\n      - df_test : ['event_timestamp','video_id','user_id','region','city']\n      - video_stat: перечисленные в условии (duration, views, long_views, cr, dates, author_id, category_id, ...)\n\n    Если передан video_emb: должен содержать по крайней мере ['video_id','emb'].\n    \"\"\"\n\n    # ---------- 0) Копии и базовые преобразования ----------\n    train = df_train.copy()\n    test  = df_test.copy()\n    vs    = video_stat.copy()\n\n    # timestamps to UTC datetime\n    train[\"event_dt\"] = to_dt(train[\"event_timestamp\"])\n    test[\"event_dt\"]  = to_dt(test[\"event_timestamp\"])\n    vs[\"v_pub_datetime\"] = to_dt(vs[\"v_pub_datetime\"])\n\n    # duration\n    vs[\"v_duration\"] = vs[\"v_duration\"].astype(float)\n\n    # просмотры по окнам\n    vs[\"views_1d\"]  = vs[\"v_day_views\"].fillna(0).astype(float)\n    vs[\"views_7d\"]  = vs[\"v_week_views\"].fillna(0).astype(float)\n    vs[\"views_30d\"] = vs[\"v_month_views\"].fillna(0).astype(float)\n\n    # long views по окнам\n    vs[\"lv_1d\"]  = vs[\"v_long_views_1_days\"].fillna(0).astype(float)\n    vs[\"lv_7d\"]  = vs[\"v_long_views_7_days\"].fillna(0).astype(float)\n    vs[\"lv_30d\"] = vs[\"v_long_views_30_days\"].fillna(0).astype(float)\n\n    # глобальный p0 по 30d (если нет — fallback 0.2)\n    sum_lv30  = vs[\"lv_30d\"].sum()\n    sum_v30   = vs[\"views_30d\"].sum()\n    p0_global = safe_div(sum_lv30, sum_v30, default=0.2)\n    p0_global = float(0.2 if (p0_global is np.nan or p0_global is None) else p0_global)\n\n    # ---------- 1) p_vid_* c бета-сглаживанием + тренды ----------\n    vs[\"p_vid_1d\"]  = beta_smooth(vs[\"lv_1d\"],  vs[\"views_1d\"],  cfg.alpha_video, p0_global)\n    vs[\"p_vid_7d\"]  = beta_smooth(vs[\"lv_7d\"],  vs[\"views_7d\"],  cfg.alpha_video, p0_global)\n    vs[\"p_vid_30d\"] = beta_smooth(vs[\"lv_30d\"], vs[\"views_30d\"], cfg.alpha_video, p0_global)\n\n    vs[\"logit_p_vid_1d\"]  = vs[\"p_vid_1d\"].clip(EPS, 1-EPS).map(logit)\n    vs[\"logit_p_vid_7d\"]  = vs[\"p_vid_7d\"].clip(EPS, 1-EPS).map(logit)\n    vs[\"logit_p_vid_30d\"] = vs[\"p_vid_30d\"].clip(EPS, 1-EPS).map(logit)\n\n    vs[\"dlogit_p_1d_7d\"]  = vs[\"logit_p_vid_1d\"]  - vs[\"logit_p_vid_7d\"]\n    vs[\"dlogit_p_7d_30d\"] = vs[\"logit_p_vid_7d\"]  - vs[\"logit_p_vid_30d\"]\n\n    # тренды просмотров\n    vs[\"trend_views_d_w\"] = np.log1p(vs[\"views_1d\"]) - np.log1p(vs[\"views_7d\"])\n    vs[\"trend_p_1d_7d\"]   = np.log(vs[\"p_vid_1d\"].clip(EPS,1)) - np.log(vs[\"p_vid_7d\"].clip(EPS,1))\n\n    # ---------- 2) Автор/категория агрегаты для backoff ----------\n    # суммарные lv/views по авторам и категориям\n    agg_author = vs.groupby(\"author_id\", dropna=False).agg(\n        lv_1d=(\"lv_1d\",\"sum\"), views_1d=(\"views_1d\",\"sum\"),\n        lv_7d=(\"lv_7d\",\"sum\"), views_7d=(\"views_7d\",\"sum\"),\n        lv_30d=(\"lv_30d\",\"sum\"), views_30d=(\"views_30d\",\"sum\"),\n    ).reset_index()\n    agg_cat = vs.groupby(\"category_id\", dropna=False).agg(\n        lv_1d=(\"lv_1d\",\"sum\"), views_1d=(\"views_1d\",\"sum\"),\n        lv_7d=(\"lv_7d\",\"sum\"), views_7d=(\"views_7d\",\"sum\"),\n        lv_30d=(\"lv_30d\",\"sum\"), views_30d=(\"views_30d\",\"sum\"),\n    ).reset_index()\n\n    for w in (\"1d\",\"7d\",\"30d\"):\n        agg_author[f\"p_author_{w}\"] = beta_smooth(agg_author[f\"lv_{w}\"], agg_author[f\"views_{w}\"], cfg.alpha_author, p0_global)\n        agg_cat[f\"p_category_{w}\"]  = beta_smooth(agg_cat[f\"lv_{w}\"],    agg_cat[f\"views_{w}\"],   cfg.alpha_category, p0_global)\n\n    vs = vs.merge(agg_author[[\"author_id\",\"p_author_1d\",\"p_author_7d\",\"p_author_30d\",\"views_1d\",\"views_7d\",\"views_30d\"]]\n                  .rename(columns={\"views_1d\":\"author_views_1d\",\"views_7d\":\"author_views_7d\",\"views_30d\":\"author_views_30d\"}),\n                  on=\"author_id\", how=\"left\")\n    vs = vs.merge(agg_cat[[\"category_id\",\"p_category_1d\",\"p_category_7d\",\"p_category_30d\",\"views_1d\",\"views_7d\",\"views_30d\"]]\n                  .rename(columns={\"views_1d\":\"cat_views_1d\",\"views_7d\":\"cat_views_7d\",\"views_30d\":\"cat_views_30d\"}),\n                  on=\"category_id\", how=\"left\")\n\n    # residuals (сложность видео относительно автора/категории)\n    vs[\"res_vid_vs_author_7d\"] = vs[\"logit_p_vid_7d\"] - vs[\"p_author_7d\"].clip(EPS,1-EPS).map(logit)\n    vs[\"res_vid_vs_cat_7d\"]    = vs[\"logit_p_vid_7d\"] - vs[\"p_category_7d\"].clip(EPS,1-EPS).map(logit)\n\n    # engagement (логиты CR по окнам)\n    def cr_logit(cols: Iterable[str], df: pd.DataFrame) -> pd.Series:\n        vals = []\n        for c in cols:\n            if c in df.columns:\n                x = pct01(df[c].astype(float))\n                x = x.clip(EPS, 1-EPS)\n                vals.append(x.map(logit))\n        if not vals:\n            return pd.Series(np.nan, index=df.index)\n        return pd.concat(vals, axis=1).mean(axis=1)\n\n    vs[\"engagement_1d\"]  = cr_logit([\"v_cr_click_like_1_days\",\"v_cr_click_comment_1_days\",\"v_cr_click_vtop_1_days\"], vs)\n    vs[\"engagement_7d\"]  = cr_logit([\"v_cr_click_like_7_days\",\"v_cr_click_comment_7_days\",\"v_cr_click_vtop_7_days\"], vs)\n    vs[\"engagement_30d\"] = cr_logit([\"v_cr_click_like_30_days\",\"v_cr_click_comment_30_days\",\"v_cr_click_vtop_30_days\"], vs)\n\n    # ---------- 3) Optional: эмбеддинги, автор/категорияные центроиды ----------\n    emb_map: Dict = {}\n    author_centroids = {}\n    category_centroids = {}\n    if video_emb is not None:\n        em = video_emb.copy()\n        # ожидаем колонки: video_id, emb (np.ndarray)\n        em[\"emb\"] = em[\"emb\"].map(ensure_array)\n        emb_map = dict(zip(em[\"video_id\"].values, em[\"emb\"].values))\n\n        # центроиды \"позитив/негатив\" по авторам/категориям через p_vid_7d как вес\n        # (быстрая приближенка без train-лейблов)\n        def weighted_centroids(key_col: str):\n            # объединяем vs с эмбеддингами\n            tmp = vs.merge(em[[\"video_id\",\"emb\"]], on=\"video_id\", how=\"inner\")\n            # веса позитивов/негативов\n            w_pos = tmp[\"p_vid_7d\"].values\n            w_neg = (1.0 - tmp[\"p_vid_7d\"].values)\n            # аккум в dict: key -> (sum_pos, wpos_sum, sum_neg, wneg_sum)\n            acc = {}\n            for k, e, wp, wn in zip(tmp[key_col].values, tmp[\"emb\"].values, w_pos, w_neg):\n                if e is None:\n                    continue\n                if k not in acc:\n                    acc[k] = [np.zeros_like(e, dtype=\"float32\"), 0.0,\n                              np.zeros_like(e, dtype=\"float32\"), 0.0]\n                acc[k][0] += e * wp; acc[k][1] += wp\n                acc[k][2] += e * wn; acc[k][3] += wn\n            out = {}\n            for k,(sp,wp,sn,wn) in acc.items():\n                cp = sp / (wp + 1e-9) if wp > 0 else None\n                cn = sn / (wn + 1e-9) if wn > 0 else None\n                out[k] = (cp, cn)\n            return out\n\n        author_centroids   = weighted_centroids(\"author_id\")\n        category_centroids = weighted_centroids(\"category_id\")\n\n    # ---------- 4) Собираем общий поток (train+test) для as-of онлайна ----------\n    train[\"is_train\"] = 1\n    test[\"is_train\"]  = 0\n    all_df = pd.concat([train, test], ignore_index=True, sort=False)\n\n    # Примержим статические видео-фичи\n    cols_keep_vs = [\n        \"video_id\",\"author_id\",\"category_id\",\"v_duration\",\"v_pub_datetime\",\n        \"views_1d\",\"views_7d\",\"views_30d\",\n        \"p_vid_1d\",\"p_vid_7d\",\"p_vid_30d\",\n        \"logit_p_vid_1d\",\"logit_p_vid_7d\",\"logit_p_vid_30d\",\n        \"dlogit_p_1d_7d\",\"dlogit_p_7d_30d\",\n        \"p_author_1d\",\"p_author_7d\",\"p_author_30d\",\n        \"p_category_1d\",\"p_category_7d\",\"p_category_30d\",\n        \"author_views_1d\",\"author_views_7d\",\"author_views_30d\",\n        \"cat_views_1d\",\"cat_views_7d\",\"cat_views_30d\",\n        \"v_avg_watchtime_1_day\",\"v_avg_watchtime_7_day\",\"v_avg_watchtime_30_day\",\n        \"v_frac_avg_watchtime_1_day_duration\",\"v_frac_avg_watchtime_7_day_duration\",\"v_frac_avg_watchtime_30_day_duration\",\n        \"engagement_1d\",\"engagement_7d\",\"engagement_30d\",\n        \"trend_views_d_w\",\"trend_p_1d_7d\",\n        \"res_vid_vs_author_7d\",\"res_vid_vs_cat_7d\",\n    ]\n    all_df = all_df.merge(vs[cols_keep_vs], on=\"video_id\", how=\"left\")\n\n    # Векторные статфичи от события\n    all_df[\"time_of_day\"] = all_df[\"event_dt\"].dt.hour.astype(\"int16\")\n    if cfg.make_time_bins:\n        # грубые бины: ночь(0-5), утро(6-11), день(12-17), вечер(18-23)\n        tod = all_df[\"time_of_day\"].values\n        all_df[\"time_bin\"] = pd.cut(tod, bins=[-1,5,11,17,24], labels=[0,1,2,3]).astype(\"int8\")\n    else:\n        all_df[\"time_bin\"] = -1\n\n    # возраст видео на момент события\n    all_df[\"age_hours\"] = (all_df[\"event_dt\"] - all_df[\"v_pub_datetime\"]).dt.total_seconds() / 3600.0\n\n    # тип порога (<=5 мин)\n    all_df[\"threshold_type\"] = (all_df[\"v_duration\"] <= 300).astype(\"int8\")\n\n    # backoff выбор p_*: видео → автор → категория → глобаль\n    def pick_backoff(row, w: str):\n        # row содержит: views_{w}, p_vid_{w}, author_views_{w}, p_author_{w}, cat_views_{w}, p_category_{w}\n        vviews = row[f\"views_{w}\"]\n        if pd.notna(vviews) and vviews >= getattr(cfg, f\"min_views_video\"):\n            return row[f\"p_vid_{w}\"]\n        aviews = row[f\"author_views_{w}\"]\n        if pd.notna(aviews) and aviews >= getattr(cfg, f\"min_views_author\"):\n            return row[f\"p_author_{w}\"]\n        cviews = row[f\"cat_views_{w}\"]\n        if pd.notna(cviews) and cviews >= getattr(cfg, f\"min_views_category\"):\n            return row[f\"p_category_{w}\"]\n        return p0_global\n\n    for w in (\"1d\",\"7d\",\"30d\"):\n        all_df[f\"p_backoff_{w}\"] = all_df.apply(lambda r: pick_backoff(r, w), axis=1)\n        all_df[f\"logit_p_backoff_{w}\"] = all_df[f\"p_backoff_{w}\"].clip(EPS,1-EPS).map(logit)\n\n    # взаимодействия\n    all_df[\"dur_x_p1d\"]   = all_df[\"v_duration\"] * all_df[\"p_backoff_1d\"]\n    all_df[\"thr_x_p1d\"]   = all_df[\"threshold_type\"] * all_df[\"p_backoff_1d\"]\n    all_df[\"dur_x_ease7\"] = all_df[\"v_duration\"] * all_df[\"v_frac_avg_watchtime_7_day_duration\"].fillna(0)\n\n    # ---------- 5) Online (as-of) фичи по пользователю, региону и т.п. ----------\n    # Состояния\n    last_time = {}                     # user -> last event ts\n    last_gap_deque = defaultdict(lambda: deque(maxlen=10))  # user -> deque последних Δt\n    session_len = defaultdict(int)     # user -> len текущей сессии\n    seen_pairs = set()                 # {(user,video)} seen\n    prev_watch_map = {}                # (user,video) -> last watchtime\n\n    # пользовательские исходы\n    user_outcomes = defaultdict(lambda: deque(maxlen=max(cfg.user_L_list)))  # user -> deque[0/1]\n    user_times = defaultdict(lambda: deque(maxlen=10))  # для медианы скоростей\n\n    # user×author/category счётчики\n    ua_pos = defaultdict(int); ua_tot = defaultdict(int)\n    uc_pos = defaultdict(int); uc_tot = defaultdict(int)\n\n    # гео-агрегаты\n    region_pos = defaultdict(int); region_tot = defaultdict(int)\n    city_pos = defaultdict(int);   city_tot = defaultdict(int)\n    region_hour_pos = defaultdict(int); region_hour_tot = defaultdict(int)\n\n    # свежесть\n    user_fresh_pos = defaultdict(int); user_fresh_tot = defaultdict(int)\n\n    # эмбеддинги — пользовательские центроиды и недавние\n    user_pos_c = {}   # user -> np.ndarray\n    user_neg_c = {}   # user -> np.ndarray\n    user_recent_emb = defaultdict(lambda: deque(maxlen=cfg.recent_k_sim))\n\n    # Подготовка списков, чтобы заполнять без .loc в цикле\n    n = len(all_df)\n    out = {\n        # user history rates\n        **{f\"user_long_rate_L{L}\": [np.nan]*n for L in cfg.user_L_list},\n        \"user_recent_gap_s\": [np.nan]*n,\n        \"session_len\": [0]*n,\n        \"user_speed_median_gap_s\": [np.nan]*n,\n\n        # seen/repeat\n        \"seen_same_video_before\": [0]*n,\n        \"prev_watchtime_same_video\": [np.nan]*n,\n\n        # affinity user×author/category\n        \"user_author_long_rate\": [np.nan]*n,\n        \"user_category_long_rate\": [np.nan]*n,\n\n        # geo rates\n        \"region_long_rate\": [np.nan]*n,\n        \"city_long_rate\": [np.nan]*n,\n        \"region_hour_long_rate\": [np.nan]*n,\n\n        # fresh pref\n        \"user_fresh_long_rate\": [np.nan]*n,\n\n        # embedding sims (optional)\n        \"sim_user_pos\": [np.nan]*n,\n        \"sim_user_neg\": [np.nan]*n,\n        \"sim_user_margin\": [np.nan]*n,\n        \"sim_user_recent_k\": [np.nan]*n,\n        \"sim_author_pos_margin\": [np.nan]*n,\n        \"sim_category_pos_margin\": [np.nan]*n,\n\n        # interactions online\n        \"dur_x_user_speed\": [np.nan]*n,\n    }\n\n    # prefetch columns as arrays (ускоряет)\n    arr_user   = all_df[\"user_id\"].values\n    arr_video  = all_df[\"video_id\"].values\n    arr_region = all_df[\"region\"].astype(str).fillna(\"Unknown\").values\n    arr_city   = all_df[\"city\"].astype(str).fillna(\"Unknown\").values\n    arr_time   = all_df[\"event_dt\"].astype(\"datetime64[s]\").astype(np.int64)  # ns->s int\n    arr_hour   = all_df[\"time_of_day\"].values\n    arr_is_tr  = all_df[\"is_train\"].values\n    arr_watch  = all_df[\"watchtime\"].values if \"watchtime\" in all_df.columns else np.array([np.nan]*n)\n    arr_dur    = all_df[\"v_duration\"].values\n    arr_age_h  = all_df[\"age_hours\"].values\n    arr_author = all_df[\"author_id\"].values\n    arr_cat    = all_df[\"category_id\"].values\n\n    # emb helpers (author/category centroids)\n    def author_pos_neg_centroids(aid):\n        if not author_centroids: return (None, None)\n        return author_centroids.get(aid, (None, None))\n    def category_pos_neg_centroids(cid):\n        if not category_centroids: return (None, None)\n        return category_centroids.get(cid, (None, None))\n\n    # главное: цикл по событиям по времени\n    # сортируем индексы по времени (стабильно)\n    order = np.argsort(arr_time, kind=\"stable\")\n    for j, idx in enumerate(order):\n        u = arr_user[idx]\n        v = arr_video[idx]\n        r = arr_region[idx]\n        c = arr_city[idx]\n        t = int(arr_time[idx])\n        hr = int(arr_hour[idx])\n        is_tr = int(arr_is_tr[idx])\n        wt = arr_watch[idx] if is_tr == 1 else np.nan\n        dur = float(arr_dur[idx]) if not pd.isna(arr_dur[idx]) else np.nan\n        ageh = float(arr_age_h[idx]) if not pd.isna(arr_age_h[idx]) else np.nan\n        aid = arr_author[idx]\n        cid = arr_cat[idx]\n\n        # ---- seen / prev watch\n        seen = 1 if (u, v) in seen_pairs else 0\n        out[\"seen_same_video_before\"][idx] = seen\n        out[\"prev_watchtime_same_video\"][idx] = prev_watch_map.get((u, v), np.nan)\n\n        # ---- gaps/speeds/session\n        last_t = last_time.get(u, None)\n        gap = (t - last_t) if last_t is not None else np.nan\n        out[\"user_recent_gap_s\"][idx] = gap\n\n        if last_t is None or (gap is not None and gap > cfg.session_gap_s):\n            session_len[u] = 1\n            last_gap_deque[u].clear()\n            user_times[u].clear()\n        else:\n            session_len[u] = session_len[u] + 1\n\n        out[\"session_len\"][idx] = session_len[u]\n        if not np.isnan(gap):\n            last_gap_deque[u].append(gap)\n            user_times[u].append(t)\n\n        # медиана последних Δt\n        if len(last_gap_deque[u]) > 0:\n            out[\"user_speed_median_gap_s\"][idx] = float(np.median(last_gap_deque[u]))\n\n        # ---- user L-rates\n        outcomes = user_outcomes[u]\n        for L in cfg.user_L_list:\n            if len(outcomes) == 0:\n                out[f\"user_long_rate_L{L}\"][idx] = np.nan\n            else:\n                # берем последние L (или все, если меньше)\n                arr = list(outcomes)[-L:]\n                out[f\"user_long_rate_L{L}\"][idx] = float(sum(arr)/len(arr))\n\n        # ---- user×author/category smoothed rates\n        key_ua = (u, aid)\n        key_uc = (u, cid)\n        p_ua = beta_smooth(ua_pos[key_ua], ua_tot[key_ua], cfg.alpha_online_small, p0_global)\n        p_uc = beta_smooth(uc_pos[key_uc], uc_tot[key_uc], cfg.alpha_online_small, p0_global)\n        out[\"user_author_long_rate\"][idx] = p_ua\n        out[\"user_category_long_rate\"][idx] = p_uc\n\n        # ---- region/city/region×hour rates\n        p_reg = beta_smooth(region_pos[r], region_tot[r], cfg.alpha_online_small, p0_global)\n        p_city = beta_smooth(city_pos[c], city_tot[c], cfg.alpha_online_small, p0_global)\n        p_reg_hr = beta_smooth(region_hour_pos[(r,hr)], region_hour_tot[(r,hr)], cfg.alpha_online_small, p0_global)\n        out[\"region_long_rate\"][idx] = p_reg\n        out[\"city_long_rate\"][idx] = p_city\n        out[\"region_hour_long_rate\"][idx] = p_reg_hr\n\n        # ---- fresh pref (<= fresh_hours)\n        p_fresh = beta_smooth(user_fresh_pos[u], user_fresh_tot[u], cfg.alpha_online_small, p0_global)\n        out[\"user_fresh_long_rate\"][idx] = p_fresh\n\n        # ---- embedding sims (optional)\n        e = emb_map.get(v, None) if emb_map else None\n        # пользовательские pos/neg центроиды\n        up = user_pos_c.get(u, None)\n        un = user_neg_c.get(u, None)\n        if e is not None:\n            out[\"sim_user_pos\"][idx] = cos_sim(e, up) if up is not None else np.nan\n            out[\"sim_user_neg\"][idx] = cos_sim(e, un) if un is not None else np.nan\n            if (up is not None) or (un is not None):\n                out[\"sim_user_margin\"][idx] = (cos_sim(e, up) if up is not None else 0.0) - (cos_sim(e, un) if un is not None else 0.0)\n            # недавние k\n            if len(user_recent_emb[u]) > 0:\n                mean_recent = np.mean(np.stack(list(user_recent_emb[u]), axis=0), axis=0)\n                out[\"sim_user_recent_k\"][idx] = cos_sim(e, mean_recent)\n            # автор/категория марджины\n            ap, an = author_pos_neg_centroids(aid)\n            cp, cn = category_pos_neg_centroids(cid)\n            if ap is not None or an is not None:\n                out[\"sim_author_pos_margin\"][idx] = (cos_sim(e, ap) if ap is not None else 0.0) - (cos_sim(e, an) if an is not None else 0.0)\n            if cp is not None or cn is not None:\n                out[\"sim_category_pos_margin\"][idx] = (cos_sim(e, cp) if cp is not None else 0.0) - (cos_sim(e, cn) if cn is not None else 0.0)\n\n        # ---- interactions online\n        sp = out[\"user_speed_median_gap_s\"][idx]\n        out[\"dur_x_user_speed\"][idx] = (dur * sp) if (not pd.isna(dur) and not pd.isna(sp)) else np.nan\n\n        # ==================== UPDATE STATE (после вычисления фичей) ====================\n        # seen/prev watch — не зависит от лейбла\n        seen_pairs.add((u, v))\n        if is_tr == 1 and not pd.isna(wt):\n            prev_watch_map[(u, v)] = float(wt)\n\n        # обновляем время/сессию\n        last_time[u] = t\n\n        # эмбеддинги: добавим в \"recent\"\n        if e is not None:\n            user_recent_emb[u].append(e)\n\n        # лейбл для train\n        if is_tr == 1 and not pd.isna(wt) and not pd.isna(dur):\n            if dur > 300:\n                y = 1 if (wt > 0.25 * dur) else 0\n            else:\n                y = 1 if (wt > 30.0) else 0\n\n            # user outcomes\n            user_outcomes[u].append(y)\n\n            # user×author/category\n            ua_tot[key_ua] += 1; uc_tot[key_uc] += 1\n            if y == 1:\n                ua_pos[key_ua] += 1; uc_pos[key_uc] += 1\n\n            # geo\n            region_tot[r] += 1; city_tot[c] += 1; region_hour_tot[(r,hr)] += 1\n            if y == 1:\n                region_pos[r] += 1; city_pos[c] += 1; region_hour_pos[(r,hr)] += 1\n\n            # fresh pref\n            if not pd.isna(ageh) and ageh <= cfg.fresh_hours:\n                user_fresh_tot[u] += 1\n                if y == 1:\n                    user_fresh_pos[u] += 1\n\n            # эмбеддинги: EMA по pos/neg центроидам\n            if e is not None:\n                if y == 1:\n                    user_pos_c[u] = ema_update(user_pos_c.get(u, None), e, cfg.ema_beta)\n                else:\n                    user_neg_c[u] = ema_update(user_neg_c.get(u, None), e, cfg.ema_beta)\n\n    # приклеим онлайновые фичи\n    for k, vals in out.items():\n        all_df[k] = vals\n\n    # ---------- 6) Target для train, финальный отбор колонок ----------\n    # целевая переменная по правилам\n    if \"watchtime\" in train.columns:\n        msk_tr = all_df[\"is_train\"] == 1\n        dur = all_df.loc[msk_tr, \"v_duration\"].values\n        wt  = all_df.loc[msk_tr, \"watchtime\"].values\n        y = np.zeros(msk_tr.sum(), dtype=np.int8)\n        longer = dur > 300\n        y[longer]  = (wt[longer] > 0.25 * dur[longer]).astype(np.int8)\n        y[~longer] = (wt[~longer] > 30.0).astype(np.int8)\n        all_df.loc[msk_tr, \"target\"] = y\n\n    # финальный список фичей\n    feat_cols = [\n        # видео-базлайн\n        \"p_backoff_1d\",\"p_backoff_7d\",\"p_backoff_30d\",\n        \"logit_p_backoff_1d\",\"logit_p_backoff_7d\",\"logit_p_backoff_30d\",\n        \"p_vid_1d\",\"p_vid_7d\",\"p_vid_30d\",\n        \"dlogit_p_1d_7d\",\"dlogit_p_7d_30d\",\n        \"v_frac_avg_watchtime_1_day_duration\",\"v_frac_avg_watchtime_7_day_duration\",\"v_frac_avg_watchtime_30_day_duration\",\n        \"engagement_1d\",\"engagement_7d\",\"engagement_30d\",\n        \"trend_views_d_w\",\"trend_p_1d_7d\",\n        \"res_vid_vs_author_7d\",\"res_vid_vs_cat_7d\",\n\n        # статические от события\n        \"v_duration\",\"age_hours\",\"time_of_day\",\"time_bin\",\"threshold_type\",\n        \"dur_x_p1d\",\"thr_x_p1d\",\"dur_x_ease7\",\n\n        # онлайн-пользовательские\n        *[f\"user_long_rate_L{L}\" for L in cfg.user_L_list],\n        \"user_recent_gap_s\",\"session_len\",\"user_speed_median_gap_s\",\n        \"seen_same_video_before\",\"prev_watchtime_same_video\",\n        \"user_author_long_rate\",\"user_category_long_rate\",\n        \"region_long_rate\",\"city_long_rate\",\"region_hour_long_rate\",\n        \"user_fresh_long_rate\",\n        \"dur_x_user_speed\",\n\n        # эмбеддинги (если были)\n        \"sim_user_pos\",\"sim_user_neg\",\"sim_user_margin\",\"sim_user_recent_k\",\n        \"sim_author_pos_margin\",\"sim_category_pos_margin\",\n    ]\n\n    # часть колонок может отсутствовать (если не было эмбеддингов) — отфильтруем\n    feat_cols = [c for c in feat_cols if c in all_df.columns]\n\n    train_feat = all_df[all_df[\"is_train\"] == 1].copy()\n    test_feat  = all_df[all_df[\"is_train\"] == 0].copy()\n\n    # safety: fillna\n    train_feat[feat_cols] = train_feat[feat_cols].replace([np.inf,-np.inf], np.nan).fillna(0)\n    test_feat[feat_cols]  = test_feat[feat_cols].replace([np.inf,-np.inf], np.nan).fillna(0)\n\n    # вернём только нужные\n    keep_train = [\"user_id\",\"video_id\",\"event_timestamp\",\"target\"] + feat_cols\n    keep_test  = [\"user_id\",\"video_id\",\"event_timestamp\"] + feat_cols\n    train_feat = train_feat[keep_train]\n    test_feat  = test_feat[keep_test]\n\n    return train_feat, test_feat\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:48.887286Z","iopub.execute_input":"2025-10-27T20:38:48.887515Z","iopub.status.idle":"2025-10-27T20:38:48.946017Z","shell.execute_reply.started":"2025-10-27T20:38:48.887499Z","shell.execute_reply":"2025-10-27T20:38:48.945297Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# video_emb: DataFrame со столбцами ['video_id','emb'], где emb — np.ndarray одинаковой длины\nvideo_emb = None  # или свой мэппинг эмбеддингов\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:48.946919Z","iopub.execute_input":"2025-10-27T20:38:48.947077Z","iopub.status.idle":"2025-10-27T20:38:48.962031Z","shell.execute_reply.started":"2025-10-27T20:38:48.947065Z","shell.execute_reply":"2025-10-27T20:38:48.961327Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\n\ndef add_text_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    # гарантируем текстовые поля\n    for col in [\"title\", \"description\"]:\n        if col not in df.columns:\n            df[col] = \"\"\n        df[col] = df[col].fillna(\"\").astype(str)\n        df[f\"{col}_lc\"] = df[col].str.lower()\n\n    # если нет длительности — заполним NaN (используется только для нормировок)\n    if \"v_duration\" not in df.columns:\n        df[\"v_duration\"] = np.nan\n    df[\"v_duration\"] = df[\"v_duration\"].astype(float)\n\n    # длины\n    df[\"title_len_char\"]  = df[\"title\"].str.len().fillna(0).astype(\"int32\")\n    df[\"desc_len_char\"]   = df[\"description\"].str.len().fillna(0).astype(\"int32\")\n    df[\"title_len_words\"] = df[\"title\"].str.split().str.len().fillna(0).astype(\"int32\")\n    df[\"desc_len_words\"]  = df[\"description\"].str.split().str.len().fillna(0).astype(\"int32\")\n    df[\"title_avg_word_len\"] = (df[\"title_len_char\"] / (df[\"title_len_words\"] + 1e-6)).astype(\"float32\")\n    df[\"desc_avg_word_len\"]  = (df[\"desc_len_char\"]  / (df[\"desc_len_words\"]  + 1e-6)).astype(\"float32\")\n\n    # пунктуация / цифры\n    df[\"title_exclam\"]   = df[\"title\"].str.count(\"!\").fillna(0).astype(\"int16\")\n    df[\"title_question\"] = df[\"title\"].str.count(r\"\\?\").fillna(0).astype(\"int16\")\n    df[\"desc_exclam\"]    = df[\"description\"].str.count(\"!\").fillna(0).astype(\"int16\")\n    df[\"desc_question\"]  = df[\"description\"].str.count(r\"\\?\").fillna(0).astype(\"int16\")\n    df[\"title_digits\"]   = df[\"title\"].str.count(r\"\\d\").fillna(0).astype(\"int16\")\n    df[\"desc_digits\"]    = df[\"description\"].str.count(r\"\\d\").fillna(0).astype(\"int16\")\n\n    # URL / таймкоды / главы\n    url_pat   = r\"(https?://|www\\.)\"\n    tcode_pat = r\"\\b(?:(?:\\d{1,2}:){1,2}\\d{2})\\b\"  # mm:ss или hh:mm:ss\n    df[\"title_has_url\"]      = df[\"title_lc\"].str.contains(url_pat, regex=True).fillna(False).astype(\"int8\")\n    df[\"desc_has_url\"]       = df[\"description_lc\"].str.contains(url_pat, regex=True).fillna(False).astype(\"int8\")\n    df[\"title_has_timecode\"] = df[\"title_lc\"].str.contains(tcode_pat, regex=True).fillna(False).astype(\"int8\")\n    df[\"desc_timecode_count\"]= df[\"description_lc\"].str.count(tcode_pat).fillna(0).astype(\"int16\")\n    df[\"desc_has_timecode\"]  = (df[\"desc_timecode_count\"] > 0).astype(\"int8\")\n    df[\"desc_has_chapters\"]  = (df[\"desc_timecode_count\"] >= 3).astype(\"int8\")  # 3+ таймкодов ~ главы\n\n    # ключевые слова\n    KW = {\n        \"live\":      [\"live\", \"стрим\", \"прямая трансляция\", \"трансляция\"],\n        \"trailer\":   [\"trailer\", \"трейлер\"],\n        \"podcast\":   [\"podcast\", \"подкаст\"],\n        \"interview\": [\"интервью\", \"interview\"],\n        \"lesson\":    [\"how to\", \"tutorial\", \"урок\", \"гайд\", \"обучение\", \"инструкция\", \"guide\"],\n        \"full\":      [\"полная версия\", \"full version\", \"full movie\", \"без сокращений\", \"без рекламы\"],\n        \"series\":    [\"серия\", \"выпуск\", \"эпизод\", \"season\", \"episode\", \"ep\"],\n        \"music\":     [\"official video\", \"official audio\", \"lyrics\", \"клип\", \"remix\"],\n    }\n    def any_kw(series_lc: pd.Series, kws) -> pd.Series:\n        pat = \"|\".join(re.escape(k) for k in kws)\n        return series_lc.str.contains(pat, regex=True)\n\n    for k, lst in KW.items():\n        df[f\"title_kw_{k}\"] = any_kw(df[\"title_lc\"], lst).fillna(False).astype(\"int8\")\n        df[f\"desc_kw_{k}\"]  = any_kw(df[\"description_lc\"], lst).fillna(False).astype(\"int8\")\n\n    # агрегаты по ключевым словам\n    info_keys = [\"podcast\", \"interview\", \"lesson\", \"full\", \"series\"]\n    df[\"kw_info_longform\"] = df[[f\"title_kw_{k}\" for k in info_keys] + [f\"desc_kw_{k}\" for k in info_keys]].max(axis=1).astype(\"int8\")\n    df[\"kw_live_any\"]      = df[[\"title_kw_live\", \"desc_kw_live\"]].max(axis=1).astype(\"int8\")\n    df[\"kw_trailer_any\"]   = df[[\"title_kw_trailer\", \"desc_kw_trailer\"]].max(axis=1).astype(\"int8\")\n    df[\"kw_music_any\"]     = df[[\"title_kw_music\", \"desc_kw_music\"]].max(axis=1).astype(\"int8\")\n\n    # нормировки на длительность\n    minutes = (df[\"v_duration\"] / 60.0).astype(\"float32\")\n    df[\"title_words_per_min\"] = (df[\"title_len_words\"] / (minutes + 1e-6)).astype(\"float32\")\n    df[\"desc_words_per_min\"]  = (df[\"desc_len_words\"]  / (minutes + 1e-6)).astype(\"float32\")\n\n    # взаимодействие с ease7 (если колонка есть; иначе 0)\n    ease7 = df[\"v_frac_avg_watchtime_7_day_duration\"].astype(float) if \"v_frac_avg_watchtime_7_day_duration\" in df.columns else 0.0\n    df[\"ease7_x_desc_len\"] = (df[\"desc_len_words\"] * ease7).astype(\"float32\")\n\n    return df\n\n# === Применяем ТОЛЬКО текстовые фичи к вашим датафреймам ===\ntrain_full_new1 = add_text_features(train_full_new1)\ntest_full_new1  = add_text_features(test_full_new1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:38:48.962815Z","iopub.execute_input":"2025-10-27T20:38:48.963113Z","execution_failed":"2025-10-27T20:41:38.365Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SKF catboost","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"useless_cols=['event_timestamp',\n          'v_pub_datetime', ]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_full_new1=train_full_new1.drop(columns=useless_cols)\n\ntest_full_new1=test_full_new1.drop(columns=useless_cols)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_full_new1=train_full_new1[:3_000_000]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X=train_full_new1.drop(columns=['watchtime', 'y_long', 'longwatch_threshold_sec', 'title', 'description'])\ny=train_full_new1['y_long']\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_eval, y_train, y_eval=train_test_split(X, y, test_size=0.15, random_state=seed)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for now just one catboost \n\nmodel=CatBoostClassifier(\n    iterations=1000,\n    depth=6,\n    learning_rate=0.03,\n    loss_function='Logloss',\n    eval_metric='F1',\n    task_type='GPU',\n    #l2_leaf_reg=,\n    \n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_feats=[ 'user_id','video_id','category_id','author_id',  'region','city']","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(X_train, y_train, eval_set=(X_eval, y_eval),cat_features=cat_feats, verbose=200, early_stopping_rounds=500)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Just mean of them ","metadata":{}},{"cell_type":"code","source":"num_feats=train_full_new1.select_dtypes(include='number').columns.tolist()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr=train_full_new1[num_feats].corrwith(train_full_new1['y_long'])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test=test_full_new1.drop(columns=['title', 'description'])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds=model.predict(X_test)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample['target']=preds","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-27T20:41:38.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Meta Models","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}