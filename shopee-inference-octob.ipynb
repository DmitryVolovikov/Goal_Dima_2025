{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================\n# Shopee Inference (Image + Text Fusion)\n# ============================\n\nimport os, gc, math, random, sys, time\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import v2\nimport timm\nfrom tqdm import tqdm\n\nfrom transformers import AutoTokenizer, AutoModel\n\n# ------------- Config -------------\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nrandom.seed(SEED); np.random.seed(SEED)\ntorch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nDATA_DIR = '/kaggle/input/shopee-product-matching'\nTEST_CSV = os.path.join(DATA_DIR, 'test.csv')\nTEST_IMG_DIR = os.path.join(DATA_DIR, 'test_images')\n\n# укажи путь к весам, если загрузил их как отдельный датасет\n# например: /kaggle/input/shopee-multimodal-weights/embedding_extractor_mix.pth\nCANDIDATE_WEIGHT_PATHS = [\n    '/kaggle/input/shopee-octob-inference/embedding_extractor_mix.pth',  # <- пример\n    '/kaggle/working/embedding_extractor_mix.pth',                          # если запускаешь в одном ноуте\n    '/kaggle/input/shopee-octob-inference/embedding_extractor.pth',\n    '/kaggle/working/embedding_extractor.pth',\n]\n\nIMAGE_SIZE = 224            # должен совпадать с train\nBATCH_IMG = 64\nBATCH_TXT = 256\nKQ = 100                    # ширина кандидатов перед обрезкой\nK_CAP = 50                  # лимит на размер группы (условие соревна)\n\n# ------------- Data -------------\ntest = pd.read_csv(TEST_CSV)\n\nclass ShopeeTestDataset(Dataset):\n    def __init__(self, df, img_root, transform):\n        self.df = df.reset_index(drop=True)\n        self.img_root = img_root\n        self.transform = transform\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_root, row['image'])\n        image = Image.open(img_path).convert('RGB')\n        if self.transform is not None:\n            image = self.transform(image)\n        return {\n            'image': image,\n            'posting_id': row['posting_id'],\n            'title': str(row['title']) if not pd.isna(row['title']) else ''\n        }\n\ntfm_test = v2.Compose([\n    v2.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    v2.ToTensor(),\n    v2.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\n\ntest_ds = ShopeeTestDataset(test, TEST_IMG_DIR, tfm_test)\ntest_dl = DataLoader(test_ds, batch_size=BATCH_IMG, shuffle=False,\n                     num_workers=4, pin_memory=True, persistent_workers=True)\n\n# ------------- Models (must match training) -------------\n# Image branch: eca_nfnet_l1 + Linear(->512)+BN\ndef build_image_branch(backbone_name='eca_nfnet_l1', emb_dim=512):\n    backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0).to(device).eval()\n    feat_dim = backbone.num_features\n    head = nn.Sequential(nn.Linear(feat_dim, emb_dim, bias=False),\n                         nn.BatchNorm1d(emb_dim)).to(device).eval()\n    return backbone, head, feat_dim\n\n# Text branch: xlm-roberta-base + mean-pooling + Linear(->512)+BN\n#TEXT_MODEL_NAME = 'xlm-roberta-base'\n#text_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME, use_fast=True)\n#from huggingface_hub import snapshot_download\n#LOCAL_XLMR_DIR = snapshot_download(repo_id=\"xlm-roberta-base\")\n#TEXT_MODEL_NAME='xlm-roberta-base'\nimport os\nfrom transformers import AutoTokenizer, XLMRobertaConfig, XLMRobertaModel\nfrom torch import nn\n\n# Папки, где ищем сохранённый токенайзер/конфиг из тренировки\nLOCAL_XLMR_DIRS = [\n    \"/kaggle/input/shopee-token/xlmr_tokenizer\",  # <-- укажи свой датасет/путь\n    \"/kaggle/input/xlmr_tokenizer\",\n    \"/kaggle/working/xlmr_tokenizer\",\n    \n]\n\ndef try_load_local_xlmr(sd_txt, emb_dim):\n    \"\"\"Возвращает (tokenizer, txt_model, txt_head) или (None,None,None), если не нашли локальные файлы.\"\"\"\n    for d in LOCAL_XLMR_DIRS:\n        tok_json = os.path.join(d, \"tokenizer.json\")\n        cfg_json = os.path.join(d, \"config.json\")\n        if os.path.exists(tok_json) and os.path.exists(cfg_json):\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(d, use_fast=True, local_files_only=True)\n                cfg = XLMRobertaConfig.from_pretrained(d, local_files_only=True)\n                txt_model = XLMRobertaModel(cfg).to(device).eval()\n                txt_model.load_state_dict(sd_txt['txt_backbone'])\n\n                txt_head = nn.Sequential(nn.Linear(cfg.hidden_size, emb_dim, bias=False),\n                                         nn.BatchNorm1d(emb_dim)).to(device).eval()\n                txt_head.load_state_dict(sd_txt['txt_embed_head'])\n                print(f\"[TEXT] XLM-R loaded offline from {d}\")\n                return tokenizer, txt_model, txt_head\n            except Exception as e:\n                print(f\"[TEXT] Failed loading XLM-R from {d}: {e}\")\n    return None, None, None\n\ndef load_first_existing(paths):\n    for p in paths:\n        if os.path.exists(p):\n            return p\n    raise FileNotFoundError(\n        \"Weights not found. Provide a valid path in CANDIDATE_WEIGHT_PATHS \"\n        \"or attach a dataset with embedding_extractor_mix.pth / embedding_extractor.pth\"\n    )\n\nweights_path = load_first_existing(CANDIDATE_WEIGHT_PATHS)\nprint(\"Using weights:\", weights_path)\n\npkg = torch.load(weights_path, map_location='cpu')\n# из твоего пакета весов\nsd = pkg[\"state_dict\"]\nhave_text_weights = (\"txt_backbone\" in sd) and (\"txt_embed_head\" in sd)\nemb_dim = int(pkg.get(\"emb_dim\", 512))\nalpha = float(pkg.get(\"fusion_alpha\", 0.70))\ntau   = float(pkg.get(\"fusion_tau\",   0.50))\n\ntext_tokenizer, txt_model, txt_head = (None, None, None)\nif have_text_weights:\n    text_tokenizer, txt_model, txt_head = try_load_local_xlmr(sd, emb_dim)\n#text_tokenizer = AutoTokenizer.from_pretrained(LOCAL_XLMR_DIR, use_fast=True, local_files_only=True)\n#txt_model      = AutoModel.from_pretrained(LOCAL_XLMR_DIR, local_files_only=True).to(device).eval()\n\n\n@torch.no_grad()\ndef mean_pooling(last_hidden_state, attention_mask):\n    mask = attention_mask.unsqueeze(-1).float()\n    summed = (last_hidden_state * mask).sum(dim=1)\n    denom = mask.sum(dim=1).clamp(min=1e-6)\n    return summed / denom\n\n\n\n# ------------- Load Weights -------------\n\n#pkg = torch.load(weights_path, map_location='cpu')\n\n# Detect package type (mix: image+text) or image-only\nhave_text = False\nif 'state_dict' in pkg:\n    sd = pkg['state_dict']\n    have_text = ('txt_backbone' in sd) and ('txt_embed_head' in sd)\nelse:\n    raise ValueError(\"Invalid weights package: missing 'state_dict'\")\n\nalpha = float(pkg.get('fusion_alpha', 0.70))\ntau   = float(pkg.get('fusion_tau',   0.50))\nprint(f\"Fusion params -> alpha={alpha:.2f}, tau={tau:.2f}\")\nemb_dim = int(pkg.get('emb_dim', 512))\n\n# Build branches and load weights\nimg_backbone, img_head, _ = build_image_branch(backbone_name=str(pkg.get('backbone_name_img', pkg.get('backbone_name', 'eca_nfnet_l1'))),\n                                               emb_dim=emb_dim)\nimg_backbone.load_state_dict(sd['img_backbone'])\nimg_head.load_state_dict(sd['img_embed_head'])\nimg_backbone.eval(); img_head.eval()\n\n#if have_text:\n#    txt_model, txt_head, _ = build_text_embeddings(model_name=str(pkg.get('backbone_name_txt', TEXT_MODEL_NAME)),\n#                                               emb_dim=emb_dim)\n#    txt_model.load_state_dict(sd['txt_backbone'])\n#    txt_head.load_state_dict(sd['txt_embed_head'])\n#    txt_model.eval(); txt_head.eval()\n#    print(\"Text branch: LOADED\")\n#else:\n#    txt_model = None; txt_head = None\n#    print(\"Text branch: NOT FOUND in weights (will run image-only)\")\n\n# ------------- Embedding builders -------------\n@torch.no_grad()\ndef build_image_embeddings(loader):\n    embs, ids = [], []\n    for b in tqdm(loader, desc=\"Embed/img\"):\n        x = b['image'].to(device, non_blocking=True)\n        e = img_backbone(x)               # (B, F)\n        e = img_head(e)                   # (B, D)\n        e = nn.functional.normalize(e, dim=1)\n        embs.append(e.cpu())\n        ids.extend(b['posting_id'])\n    embs = torch.cat(embs, dim=0).numpy().astype('float32')\n    embs /= (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-8)\n    return embs.astype('float16'), ids\n\n@torch.no_grad()\ndef build_text_embeddings(df, batch_size=BATCH_TXT, max_len=64):\n    titles = df['title'].fillna('').astype(str).tolist()\n    out = []\n    for i in tqdm(range(0, len(titles), batch_size), desc=\"Embed/txt\"):\n        batch = titles[i:i+batch_size]\n        tok = text_tokenizer(batch, padding=True, truncation=True,\n                             max_length=max_len, return_tensors='pt')\n        tok = {k: v.to(device, non_blocking=True) for k, v in tok.items()}\n        h = txt_model(**tok).last_hidden_state\n        sent = mean_pooling(h, tok['attention_mask'])    # (B, 768)\n        e = txt_head(sent)                                # (B, D)\n        e = nn.functional.normalize(e, dim=1)\n        out.append(e.cpu())\n    embs = torch.cat(out, dim=0).numpy().astype('float32')\n    embs /= (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-8)\n    return embs.astype('float16')\n\n# ------------- Retrieval utils -------------\ndef topk_chunked_cos(embs_f16: np.ndarray, K: int, qbs: int = 128):\n    \"\"\"\n    OOM-safe top-K по косинусу (L2-нормированные эмбеддинги).\n    Возвращает (sims, idxs) формы (N, K).\n    \"\"\"\n    N, D = embs_f16.shape\n    device_t = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    db = torch.from_numpy(embs_f16.astype('float32', copy=False)).to(device_t, non_blocking=True)\n    K = min(K, N)\n    idxs_list, sims_list = [], []\n    for start in tqdm(range(0, N, qbs), desc=\"TopK (torch-chunk)\"):\n        q = db[start:start+qbs]\n        S = torch.matmul(q, db.T)  # косинус\n        vals, ids = torch.topk(S, k=K, dim=1, largest=True, sorted=True)\n        idxs_list.append(ids.cpu().numpy().astype('int32'))\n        sims_list.append(vals.cpu().numpy().astype('float32'))\n        del S, vals, ids\n        if device_t.type == 'cuda':\n            torch.cuda.empty_cache()\n    idxs = np.vstack(idxs_list)\n    sims = np.vstack(sims_list)\n    del db\n    return sims, idxs\n\ndef build_preds_fused_mutual(ids, idxs_img, sims_img, idxs_txt, sims_txt,\n                             alpha=0.7, tau=0.50, K_cap=50):\n    \"\"\"\n    Взаимность проверяется на объединённом кандидатном множестве и по fused-оценке.\n    Если текст отсутствует, просто используем image (sims_txt будет None).\n    \"\"\"\n    N = len(ids)\n    if sims_txt is None or idxs_txt is None:\n        # Image-only fallback (вес текста = 0)\n        idxs_txt = [set() for _ in range(N)]\n        sims_txt = None\n\n    cand_sets = []\n    for i in range(N):\n        s = set(idxs_img[i])\n        if isinstance(idxs_txt, np.ndarray):\n            s = s.union(set(idxs_txt[i]))\n        elif isinstance(idxs_txt, list):\n            s = s.union(set(idxs_txt[i]))  # list of sets (image-only path also ok)\n        cand_sets.append(s)\n\n    out = {}\n    for i in range(N):\n        map_img = {int(j): float(s) for j, s in zip(idxs_img[i], sims_img[i])}\n        map_txt = {}\n        if isinstance(idxs_txt, np.ndarray) and sims_txt is not None:\n            map_txt = {int(j): float(s) for j, s in zip(idxs_txt[i], sims_txt[i])}\n\n        fused = []\n        for j in cand_sets[i]:\n            si = (map_img.get(j, -1.0) + 1.0) / 2.0\n            if map_txt:\n                st = (map_txt.get(j, -1.0) + 1.0) / 2.0\n            else:\n                st = 0.0\n            s  = alpha * si + (1.0 - alpha) * st\n            # взаимность\n            if s >= tau and (i in cand_sets[j]):\n                fused.append((j, s))\n\n        fused.sort(key=lambda x: -x[1])\n        keep = [ids[j] for j, _ in fused][:K_CAP]\n        if ids[i] not in keep:\n            keep = [ids[i]] + keep\n        out[ids[i]] = set(keep[:K_CAP])\n    return out\n\n# ------------- Run inference -------------\n# 1) image embeddings\nimg_embs, test_ids = build_image_embeddings(test_dl)\n\n# 2) text embeddings (if present)\nif have_text:\n    txt_embs = build_text_embeddings(test, batch_size=BATCH_TXT, max_len=64)\nelse:\n    txt_embs = None\n\n# 3) top-K candidates\nsims_img, idxs_img = topk_chunked_cos(img_embs, K=KQ, qbs=128)\nif txt_embs is not None:\n    sims_txt, idxs_txt = topk_chunked_cos(txt_embs, K=KQ, qbs=256)\nelse:\n    sims_txt, idxs_txt = None, None\n\n# 4) fused mutual\npreds_test = build_preds_fused_mutual(test_ids, idxs_img, sims_img, idxs_txt, sims_txt,\n                                      alpha=alpha, tau=tau, K_cap=K_CAP)\n\n# 5) submission\nmatches_col = [' '.join(preds_test[pid]) for pid in test_ids]\nsub = pd.DataFrame({'posting_id': test_ids, 'matches': matches_col})\nout_path = '/kaggle/working/submission.csv'\nsub.to_csv(out_path, index=False)\nprint(\"Saved:\", out_path)\n\n# sanity: средний размер групп\navg_group_size = float(np.mean([len(v) for v in preds_test.values()]))\nprint(f\"Avg predicted group size: {avg_group_size:.2f}\")\n\n# ------------- Cleanup -------------\ndel img_embs, txt_embs, sims_img, idxs_img, sims_txt, idxs_txt\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T15:56:47.852806Z","iopub.execute_input":"2025-10-10T15:56:47.853546Z","iopub.status.idle":"2025-10-10T15:56:56.924439Z","shell.execute_reply.started":"2025-10-10T15:56:47.853520Z","shell.execute_reply":"2025-10-10T15:56:56.923470Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using weights: /kaggle/input/shopee-octob-inference/embedding_extractor_mix.pth\n[TEXT] XLM-R loaded offline from /kaggle/input/shopee-token/xlmr_tokenizer\nFusion params -> alpha=0.90, tau=0.74\n","output_type":"stream"},{"name":"stderr","text":"Embed/img: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\nEmbed/txt: 100%|██████████| 1/1 [00:00<00:00,  2.80it/s]\nTopK (torch-chunk): 100%|██████████| 1/1 [00:00<00:00, 11.86it/s]\nTopK (torch-chunk): 100%|██████████| 1/1 [00:00<00:00, 1376.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: /kaggle/working/submission.csv\nAvg predicted group size: 1.00\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}