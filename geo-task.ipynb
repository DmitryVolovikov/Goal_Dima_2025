{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13367037,"sourceType":"datasetVersion","datasetId":8479550}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q sentence-transformers==2.7.0 transformers==4.45.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport random\n\nseed=\n\nos.environ['PYTHONHASHSEED']=str(seed)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, re, ast, gc, warnings\nfrom typing import Tuple, List\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_absolute_error\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.neighbors import BallTree\nfrom scipy import sparse\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RANDOM_STATE = 42\nN_FOLDS = 5\nUSE_GPU = False\n\nTFIDF_MAX_FEATURES = 120_000\nMIN_DF = 3\nMAX_DF = 0.975\nNGRAM_RANGE = (1, 2)\nSVD_COMPONENTS = 200\nUSE_CHAR_TFIDF = True \n\nUSE_KEYWORD_FLAGS = True\nKEYWORD_MAX_DF = 0.85\nKEYWORD_TOP_K  = 350\nTOKEN_PATTERN  = r\"(?u)\\b[а-яa-z0-9][а-яa-z0-9\\-]{2,}\\b\"\n\nUSE_RUBERT = True\nBERT_MODEL = \"cointegrated/rubert-tiny2\"   # можно sbert_large_nlu_ru если успею\nBERT_BATCH_SIZE = 256\nBERT_NORMALIZE = True\nBERT_SVD_COMPONENTS = 96\nBERT_RAW_KEEP_DIMS = 64 \nRIDGE_ALPHA  = 3.0\nRIDGE_SOLVER = \"lsqr\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MOSCOW_CENTER_LAT, MOSCOW_CENTER_LON = 55.752023, 37.617499 ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/input/vseros-geo-task/train.tsv\"\nTEST_PATH  = \"/kaggle/input/vseros-geo-task/test.tsv\"\nREVIEWS_PATH_CANDIDATES = [\"/kaggle/input/vseros-geo-task/reviews.txv/reviews.tsv\"]\nSUBMISSION_PATH = \"submission_catboost_tfidf.csv\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def smart_read_table(path, sep=\"\\t\", **kwargs):\n    if not os.path.exists(path):\n        raise FileNotFoundError(path)\n    try:\n        return pd.read_csv(path, sep=sep, **kwargs)\n    \n\ndef try_read_reviews(candidates: List[str]) -> pd.DataFrame:\n    for p in candidates:\n        if os.path.exists(p):\n            for sep in [\"\\t\", \",\"]:\n                try:\n                    df = pd.read_csv(p, sep=sep)\n                    if {\"id\",\"text\"}.issubset(df.columns):\n                        return df[[\"id\",\"text\"]]\n\ndef parse_coordinates_to_lon_lat(s) -> Tuple[float, float]:\n    if pd.isna(s):\n        return np.nan, np.nan\n    try:\n        val = ast.literal_eval(s)\n        if isinstance(val, (list, tuple)) and len(val) == 2:\n            return float(val[0]), float(val[1])\n    except Exception:\n        pass\n    m = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", str(s))\n    if len(m) >= 2:\n        return float(m[0]), float(m[1])\n    return np.nan, np.nan\n\ndef add_share_pair(df: pd.DataFrame, a: str, b: str, new_col: str):\n    if a in df.columns and b in df.columns:\n        denom = (pd.to_numeric(df[a], errors=\"coerce\").fillna(0) +\n                 pd.to_numeric(df[b], errors=\"coerce\").fillna(0))\n        num = pd.to_numeric(df[a], errors=\"coerce\").fillna(0)\n        df[new_col] = np.where(denom > 0, num / denom, np.nan)\n\ndef light_log1p_skewed(df: pd.DataFrame, numeric_cols: List[str], skew_thr: float = 1.0) -> List[str]:\n    transformed = []\n    for c in numeric_cols:\n        try:\n            s = pd.to_numeric(df[c], errors=\"coerce\")\n            if s.notna().sum() == 0: continue\n            if (s >= 0).all():\n                skew = s.skew(skipna=True)\n                if abs(skew) > skew_thr:\n                    df[c] = np.log1p(s)\n                    transformed.append(c)\n        except Exception:\n            continue\n    return transformed\n\nR_EARTH = 6_371_000.0\ndef _to_rad_series(df):\n    lat = pd.to_numeric(df[\"lat\"], errors=\"coerce\").values\n    lon = pd.to_numeric(df[\"lon\"], errors=\"coerce\").values\n    lat_rad = np.deg2rad(lat); lon_rad = np.deg2rad(lon)\n    valid = np.isfinite(lat_rad) & np.isfinite(lon_rad)\n    return lat_rad, lon_rad, valid\n\ndef _safe_div(a, b):\n    b = np.asarray(b)\n    return np.where(b != 0, np.asarray(a) / b, np.nan)\n\ndef haversine_dist_m(lat1, lon1, lat2, lon2):\n    lat1 = np.deg2rad(lat1); lon1 = np.deg2rad(lon1)\n    lat2 = np.deg2rad(lat2); lon2 = np.deg2rad(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*(np.sin(dlon/2.0)**2)\n    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    return R_EARTH * c","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = smart_read_table(TRAIN_PATH, sep=\"\\t\")\ntest  = smart_read_table(TEST_PATH,  sep=\"\\t\")\nassert {\"id\",\"target\"}.issubset(train.columns) and \"id\" in test.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"KEYWORD_MIN_DF = max(8, int(0.004 * len(train)))\n\ntrain = train[train[\"target\"].fillna(0) > 0].reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for df in (train, test):\n    if \"coordinates\" in df.columns:\n        lon_lat = df[\"coordinates\"].apply(parse_coordinates_to_lon_lat)\n        df[\"lon\"] = [x[0] for x in lon_lat]\n        df[\"lat\"] = [x[1] for x in lon_lat]\n\nprint(\"Чтение и агрегация отзывов...\")\nreviews = try_read_reviews(REVIEWS_PATH_CANDIDATES)\nreviews[\"text\"] = reviews[\"text\"].astype(str).fillna(\"\")\nagg = (\n    reviews.groupby(\"id\")[\"text\"]\n    .agg([\n        (\"reviews_text\", lambda s: \" \".join(s.astype(str))),\n        (\"reviews_count\", \"size\"),\n        (\"reviews_avg_len\", lambda s: np.mean([len(t) for t in s.astype(str)]))\n    ])\n    .reset_index()\n)\ntrain = train.merge(agg, on=\"id\", how=\"left\")\ntest  = test.merge(agg, on=\"id\",  how=\"left\")\nfor df in (train, test):\n    df[\"reviews_text\"] = df[\"reviews_text\"].fillna(\"\")\n    df[\"reviews_count\"] = df[\"reviews_count\"].fillna(0).astype(int)\n    df[\"reviews_avg_len\"] = df[\"reviews_avg_len\"].fillna(0.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if USE_KEYWORD_FLAGS:\n    vect = CountVectorizer(\n        binary=True, min_df=KEYWORD_MIN_DF, max_df=KEYWORD_MAX_DF,\n        lowercase=True, token_pattern=TOKEN_PATTERN\n    )\n    X_tr_bin = vect.fit_transform(train[\"reviews_text\"].astype(str))\n    X_te_bin = vect.transform(test[\"reviews_text\"].astype(str))\n    feats = vect.get_feature_names_out()\n\n    y_arr = train[\"target\"].astype(float).values\n    n = len(y_arr); y_mean = y_arr.mean(); vary = y_arr.var() + 1e-12\n    p = (X_tr_bin.sum(axis=0).A1) / n\n    xy = (X_tr_bin.T @ y_arr) / n\n    cov = xy - y_mean * p\n    varx = p * (1 - p) + 1e-12\n    corr = cov / np.sqrt(varx * vary)\n\n    top_idx = np.argsort(-np.abs(corr))[:KEYWORD_TOP_K]\n    sel_feats = feats[top_idx]\n    X_tr_sel = X_tr_bin[:, top_idx]\n    X_te_sel = X_te_bin[:, top_idx]\n\n    sel_cols = [f\"kw_{i}_{t}\" for i, t in enumerate(sel_feats)]\n    train_kw = pd.DataFrame(X_tr_sel.toarray().astype(np.int8), index=train.index, columns=sel_cols)\n    test_kw  = pd.DataFrame(X_te_sel.toarray().astype(np.int8), index=test.index,  columns=sel_cols)\n\n    MANUAL_REGEX = {\n        \"kw_vkus\": r\"\\bвкус\\w*\", \"kw_chist\": r\"\\bчист\\w*\", \"kw_svezh\": r\"\\bсвеж\\w*\",\n        \"kw_bystr\": r\"\\bбыстр\\w*\", \"kw_medlen\": r\"\\bмедлен\\w*\", \"kw_dorog\": r\"\\bдорог\\w*\",\n        \"kw_deshev\": r\"\\bдешев\\w*\", \"kw_holodn\": r\"\\bхолодн\\w*\", \"kw_goryach\": r\"\\bгоряч\\w*\",\n        \"kw_ochered\": r\"\\bочеред\\w*\", \"kw_vejl\": r\"\\bвежлив\\w*\", \"kw_gryb\": r\"\\bгруб\\w*\",\n        \"kw_rekom\": r\"\\bрекоменд\\w*\", \"kw_ne_rekom\": r\"не\\s*рекоменд\\w*\", \"kw_servis\": r\"\\bсервис\\w*\",\n        \"kw_uzhast\": r\"\\bужас\\w*\", \"kw_otlich\": r\"\\bотлич\\w*\",\n    }\n    for col, pattern in MANUAL_REGEX.items():\n        train_kw[col] = train[\"reviews_text\"].str.contains(pattern, case=False, regex=True).fillna(False).astype(np.int8)\n        test_kw[col]  = test[\"reviews_text\"].str.contains(pattern, case=False, regex=True).fillna(False).astype(np.int8)\n\n    POS_KEYS = [\"kw_vkus\",\"kw_chist\",\"kw_svezh\",\"kw_bystr\",\"kw_vejl\",\"kw_rekom\",\"kw_otlich\",\"kw_servis\"]\n    NEG_KEYS = [\"kw_gryb\",\"kw_medlen\",\"kw_dorog\",\"kw_holodn\",\"kw_ochered\",\"kw_uzhast\",\"kw_ne_rekom\"]\n    for df_kw in (train_kw, test_kw):\n        pos_cols = [c for c in POS_KEYS if c in df_kw.columns]\n        neg_cols = [c for c in NEG_KEYS if c in df_kw.columns]\n        df_kw[\"kw_pos_hits\"] = df_kw[pos_cols].sum(axis=1).astype(np.int16) if pos_cols else 0\n        df_kw[\"kw_neg_hits\"] = df_kw[neg_cols].sum(axis=1).astype(np.int16) if neg_cols else 0\n        df_kw[\"kw_pos_any\"] = (df_kw[\"kw_pos_hits\"] > 0).astype(np.int8)\n        df_kw[\"kw_neg_any\"] = (df_kw[\"kw_neg_hits\"] > 0).astype(np.int8)\n\n    train = pd.concat([train, train_kw], axis=1)\n    test  = pd.concat([test,  test_kw],  axis=1)\n    del X_tr_bin, X_te_bin, X_tr_sel, X_te_sel, train_kw, test_kw; gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Строим TF-IDF (word + optional char) + SVD...\")\ncorpus = pd.concat([train[\"reviews_text\"], test[\"reviews_text\"]], axis=0).astype(str)\n\n# Word TF-IDF\ntfidf_word = TfidfVectorizer(\n    max_features=TFIDF_MAX_FEATURES,\n    min_df=MIN_DF,\n    max_df=MAX_DF,\n    ngram_range=NGRAM_RANGE,\n    lowercase=True,\n    strip_accents=\"unicode\",\n    sublinear_tf=True\n)\nXw = tfidf_word.fit_transform(corpus.values)\n\nif USE_CHAR_TFIDF:\n    tfidf_char = TfidfVectorizer(\n        analyzer=\"char\",\n        ngram_range=(3,5),\n        min_df=3,\n        max_df=0.98,\n        lowercase=True,\n        sublinear_tf=True\n    )\n    Xc = tfidf_char.fit_transform(corpus.values)\n    X_all = sparse.hstack([Xw, Xc]).tocsr()\n    del Xw, Xc\nelse:\n    X_all = Xw\n\nsvd = TruncatedSVD(n_components=SVD_COMPONENTS, random_state=RANDOM_STATE)\nX_svd = svd.fit_transform(X_all)\ndel X_all; gc.collect()\n\nX_svd_train = X_svd[:len(train)]\nX_svd_test  = X_svd[len(train):]\nsvd_cols = [f\"text_svd_{i}\" for i in range(X_svd.shape[1])]\nfor i, col in enumerate(svd_cols):\n    train[col] = X_svd_train[:, i]\n    test[col]  = X_svd_test[:, i]\ndel X_svd, X_svd_train, X_svd_test, corpus; gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pairs = [\n    (\"female_300m\",\"male_300m\",\"female_share_300m\"),\n    (\"female_1000m\",\"male_1000m\",\"female_share_1000m\"),\n    (\"employed_300m\",\"unemployed_300m\",\"employed_share_300m\"),\n    (\"employed_1000m\",\"unemployed_1000m\",\"employed_share_1000m\"),\n    (\"married_300m\",\"not_married_300m\",\"married_share_300m\"),\n    (\"married_1000m\",\"not_married_1000m\",\"married_share_1000m\"),\n    (\"has_children_300m\",\"no_children_300m\",\"has_children_share_300m\"),\n    (\"has_children_1000m\",\"no_children_1000m\",\"has_children_share_1000m\"),\n    (\"higher_education_300m\",\"no_higher_education_300m\",\"higher_education_share_300m\"),\n    (\"higher_education_1000m\",\"no_higher_education_1000m\",\"higher_education_share_1000m\"),\n]\nfor (a,b,nc) in pairs:\n    add_share_pair(train, a, b, nc)\n    add_share_pair(test,  a, b, nc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def samecat_counts_multi_r(train_df, test_df, radii_m=(100,200,300)):\n    all_df = pd.concat([\n        train_df[[\"id\",\"lat\",\"lon\",\"category\"]],\n        test_df[[\"id\",\"lat\",\"lon\",\"category\"]]\n    ], axis=0, ignore_index=True)\n    all_df[\"category\"] = all_df[\"category\"].astype(str).fillna(\"Unknown\")\n    lat_rad, lon_rad, valid = _to_rad_series(all_df)\n    X_valid = np.c_[lat_rad[valid], lon_rad[valid]]\n    idx_valid = np.where(valid)[0]; cats = all_df[\"category\"].values\n    tree = BallTree(X_valid, metric=\"haversine\")\n    out = {r: np.full(len(all_df), np.nan, dtype=np.float32) for r in radii_m}\n    for r_m in radii_m:\n        neigh = tree.query_radius(X_valid, r=r_m/R_EARTH, return_distance=False)\n        cnt = np.zeros(len(idx_valid), dtype=np.int32)\n        for j, nb_local in enumerate(neigh):\n            gi = idx_valid[j]\n            if nb_local.size == 0: cnt[j] = 0; continue\n            nb_global = idx_valid[nb_local]; nb_global = nb_global[nb_global != gi]\n            if nb_global.size == 0: cnt[j] = 0; continue\n            cnt[j] = int(np.sum(cats[nb_global] == cats[gi]))\n        res = np.full(len(all_df), np.nan, dtype=np.float32); res[idx_valid] = cnt\n        out[r_m] = res\n    ntr = len(train_df)\n    res_train = {r: out[r][:ntr] for r in radii_m}; res_test  = {r: out[r][ntr:] for r in radii_m}\n    return res_train, res_test\n\ncnt_tr, cnt_te = samecat_counts_multi_r(train, test, radii_m=(100,200,300))\nfor r_m in (100,200,300):\n    train[f\"samecat_cnt_r{r_m}\"] = cnt_tr[r_m]; test[f\"samecat_cnt_r{r_m}\"]  = cnt_te[r_m]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def oof_local_target_stats_by_cat(train_df, test_df, radii_m=(200,300), n_splits=N_FOLDS, seed=RANDOM_STATE):\n    y = train_df[\"target\"].astype(float).values\n    cat_tr = train_df[\"category\"].astype(str).fillna(\"Unknown\").values\n    cat_te = test_df[\"category\"].astype(str).fillna(\"Unknown\").values\n    lat_tr = pd.to_numeric(train_df[\"lat\"], errors=\"coerce\").values\n    lon_tr = pd.to_numeric(train_df[\"lon\"], errors=\"coerce\").values\n    lat_te = pd.to_numeric(test_df[\"lat\"],  errors=\"coerce\").values\n    lon_te = pd.to_numeric(test_df[\"lon\"],  errors=\"coerce\").values\n\n    bins = pd.qcut(y, q=min(10, max(2, len(y)//50)), duplicates=\"drop\")\n    y_bins = pd.factorize(bins)[0]\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    res_tr = {r: dict(mean=np.full(len(train_df), np.nan, dtype=np.float32),\n                      median=np.full(len(train_df), np.nan, dtype=np.float32),\n                      count=np.zeros(len(train_df), dtype=np.float32)) for r in radii_m}\n    res_te = {r: dict(mean=np.full(len(test_df), np.nan, dtype=np.float32),\n                      median=np.full(len(test_df), np.nan, dtype=np.float32),\n                      count=np.zeros(len(test_df), dtype=np.float32)) for r in radii_m}\n\n    valid_tr = np.isfinite(lat_tr) & np.isfinite(lon_tr)\n    valid_te = np.isfinite(lat_te) & np.isfinite(lon_te)\n    X_tr_all = np.c_[np.deg2rad(np.where(valid_tr, lat_tr, 0.0)), np.deg2rad(np.where(valid_tr, lon_tr, 0.0))]\n    X_te_all = np.c_[np.deg2rad(np.where(valid_te, lat_te, 0.0)), np.deg2rad(np.where(valid_te, lon_te, 0.0))]\n\n    for r_m in radii_m:\n        r = r_m / R_EARTH\n        for tr_idx, va_idx in skf.split(train_df, y_bins):\n            tr_idx_valid = tr_idx[valid_tr[tr_idx]]\n            va_idx_valid = va_idx[valid_tr[va_idx]]\n            if len(tr_idx_valid) == 0 or len(va_idx_valid) == 0: continue\n            Xtr = X_tr_all[tr_idx_valid]; ytr = y[tr_idx_valid]\n            cat_tr_fold = cat_tr[tr_idx_valid]; cat_va_fold = cat_tr[va_idx_valid]\n            tree = BallTree(Xtr, metric=\"haversine\")\n            neigh = tree.query_radius(X_tr_all[va_idx_valid], r=r, return_distance=False)\n            for j, nb in enumerate(neigh):\n                if nb.size == 0: continue\n                nb = nb[cat_tr_fold[nb] == cat_va_fold[j]]\n                if nb.size == 0: continue\n                vals = ytr[nb]\n                res_tr[r_m][\"mean\"][va_idx_valid[j]] = float(np.mean(vals))\n                res_tr[r_m][\"median\"][va_idx_valid[j]] = float(np.median(vals))\n                res_tr[r_m][\"count\"][va_idx_valid[j]]  = float(len(vals))\n        tr_all_valid_idx = np.where(valid_tr)[0]\n        if len(tr_all_valid_idx) > 0 and np.any(valid_te):\n            Xtr = X_tr_all[tr_all_valid_idx]; ytr = y[tr_all_valid_idx]\n            cat_tr_all = cat_tr[tr_all_valid_idx]\n            tree_full = BallTree(Xtr, metric=\"haversine\")\n            te_valid_idx = np.where(valid_te)[0]\n            neigh_te = tree_full.query_radius(X_te_all[te_valid_idx], r=r, return_distance=False)\n            for j, nb in enumerate(neigh_te):\n                if nb.size == 0: continue\n                idx = te_valid_idx[j]\n                nb = nb[cat_tr_all[nb] == cat_te[idx]]\n                if nb.size == 0: continue\n                vals = ytr[nb]\n                res_te[r_m][\"mean\"][idx]   = float(np.mean(vals))\n                res_te[r_m][\"median\"][idx] = float(np.median(vals))\n                res_te[r_m][\"count\"][idx]  = float(len(vals))\n    return res_tr, res_te\n\nloc_tr, loc_te = oof_local_target_stats_by_cat(train, test, radii_m=(200,300), n_splits=N_FOLDS, seed=RANDOM_STATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for r_m in (200, 300):\n    train[f\"loc_cat_mean_r{r_m}\"]   = loc_tr[r_m][\"mean\"]\n    train[f\"loc_cat_median_r{r_m}\"] = loc_tr[r_m][\"median\"]\n    train[f\"loc_cat_count_r{r_m}\"]  = loc_tr[r_m][\"count\"]\n    test[f\"loc_cat_mean_r{r_m}\"]    = loc_te[r_m][\"mean\"]\n    test[f\"loc_cat_median_r{r_m}\"]  = loc_te[r_m][\"median\"]\n    test[f\"loc_cat_count_r{r_m}\"]   = loc_te[r_m][\"count\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_location_interactions(df: pd.DataFrame):\n    for suf in (\"300m\",\"1000m\"):\n        if f\"traffic_{suf}\" in df.columns and f\"homes_{suf}\" in df.columns:\n            df[f\"traffic_per_home_{suf}\"] = _safe_div(pd.to_numeric(df[f\"traffic_{suf}\"], errors=\"coerce\"),\n                                                      (pd.to_numeric(df[f\"homes_{suf}\"], errors=\"coerce\") + 1))\n        if f\"works_{suf}\" in df.columns and f\"homes_{suf}\" in df.columns:\n            df[f\"works_to_homes_{suf}\"] = _safe_div(pd.to_numeric(df[f\"works_{suf}\"], errors=\"coerce\"),\n                                                    (pd.to_numeric(df[f\"homes_{suf}\"], errors=\"coerce\") + 1))\n    income_cols = [\"below_average_income\",\"average_income\",\"above_average_income\",\"high_income\",\"premium_income\"]\n    for rad in (\"300m\",\"1000m\"):\n        present = [c for c in income_cols if f\"{c}_{rad}\" in df.columns]\n        if present:\n            total = np.zeros(len(df), dtype=np.float64)\n            for c in present:\n                total += pd.to_numeric(df[f\"{c}_{rad}\"], errors=\"coerce\").fillna(0).values\n            total[total == 0] = np.nan\n            if f\"below_average_income_{rad}\" in df.columns:\n                df[f\"low_income_share_{rad}\"] = pd.to_numeric(df[f\"below_average_income_{rad}\"], errors=\"coerce\") / total\n            hi_sum = np.zeros(len(df), dtype=np.float64)\n            for c in (\"high_income\",\"premium_income\"):\n                if f\"{c}_{rad}\" in df.columns:\n                    hi_sum += pd.to_numeric(df[f\"{c}_{rad}\"], errors=\"coerce\").fillna(0).values\n            df[f\"high_income_share_{rad}\"] = _safe_div(hi_sum, total)\n    for r_m in (200, 300):\n        if f\"samecat_cnt_r{r_m}\" in df.columns:\n            if \"low_income_share_300m\" in df.columns:\n                df[f\"samecat_x_lowincome_r{r_m}\"]  = df[f\"samecat_cnt_r{r_m}\"] * df[\"low_income_share_300m\"]\n            if \"high_income_share_300m\" in df.columns:\n                df[f\"samecat_x_highincome_r{r_m}\"] = df[f\"samecat_cnt_r{r_m}\"] * df[\"high_income_share_300m\"]\n\nadd_location_interactions(train); add_location_interactions(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_dist_to_center(train_df, test_df, lat_col=\"lat\", lon_col=\"lon\",\n                       center_lat=MOSCOW_CENTER_LAT, center_lon=MOSCOW_CENTER_LON):\n    for df in (train_df, test_df):\n        df[\"dist_to_center_m\"] = haversine_dist_m(\n            pd.to_numeric(df[lat_col], errors=\"coerce\"),\n            pd.to_numeric(df[lon_col], errors=\"coerce\"),\n            center_lat, center_lon\n        )\n\nadd_dist_to_center(train, test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"METRO_STATIONS = [\n    (\"Охотный ряд\",55.75703,37.61614), (\"Октябрьское поле\",55.793615,37.493496),\n    (\"Октябрьская\",55.729,37.61139), (\"Орехово\",55.61214,37.69584),\n    (\"Отрадное\",55.86417,37.60488), (\"Парк культуры\",55.73512,37.59328),\n    (\"Парк Победы (стр.)\",55.736559,37.512591), (\"Партизанская\",55.78962,37.7479),\n    (\"Павелецкая\",55.7313,37.63612), (\"Печатники\",55.69252,37.7295),\n    (\"Перово\",55.75109,37.78854), (\"Первомайская\",55.79342,37.79979),\n    (\"Петровско-Разумовская\",55.83712,37.57349), (\"Пионерская\",55.73583,37.46731),\n    (\"Планерная\",55.85931,37.43687), (\"Площадь Ильича\",55.745663,37.681123),\n    (\"Улица Подбельского\",55.81503,37.73209), (\"Улица Подбельского\",55.81336,37.73524),\n    (\"Полежаевская\",55.77691,37.51692), (\"Полянка\",55.73654,37.61856),\n    (\"Пражская\",55.61354,37.60499), (\"Преображенская площадь\",55.79655,37.71591),\n    (\"Профсоюзная\",55.67822,37.56381), (\"Пролетарская\",55.73171,37.66726),\n    (\"Пронская (стр.)\",55.698344,37.850869), (\"Пушкинская\",55.76565,37.60417),\n    (\"Речной вокзал\",55.85378,37.47679), (\"Площадь Революции\",55.75646,37.62321),\n    (\"Рижская\",55.79222,37.63557), (\"Римская\",55.746487,37.682631),\n    (\"Рязанский проспект\",55.71753,37.79425), (\"Cавеловская\",55.79421,37.58666),\n    (\"Щелковская\",55.80955,37.79884), (\"Щукинская\",55.80796,37.46629),\n    (\"Cеменовская\",55.78279,37.71844), (\"Cерпуховская\",55.72658,37.62462),\n    (\"Cевастопольская\",55.65121,37.59939), (\"Шаболовская\",55.71886,37.60797),\n    (\"Cходненская\",55.84937,37.43951), (\"Cлавянский бульвар (стр.)\",55.729508,37.468829),\n    (\"Cмоленская\",55.74823,37.58384), (\"Cокол\",55.80518,37.51495),\n    (\"Cокольники\",55.78893,37.67943), (\"Cпортивная\",55.72397,37.56547),\n    (\"Cретенский бульвар (стр.)\",55.765551,37.635261), (\"Cтрогино (стр.)\",55.80435,37.396363),\n    (\"Cтуденческая\",55.73873,37.54825), (\"Cухаревская\",55.77211,37.63239),\n    (\"Площадь Cуворова (стр.)\",55.781984,37.614487), (\"Cвиблово\",55.85543,37.65419),\n    (\"Таганская\",55.74255,37.65389), (\"Театральная\",55.75857,37.6177),\n    (\"Текстильшики\",55.70947,37.73282), (\"Теплый стан\",55.61814,37.50814),\n    (\"Тимирязевская\",55.81842,37.57571), (\"Третьяковская\",55.74061,37.62492),\n    (\"Трубная (стр.)\",55.767605,37.6221), (\"Царицино\",55.62011,37.66939),\n    (\"Цветной бульвар\",55.7716,37.62058), (\"Тульская\",55.70901,37.6226),\n    (\"Тургеневская\",55.7646,37.63623), (\"Тушинская\",55.8258,37.43621),\n    (\"Тверская\",55.7652,37.60352), (\"Улица 1905 года\",55.76355,37.56375),\n    (\"Университет\",55.69167,37.53433), (\"Варшавская\",55.65381,37.62084),\n    (\"ВДНХ\",55.82177,37.64107), (\"Проспект Вернадского\",55.67613,37.5045),\n    (\"Владыкино\",55.84669,37.59251), (\"Водный стадион\",55.8386,37.48749),\n    (\"Войковская\",55.81811,37.49905), (\"Волоколамская (стр.)\",55.83459,37.38367),\n    (\"Волгоградский проспект\",55.7243,37.68795), (\"Волжская\",55.69101,37.75498),\n    (\"Воробьёвы горы\",55.710454,37.558601), (\"Выхино\",55.715,37.81802),\n    (\"Ясенево\",55.60535,37.53494), (\"Юго-западная\",55.66464,37.48421)\n]\nmetro_df = pd.DataFrame(METRO_STATIONS, columns=[\"name\",\"lat\",\"lon\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_nearest_poi_features(train_df, test_df, poi_df,\n                             lat_col=\"lat\", lon_col=\"lon\",\n                             ks=(1,3,5), radii=(300, 500, 1000),\n                             prefix=\"metro\"):\n    if poi_df is None or len(poi_df)==0:\n        print(f\"[{prefix}] empty — skipping.\")\n        return\n    poi_lat = np.deg2rad(pd.to_numeric(poi_df[\"lat\"], errors=\"coerce\").values)\n    poi_lon = np.deg2rad(pd.to_numeric(poi_df[\"lon\"], errors=\"coerce\").values)\n    mask_poi = np.isfinite(poi_lat) & np.isfinite(poi_lon)\n    X_poi = np.c_[poi_lat[mask_poi], poi_lon[mask_poi]]\n    if X_poi.shape[0] == 0:\n        print(f\"[{prefix}] no valid points — skipping.\")\n        return\n    tree = BallTree(X_poi, metric=\"haversine\")\n\n    def _process(df):\n        lat = pd.to_numeric(df[lat_col], errors=\"coerce\").values\n        lon = pd.to_numeric(df[lon_col], errors=\"coerce\").values\n        valid = np.isfinite(lat) & np.isfinite(lon)\n        X = np.c_[np.deg2rad(np.where(valid, lat, 0.0)),\n                  np.deg2rad(np.where(valid, lon, 0.0))]\n\n        for k in ks:\n            k_eff = min(k, X_poi.shape[0])\n            dist, _ = tree.query(X[valid], k=k_eff)\n            dist_m = dist * R_EARTH\n            if k_eff == 1:\n                out = np.full(len(df), np.nan, dtype=np.float32)\n                out[valid] = dist_m[:, 0]\n                df[f\"{prefix}_dist_min_m\"] = out\n            out_mean = np.full(len(df), np.nan, dtype=np.float32)\n            out_mean[valid] = dist_m.mean(axis=1)\n            df[f\"{prefix}_mean_k{k_eff}_m\"] = out_mean\n\n        for r in radii:\n            ind = tree.query_radius(X[valid], r=(r / R_EARTH), count_only=False, return_distance=False)\n            cnt = np.zeros(np.sum(valid), dtype=np.int32)\n            for i, nb in enumerate(ind):\n                cnt[i] = len(nb)\n            out = np.full(len(df), np.nan, dtype=np.float32)\n            out[valid] = cnt\n            df[f\"{prefix}_cnt_r{r}\"] = out\n\n    _process(train_df); _process(test_df)\n\nadd_nearest_poi_features(train, test, metro_df, ks=(1,3,5), radii=(300,500,1000), prefix=\"metro\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for df in (train, test):\n    if \"traffic_300m\" in df.columns:\n        for r in (300, 500, 1000):\n            df[f\"metro_per1k_traffic_r{r}\"] = _safe_div(\n                df.get(f\"metro_cnt_r{r}\", np.nan),\n                _safe_div(pd.to_numeric(df[\"traffic_300m\"], errors=\"coerce\"), 1000.0)\n            )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if USE_RUBERT:\n    try:\n        import torch\n        from sentence_transformers import SentenceTransformer\n        from sklearn.linear_model import Ridge\n\n        train_ids_set = set(train[\"id\"].tolist())\n        test_ids_set  = set(test[\"id\"].tolist())\n\n        tr_rev = reviews[reviews[\"id\"].isin(train_ids_set)].merge(\n            train[[\"id\",\"target\"]], on=\"id\", how=\"left\"\n        ).dropna(subset=[\"target\"]).reset_index(drop=True)\n        te_rev = reviews[reviews[\"id\"].isin(test_ids_set)].reset_index(drop=True)\n\n        device = \"cuda\" if (USE_GPU and torch.cuda.is_available()) else \"cpu\"\n        print(f\"[RuBERT] Loading model: {BERT_MODEL} on {device}\")\n        bert = SentenceTransformer(BERT_MODEL, device=device)\n\n        def encode_texts(texts):\n            return bert.encode(\n                list(texts),\n                batch_size=BERT_BATCH_SIZE,\n                convert_to_numpy=True,\n                show_progress_bar=True,\n                normalize_embeddings=BERT_NORMALIZE,\n            ).astype(np.float32)\n\n        print(\"[RuBERT] Encoding train reviews...\")\n        X_tr_rev = encode_texts(tr_rev[\"text\"].astype(str))\n        print(\"[RuBERT] Encoding test reviews...\")\n        X_te_rev = encode_texts(te_rev[\"text\"].astype(str))\n        y_rev = tr_rev[\"target\"].astype(float).values\n\n        uniq_ids = train[\"id\"].values\n        uniq_targets = train[\"target\"].astype(float).values\n        bins_ids = pd.qcut(uniq_targets, q=min(10, max(2, len(uniq_targets)//50)), duplicates=\"drop\")\n        y_bins_ids = pd.factorize(bins_ids)[0]\n        skf_ids = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n\n        oof_pred_rev = np.zeros(len(tr_rev), dtype=np.float32)\n        te_pred_accum = np.zeros(len(te_rev), dtype=np.float32)\n        id_arr_tr = tr_rev[\"id\"].values\n\n        print(\"[RuBERT] OOF training Ridge head...\")\n        for fold, (tr_id_idx, va_id_idx) in enumerate(skf_ids.split(uniq_ids, y_bins_ids), 1):\n            tr_ids_fold = set(uniq_ids[tr_id_idx]); va_ids_fold = set(uniq_ids[va_id_idx])\n            tr_mask = np.isin(id_arr_tr, list(tr_ids_fold))\n            va_mask = np.isin(id_arr_tr, list(va_ids_fold))\n\n            head = Ridge(alpha=RIDGE_ALPHA, solver=RIDGE_SOLVER, tol=1e-3, max_iter=2000, fit_intercept=True)\n            head.fit(X_tr_rev[tr_mask], y_rev[tr_mask])\n\n            oof_pred_rev[va_mask] = head.predict(X_tr_rev[va_mask]).astype(np.float32)\n            te_pred_accum += head.predict(X_te_rev).astype(np.float32) / N_FOLDS\n\n            print(f\"[RuBERT][Fold {fold}] train_reviews={tr_mask.sum()}, val_reviews={va_mask.sum()}\")\n\n        tr_rev[\"rev_bert_oof\"] = oof_pred_rev\n        te_rev[\"rev_bert_pred\"] = te_pred_accum\n\n        tr_agg = (\n            tr_rev.groupby(\"id\")[\"rev_bert_oof\"]\n            .agg([\"mean\",\"median\",\"std\",\"min\",\"max\",\"size\"])\n            .reset_index()\n            .rename(columns={\n                \"mean\":\"rev_bert_mean\",\"median\":\"rev_bert_median\",\"std\":\"rev_bert_std\",\n                \"min\":\"rev_bert_min\",\"max\":\"rev_bert_max\",\"size\":\"rev_bert_cnt\"\n            })\n        )\n        te_agg = (\n            te_rev.groupby(\"id\")[\"rev_bert_pred\"]\n            .agg([\"mean\",\"median\",\"std\",\"min\",\"max\",\"size\"])\n            .reset_index()\n            .rename(columns={\n                \"mean\":\"rev_bert_mean\",\"median\":\"rev_bert_median\",\"std\":\"rev_bert_std\",\n                \"min\":\"rev_bert_min\",\"max\":\"rev_bert_max\",\"size\":\"rev_bert_cnt\"\n            })\n        )\n        train = train.merge(tr_agg, on=\"id\", how=\"left\")\n        test  = test.merge(te_agg, on=\"id\",  how=\"left\")\n\n        def mean_embed_by_id(ids, X):\n            codes, uniques = pd.factorize(ids)\n            K = len(uniques); D = X.shape[1]\n            sum_mat = np.zeros((K, D), dtype=np.float32)\n            cnt = np.zeros(K, dtype=np.int32)\n            for i, c in enumerate(codes):\n                if c >= 0:\n                    sum_mat[c] += X[i]; cnt[c] += 1\n            cnt[cnt == 0] = 1\n            mean_mat = sum_mat / cnt[:, None]\n            id2row = {u: i for i, u in enumerate(uniques)}\n            return mean_mat, id2row\n\n        tr_mean_emb, tr_map = mean_embed_by_id(tr_rev[\"id\"].values, X_tr_rev)\n        te_mean_emb, te_map = mean_embed_by_id(te_rev[\"id\"].values, X_te_rev)\n\n        D = tr_mean_emb.shape[1]\n        train_mean = np.zeros((len(train), D), dtype=np.float32)\n        test_mean  = np.zeros((len(test),  D), dtype=np.float32)\n        for i, pid in enumerate(train[\"id\"].values):\n            row = tr_map.get(pid, None)\n            if row is not None: train_mean[i] = tr_mean_emb[row]\n        for i, pid in enumerate(test[\"id\"].values):\n            row = te_map.get(pid, None)\n            if row is not None: test_mean[i] = te_mean_emb[row]\n\n        all_mean = np.vstack([train_mean, test_mean])\n\n        if BERT_SVD_COMPONENTS and all_mean.shape[1] > BERT_SVD_COMPONENTS:\n            bert_svd = TruncatedSVD(n_components=BERT_SVD_COMPONENTS, random_state=RANDOM_STATE)\n            all_svd = bert_svd.fit_transform(all_mean)\n            tr_svd = all_svd[:len(train)]; te_svd = all_svd[len(train):]\n            bert_svd_cols = [f\"bert_svd_{i}\" for i in range(BERT_SVD_COMPONENTS)]\n            for i, col in enumerate(bert_svd_cols):\n                train[col] = tr_svd[:, i]; test[col] = te_svd[:, i]\n        else:\n            bert_svd_cols = []\n\n        raw_k = min(BERT_RAW_KEEP_DIMS, all_mean.shape[1])\n        tr_raw = all_mean[:len(train), :raw_k]\n        te_raw = all_mean[len(train):, :raw_k]\n        bert_raw_cols = [f\"bert_raw_{i}\" for i in range(raw_k)]\n        for i, col in enumerate(bert_raw_cols):\n            train[col] = tr_raw[:, i]; test[col] = te_raw[:, i]\n\n        del X_tr_rev, X_te_rev, tr_mean_emb, te_mean_emb, train_mean, test_mean, all_mean\n        gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_samecat_knn_distance(train_df, test_df,\n                             ks=(1, 3, 5),\n                             lat_col=\"lat\", lon_col=\"lon\", cat_col=\"category\"):\n    all_df = pd.concat([\n        train_df[[lat_col, lon_col, cat_col]].copy(),\n        test_df[[lat_col, lon_col, cat_col]].copy()\n    ], axis=0, ignore_index=True)\n\n    cats = all_df[cat_col].astype(str).fillna(\"Unknown\").values\n    lat = pd.to_numeric(all_df[lat_col], errors=\"coerce\").values\n    lon = pd.to_numeric(all_df[lon_col], errors=\"coerce\").values\n    lat_rad = np.deg2rad(lat); lon_rad = np.deg2rad(lon)\n    valid = np.isfinite(lat_rad) & np.isfinite(lon_rad)\n\n    n_all = len(all_df)\n    out_min  = np.full(n_all, np.nan, dtype=np.float32)\n    out_k3   = np.full(n_all, np.nan, dtype=np.float32)\n    out_k5   = np.full(n_all, np.nan, dtype=np.float32)\n\n    uniq_cats = pd.unique(cats[valid])\n    for cat in uniq_cats:\n        mask_cat = (cats == cat) & valid\n        idx_cat = np.where(mask_cat)[0]\n        if idx_cat.size <= 1:\n            continue\n\n        X = np.c_[lat_rad[idx_cat], lon_rad[idx_cat]]\n        tree = BallTree(X, metric=\"haversine\")\n\n        def knn_mean(k_need):\n            kq = min(k_need + 1, X.shape[0])\n            dist_rad, _ = tree.query(X, k=kq)\n            dist_rad = dist_rad[:, 1:]\n            if dist_rad.shape[1] == 0:\n                return np.full(X.shape[0], np.nan, dtype=np.float32)\n            return (dist_rad.mean(axis=1) * R_EARTH).astype(np.float32)\n\n        k1q = min(2, X.shape[0])\n        d1_rad, _ = tree.query(X, k=k1q)\n        if k1q > 1:\n            d1 = (d1_rad[:, 1] * R_EARTH).astype(np.float32)\n            out_min[idx_cat] = d1\n\n        out_k3[idx_cat] = knn_mean(3)\n        out_k5[idx_cat] = knn_mean(5)\n\n    n_tr = len(train_df)\n    train_df[\"dist_samecat_min_m\"]     = out_min[:n_tr]\n    train_df[\"dist_samecat_mean_k3_m\"] = out_k3[:n_tr]\n    train_df[\"dist_samecat_mean_k5_m\"] = out_k5[:n_tr]\n\n    test_df[\"dist_samecat_min_m\"]      = out_min[n_tr:]\n    test_df[\"dist_samecat_mean_k3_m\"]  = out_k3[n_tr:]\n    test_df[\"dist_samecat_mean_k5_m\"]  = out_k5[n_tr:]\n    \nadd_samecat_knn_distance(train, test, ks=(1,3,5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _make_grid_key(df, step_deg=0.01):\n    lat = pd.to_numeric(df[\"lat\"], errors=\"coerce\").to_numpy()\n    lon = pd.to_numeric(df[\"lon\"], errors=\"coerce\").to_numpy()\n    cat = df[\"category\"].astype(\"string\").fillna(\"Unknown\").to_numpy(dtype=\"U\")\n\n    ok = np.isfinite(lat) & np.isfinite(lon)\n\n    glat = np.empty_like(lat); glat.fill(np.nan)\n    glon = np.empty_like(lon); glon.fill(np.nan)\n    glat[ok] = np.floor(lat[ok] / step_deg)\n    glon[ok] = np.floor(lon[ok] / step_deg)\n\n    key = np.full(len(df), None, dtype=object)\n    if np.any(ok):\n        lat_str = glat[ok].astype(np.int64).astype(\"U\")\n        lon_str = glon[ok].astype(np.int64).astype(\"U\")\n        cat_ok  = cat[ok].astype(\"U\")\n\n        s = np.char.add(np.char.add(np.char.add(lat_str, \"_\"), lon_str), \"_\")\n        key[ok] = np.char.add(s, cat_ok)\n\n    return key\n\n\n\ndef add_oof_grid_cat_mean(train_df, test_df, y_col=\"target\",\n                          steps=(0.01, 0.02),\n                          prior=10.0, n_splits=5, seed=42):\n    y = pd.to_numeric(train_df[y_col], errors=\"coerce\").values.astype(float)\n    bins = pd.qcut(y, q=min(10, max(2, len(y)//50)), duplicates=\"drop\")\n    y_bins = pd.factorize(bins)[0]\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    global_mean = np.nanmean(y)\n\n    for step in steps:\n        tag = str(step).replace(\".\", \"p\")\n        k_tr_all = _make_grid_key(train_df, step_deg=step)\n        k_te     = _make_grid_key(test_df,  step_deg=step)\n\n        oof_mean = np.full(len(train_df), np.nan, dtype=np.float32)\n        oof_cnt  = np.zeros(len(train_df), dtype=np.float32)\n\n        for tr_idx, va_idx in skf.split(train_df, y_bins):\n            k_tr = k_tr_all[tr_idx]; y_tr = y[tr_idx]\n            g = pd.DataFrame({\"k\": k_tr, \"y\": y_tr}).dropna()\n            gb = g.groupby(\"k\")[\"y\"].agg([\"sum\",\"count\"])\n            sm = gb[\"sum\"].to_dict(); ct = gb[\"count\"].to_dict()\n\n            keys_va = k_tr_all[va_idx]\n            m = np.array([(sm.get(k, 0.0) + prior*global_mean) / (ct.get(k, 0.0) + prior) if k is not None else global_mean\n                          for k in keys_va], dtype=np.float32)\n            c = np.array([ct.get(k, 0.0) if k is not None else 0.0 for k in keys_va], dtype=np.float32)\n\n            oof_mean[va_idx] = m\n            oof_cnt[va_idx]  = c\n\n        train_df[f\"oof_grid{tag}_cat_mean\"] = oof_mean\n        train_df[f\"grid{tag}_cat_cnt\"]      = oof_cnt\n\n        # фичи для test — по full train\n        gfull = pd.DataFrame({\"k\": k_tr_all, \"y\": y}).dropna()\n        gbf = gfull.groupby(\"k\")[\"y\"].agg([\"sum\",\"count\"])\n        smf = gbf[\"sum\"].to_dict(); ctf = gbf[\"count\"].to_dict()\n        te_mean = np.array([(smf.get(k, 0.0) + prior*global_mean) / (ctf.get(k, 0.0) + prior) if k is not None else global_mean\n                            for k in k_te], dtype=np.float32)\n        te_cnt  = np.array([ctf.get(k, 0.0) if k is not None else 0.0 for k in k_te], dtype=np.float32)\n\n        test_df[f\"oof_grid{tag}_cat_mean\"] = te_mean\n        test_df[f\"grid{tag}_cat_cnt\"]      = te_cnt\n\nadd_oof_grid_cat_mean(train, test, steps=(0.01, 0.02), prior=12.0, n_splits=N_FOLDS, seed=RANDOM_STATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_metro_access_score(train_df, test_df, poi_df, radius_m=1500, eps=50.0, prefix=\"metro\"):\n    poi_lat = np.deg2rad(pd.to_numeric(poi_df[\"lat\"], errors=\"coerce\").values)\n    poi_lon = np.deg2rad(pd.to_numeric(poi_df[\"lon\"], errors=\"coerce\").values)\n    ok = np.isfinite(poi_lat) & np.isfinite(poi_lon)\n    X_poi = np.c_[poi_lat[ok], poi_lon[ok]]\n    if X_poi.shape[0] == 0:\n        return\n    tree = BallTree(X_poi, metric=\"haversine\")\n\n    def process(df):\n        la = pd.to_numeric(df[\"lat\"], errors=\"coerce\").values\n        lo = pd.to_numeric(df[\"lon\"], errors=\"coerce\").values\n        v = np.isfinite(la) & np.isfinite(lo)\n        X = np.c_[np.deg2rad(np.where(v, la, 0.0)), np.deg2rad(np.where(v, lo, 0.0))]\n        ind, dist = tree.query_radius(X[v], r=radius_m/R_EARTH, return_distance=True, sort_results=True)\n        invsum = np.zeros(np.sum(v), dtype=np.float32)\n        inv2sum = np.zeros(np.sum(v), dtype=np.float32)\n        for i, d in enumerate(dist):\n            m = d * R_EARTH\n            invsum[i] = np.sum(1.0 / (eps + m))\n            inv2sum[i] = np.sum(1.0 / ((eps + m)**2))\n        out1 = np.full(len(df), np.nan, dtype=np.float32); out1[v] = invsum\n        out2 = np.full(len(df), np.nan, dtype=np.float32); out2[v] = inv2sum\n        df[f\"{prefix}_invsum_r{radius_m}\"]  = out1\n        df[f\"{prefix}_inv2sum_r{radius_m}\"] = out2\n\n    process(train_df); process(test_df)\n\nadd_metro_access_score(train, test, metro_df, radius_m=1500, eps=50.0, prefix=\"metro\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_center_rings(train_df, test_df, rings_km=(3,6,10,15)):\n    for df in (train_df, test_df):\n        d = pd.to_numeric(df[\"dist_to_center_m\"], errors=\"coerce\")\n        df[\"center_logdist\"]   = np.log1p(d)\n        df[\"center_invdist_km\"] = 1.0 / (1e-3 + d/1000.0)\n        bins = [0] + [r*1000 for r in rings_km] + [np.inf]\n        df[\"center_ring_id\"] = pd.cut(d, bins=bins, labels=False, right=True).astype(\"float32\")\n\n        if \"traffic_300m\" in df.columns:\n            df[\"center_invdist_x_traffic\"] = df[\"center_invdist_km\"] * pd.to_numeric(df[\"traffic_300m\"], errors=\"coerce\")\n        if \"samecat_cnt_r300\" in df.columns:\n            df[\"center_invdist_x_comp300\"] = df[\"center_invdist_km\"] * pd.to_numeric(df[\"samecat_cnt_r300\"], errors=\"coerce\")\n\nadd_center_rings(train, test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_text_geo_interactions(df):\n    for tcol in [c for c in df.columns if c in (\"tfidf_pls16_oof\",\"tfidf_ridge_oof\",\"tokte_oof\",\"text_cb_oof\")]:\n        if \"center_invdist_km\" in df.columns:\n            df[f\"{tcol}_x_centerinv\"] = df[tcol] * df[\"center_invdist_km\"]\n        if \"metro_invsum_r1500\" in df.columns:\n            df[f\"{tcol}_x_metroacc\"] = df[tcol] * df[\"metro_invsum_r1500\"]\n        if \"samecat_cnt_r300\" in df.columns:\n            df[f\"{tcol}_x_comp300\"] = df[tcol] * df[\"samecat_cnt_r300\"]\n\nadd_text_geo_interactions(train); add_text_geo_interactions(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_oof_linear_head(train_df, test_df, feature_cols, y, name=\"tfidf_ridge\",\n                        alpha=3.0, n_splits=5, seed=42):\n    X_tr = train_df[feature_cols].astype(np.float32).fillna(0.0).to_numpy()\n    X_te = test_df[feature_cols].astype(np.float32).fillna(0.0).to_numpy()\n    y_all = np.asarray(pd.to_numeric(y, errors=\"coerce\"), dtype=np.float32)\n\n    bins = pd.qcut(y_all, q=min(10, max(2, len(y_all)//50)), duplicates=\"drop\")\n    y_bins = pd.factorize(bins)[0]\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = np.zeros(len(train_df), dtype=np.float32)\n    te  = np.zeros(len(test_df),  dtype=np.float32)\n\n    for tr_idx, va_idx in skf.split(X_tr, y_bins):\n        mdl = Ridge(alpha=alpha, solver=\"auto\", random_state=seed)\n        mdl.fit(X_tr[tr_idx], y_all[tr_idx])\n        oof[va_idx] = mdl.predict(X_tr[va_idx]).astype(np.float32)\n        te += (mdl.predict(X_te).astype(np.float32) / n_splits)\n\n    train_df[f\"{name}_oof\"] = oof\n    test_df[f\"{name}_pred\"] = te\n\n# вызов\nsvd_cols = [c for c in train.columns if c.startswith(\"text_svd_\")]\nadd_oof_linear_head(train, test, svd_cols, y=train[\"target\"].values, name=\"tfidf_ridge\", alpha=3.0,n_splits=N_FOLDS, seed=RANDOM_STATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_oof_pls_head(train_df, test_df, feature_cols, y, n_comp=16,\n                     name=\"pls\", n_splits=5, seed=42):\n    X_tr = train_df[feature_cols].astype(np.float32).fillna(0.0).to_numpy()\n    X_te = test_df[feature_cols].astype(np.float32).fillna(0.0).to_numpy()\n    y_all = np.asarray(pd.to_numeric(y, errors=\"coerce\"), dtype=np.float32)\n\n    bins = pd.qcut(y_all, q=min(10, max(2, len(y_all)//50)), duplicates=\"drop\")\n    y_bins = pd.factorize(bins)[0]\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = np.zeros(len(train_df), dtype=np.float32)\n    te  = np.zeros(len(test_df),  dtype=np.float32)\n\n    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_tr, y_bins), 1):\n        xs = StandardScaler(with_mean=True, with_std=True)\n        ys = StandardScaler(with_mean=True, with_std=True)\n\n        Xtr_f = xs.fit_transform(X_tr[tr_idx])\n        Xva_f = xs.transform(X_tr[va_idx])\n        Xte_f = xs.transform(X_te)\n\n        ytr_f = ys.fit_transform(y_all[tr_idx].reshape(-1, 1)).ravel()\n\n        n_comp_eff = int(min(n_comp, Xtr_f.shape[1], len(tr_idx) - 1))\n        if n_comp_eff < 1:\n            oof[va_idx] = ys.inverse_transform(\n                np.full((len(va_idx), 1), ytr_f.mean(), dtype=np.float32)\n            )[:, 0]\n            te += ys.inverse_transform(\n                np.full((len(X_te), 1), ytr_f.mean(), dtype=np.float32)\n            )[:, 0] / n_splits\n            print(f\"[PLS-{name}] fold {fold}: n_comp_eff<1, used mean fallback\")\n            continue\n\n        pls = PLSRegression(n_components=n_comp_eff, scale=False)\n        pls.fit(Xtr_f, ytr_f)\n\n        oof_pred = ys.inverse_transform(pls.predict(Xva_f))[:, 0]\n        te_pred  = ys.inverse_transform(pls.predict(Xte_f))[:, 0]\n\n        oof[va_idx] = oof_pred.astype(np.float32)\n        te += (te_pred.astype(np.float32) / n_splits)\n\n        print(f\"[PLS-{name}] fold {fold}: n_comp={n_comp_eff}, \"\n              f\"train={len(tr_idx)}, val={len(va_idx)}\")\n\n    train_df[f\"{name}_oof\"] = oof\n    test_df[f\"{name}_pred\"] = te\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"svd_cols = [c for c in train.columns if c.startswith(\"text_svd_\")]\n\n\nfor k in (10, 16, 22):\n    add_oof_pls_head(train, test, svd_cols, y=train[\"target\"].astype(float).values,\n                     n_comp=k, name=f\"tfidf_pls{k}\", n_splits=N_FOLDS, seed=RANDOM_STATE)\n\n    \nbert_pls_cols = [c for c in train.columns if c.startswith(\"bert_svd_\") or c.startswith(\"bert_raw_\")]\nif len(bert_pls_cols) >= 8:\n    add_oof_pls_head(\n        train, test,\n        feature_cols=bert_pls_cols,\n        y=train[\"target\"].astype(float).values,\n        n_comp=8,\n        name=\"bert_pls8\",\n        n_splits=N_FOLDS, seed=RANDOM_STATE\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_oof_pls_per_category(train_df, test_df, feature_cols, y,\n                             cat_col=\"category\", min_size=250,\n                             n_comp=12, name=\"tfidf_pls_cat\",\n                             n_splits=5, seed=42):\n    add_oof_pls_head(train_df, test_df, feature_cols, y,\n                     n_comp=n_comp, name=f\"{name}_global\",\n                     n_splits=n_splits, seed=seed)\n\n    key = train_df[cat_col].astype(str).fillna(\"Unknown\")\n    sizes = key.value_counts()\n    big = set(sizes[sizes >= min_size].index)\n\n    oof = train_df[f\"{name}_global_oof\"].copy().to_numpy()\n    pred = test_df[f\"{name}_global_pred\"].copy().to_numpy()\n\n    for cat in big:\n        m_tr = key == cat\n        m_te = test_df[cat_col].astype(str).fillna(\"Unknown\") == cat\n        if m_tr.sum() < n_splits + 5:\n            continue\n        add_oof_pls_head(train_df[m_tr], test_df[m_te], feature_cols, y[m_tr],\n                         n_comp=n_comp, name=f\"{name}_{cat}\", n_splits=n_splits, seed=seed)\n        oof[m_tr] = train_df.loc[m_tr, f\"{name}_{cat}_oof\"].to_numpy()\n        pred[m_te] = test_df.loc[m_te,  f\"{name}_{cat}_pred\"].to_numpy()\n\n    train_df[f\"{name}_oof\"] = oof.astype(np.float32)\n    test_df[f\"{name}_pred\"] = pred.astype(np.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_oof_token_score(train_df, test_df, text_col=\"reviews_text\",\n                        y_col=\"target\", name=\"tokte\",\n                        min_df=20, max_df=0.9, ngram_range=(1,2),\n                        prior=12.0, n_splits=5, seed=42):\n    vect = CountVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range,\n                           token_pattern=r\"(?u)\\b[а-яa-z0-9][а-яa-z0-9\\-]{2,}\\b\",\n                           lowercase=True, binary=False)\n    X_all = vect.fit_transform(pd.concat([train_df[text_col], test_df[text_col]]).astype(str).values)\n    X_tr = X_all[:len(train_df)]\n    X_te = X_all[len(train_df):]\n    y = pd.to_numeric(train_df[y_col], errors=\"coerce\").values.astype(np.float32)\n    global_mean = float(np.nanmean(y))\n\n    bins = pd.qcut(y, q=min(10, max(2, len(y)//50)), duplicates=\"drop\")\n    y_bins = pd.factorize(bins)[0]\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof_score = np.zeros(len(train_df), dtype=np.float32); te_score = np.zeros(len(test_df), dtype=np.float32)\n\n    for tr_idx, va_idx in skf.split(X_tr, y_bins):\n        Xtr = X_tr[tr_idx]; ytr = y[tr_idx]\n        term_sum = np.asarray(Xtr.T @ ytr).ravel().astype(np.float64)\n        term_cnt = np.asarray(Xtr.T.sum(axis=1)).ravel().astype(np.float64)\n        term_mean = (term_sum + prior * global_mean) / (term_cnt + prior)\n\n        def doc_score(X, w):\n            num = np.asarray(X @ w).ravel()\n            den = np.asarray(X.sum(axis=1)).ravel() + 1e-6\n            return (num / den).astype(np.float32)\n\n        oof_score[va_idx] = doc_score(X_tr[va_idx], term_mean)\n        te_score += doc_score(X_te, term_mean) / n_splits\n\n    train_df[f\"{name}_oof\"] = oof_score\n    test_df[f\"{name}_pred\"] = te_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _normalize_chain_name(s: str) -> str:\n    s = str(s).lower()\n    s = re.sub(r\"[\\\"«».,!?:;()\\-–—\\[\\]/\\\\]+\", \" \", s)\n    s = re.sub(r\"\\b(ооо|ип|зао|оао|пао|ao|ooo|zao|oao|ooo\\s+|ооо\\s+)\\b\\.?\", \" \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\nfor df in (train, test):\n    df[\"name_norm\"] = df.get(\"name\", \"\").astype(str).map(_normalize_chain_name).fillna(\"unknown\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"name_counts = pd.concat([train[\"name_norm\"], test[\"name_norm\"]]).value_counts()\nfor df in (train, test):\n    df[\"chain_count_all\"] = df[\"name_norm\"].map(name_counts).astype(\"float32\")\n    df[\"chain_is_big_3+\"] = (df[\"chain_count_all\"] >= 3).astype(\"int8\")\n    df[\"chain_is_big_5+\"] = (df[\"chain_count_all\"] >= 5).astype(\"int8\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_oof_mean_by_key(train_df, test_df, key_cols, y_col=\"target\",\n                        prior=15.0, name=\"te\", n_splits=5, seed=42):\n    key = train_df[key_cols].astype(str).fillna(\"Unknown\").agg(\"||\".join, axis=1).values\n    key_te = test_df[key_cols].astype(str).fillna(\"Unknown\").agg(\"||\".join, axis=1).values\n    y = pd.to_numeric(train_df[y_col], errors=\"coerce\").values.astype(np.float32)\n    gmean = float(np.nanmean(y))\n    bins = pd.qcut(y, q=min(10, max(2, len(y)//50)), duplicates=\"drop\")\n    y_bins = pd.factorize(bins)[0]\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = np.zeros(len(train_df), dtype=np.float32)\n    te  = np.zeros(len(test_df),  dtype=np.float32)\n\n    for tr_idx, va_idx in skf.split(train_df, y_bins):\n        k_tr = key[tr_idx]; y_tr = y[tr_idx]\n        df = pd.DataFrame({\"k\": k_tr, \"y\": y_tr})\n        gb = df.groupby(\"k\")[\"y\"].agg([\"sum\",\"count\"])\n        s = gb[\"sum\"].to_dict(); c = gb[\"count\"].to_dict()\n        def map_keys(keys):\n            return np.array([(s.get(k,0.0) + prior*gmean) / (c.get(k,0.0) + prior) for k in keys], dtype=np.float32)\n        oof[va_idx] = map_keys(key[va_idx])\n        te += map_keys(key_te) / n_splits\n\n    train_df[f\"{name}_oof\"] = oof\n    test_df[f\"{name}_pred\"] = te\n\nadd_oof_mean_by_key(train, test, [\"name_norm\"],name=\"chain_te\", prior=20.0, n_splits=N_FOLDS, seed=RANDOM_STATE)\nif \"category\" in train.columns:\n    add_oof_mean_by_key(train, test, [\"name_norm\",\"category\"], name=\"chaincat_te\", prior=20.0, n_splits=N_FOLDS, seed=RANDOM_STATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_same_name_counts(train_df, test_df, name_col=\"name_norm\", radii=(300, 1000)):\n    all_df = pd.concat([train_df[[\"lat\",\"lon\",name_col]],\n                        test_df[[\"lat\",\"lon\",name_col]]], ignore_index=True)\n    lat = pd.to_numeric(all_df[\"lat\"], errors=\"coerce\").values\n    lon = pd.to_numeric(all_df[\"lon\"], errors=\"coerce\").values\n    ok = np.isfinite(lat) & np.isfinite(lon)\n    lat_r = np.deg2rad(np.where(ok, lat, 0.0)); lon_r = np.deg2rad(np.where(ok, lon, 0.0))\n    names = all_df[name_col].astype(str).fillna(\"unknown\").values\n    out = {r: np.full(len(all_df), np.nan, dtype=np.float32) for r in radii}\n\n    for nm in pd.unique(names[ok]):\n        idx = np.where((names==nm) & ok)[0]\n        if idx.size <= 1: \n            for r in radii: out[r][idx] = 0\n            continue\n        X = np.c_[lat_r[idx], lon_r[idx]]\n        tree = BallTree(X, metric=\"haversine\")\n        for r in radii:\n            inds = tree.query_radius(X, r=r/R_EARTH, return_distance=False)\n            cnt = np.fromiter((max(0, len(ii)-1) for ii in inds), dtype=np.int32)\n            out[r][idx] = cnt.astype(np.float32)\n\n    ntr = len(train_df)\n    for r in radii:\n        train_df[f\"same_name_cnt_r{r}\"] = out[r][:ntr]\n        test_df[f\"same_name_cnt_r{r}\"]  = out[r][ntr:]\n\nadd_same_name_counts(train, test, name_col=\"name_norm\", radii=(300, 1000))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_center_angle_features(train_df, test_df, n_sectors=16):\n    for df in (train_df, test_df):\n        lat = pd.to_numeric(df[\"lat\"], errors=\"coerce\").values\n        lon = pd.to_numeric(df[\"lon\"], errors=\"coerce\").values\n        ang = np.arctan2(lat - MOSCOW_CENTER_LAT, lon - MOSCOW_CENTER_LON)\n        ang[~np.isfinite(ang)] = np.nan\n        df[\"center_angle_sin\"] = np.sin(ang).astype(\"float32\")\n        df[\"center_angle_cos\"] = np.cos(ang).astype(\"float32\")\n        sec = np.floor(((ang + np.pi) / (2*np.pi)) * n_sectors)\n        df[f\"center_sector_{n_sectors}\"] = sec.astype(\"float32\")\n\nadd_center_angle_features(train, test, n_sectors=16)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_kmeans_cluster_te(train_df, test_df, n_clusters=60, seed=42):\n    xy_tr = train_df[[\"lat\",\"lon\"]].apply(pd.to_numeric, errors=\"coerce\")\n    xy_te = test_df[[\"lat\",\"lon\"]].apply(pd.to_numeric, errors=\"coerce\")\n    scaler = StandardScaler()\n    xy_all = pd.concat([xy_tr, xy_te], axis=0).to_numpy()\n    mask = np.isfinite(xy_all).all(axis=1)\n    xy_scaled = np.zeros_like(xy_all, dtype=np.float32)\n    xy_scaled[mask] = scaler.fit_transform(xy_all[mask])\n\n    km = KMeans(n_clusters=n_clusters, random_state=seed, n_init=10)\n    lab = np.full(len(xy_all), -1, dtype=np.int32)\n    lab[mask] = km.fit_predict(xy_scaled[mask])\n    lab_tr = lab[:len(train_df)]; lab_te = lab[len(train_df):]\n\n    train_df[f\"kmeans{n_clusters}\"] = lab_tr\n    test_df[f\"kmeans{n_clusters}\"]  = lab_te\n\n    add_oof_mean_by_key(train_df, test_df, [f\"kmeans{n_clusters}\"], name=f\"kmeans{n_clusters}_te\",\n                        prior=12.0, n_splits=N_FOLDS, seed=RANDOM_STATE)\n\nadd_kmeans_cluster_te(train, test, n_clusters=60, seed=RANDOM_STATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_local_market_structure(train_df, test_df, radii=(200, 300, 500)):\n    all_df = pd.concat([train_df[[\"lat\",\"lon\",\"category\"]],\n                        test_df[[\"lat\",\"lon\",\"category\"]]], ignore_index=True)\n    lat = pd.to_numeric(all_df[\"lat\"], errors=\"coerce\").values\n    lon = pd.to_numeric(all_df[\"lon\"], errors=\"coerce\").values\n    ok = np.isfinite(lat) & np.isfinite(lon)\n    X = np.c_[np.deg2rad(np.where(ok, lat, 0.0)), np.deg2rad(np.where(ok, lon, 0.0))]\n    cats = all_df[\"category\"].astype(str).fillna(\"Unknown\").values\n    tree = BallTree(X[ok], metric=\"haversine\")\n\n    res = {r: dict(all=np.full(len(all_df), np.nan, dtype=np.float32),\n                   uniq=np.full(len(all_df), np.nan, dtype=np.float32)) for r in radii}\n\n    for r in radii:\n        ind = tree.query_radius(X[ok], r=r/R_EARTH, return_distance=False)\n        cnt_all = np.zeros(np.sum(ok), dtype=np.int32)\n        cnt_uniq= np.zeros(np.sum(ok), dtype=np.int32)\n        for i, nb in enumerate(ind):\n            gidx = np.where(ok)[0][i]\n            nb = np.where(ok)[0][nb]\n            nb = nb[nb != gidx]\n            cnt_all[i] = len(nb)\n            if len(nb)>0:\n                cnt_uniq[i] = len(np.unique(cats[nb]))\n        res[r][\"all\"][ok]  = cnt_all\n        res[r][\"uniq\"][ok] = cnt_uniq\n\n    ntr = len(train_df)\n    for r in radii:\n        train_df[f\"allcat_cnt_r{r}\"]  = res[r][\"all\"][:ntr]\n        test_df[f\"allcat_cnt_r{r}\"]   = res[r][\"all\"][ntr:]\n        train_df[f\"uniqcat_cnt_r{r}\"] = res[r][\"uniq\"][:ntr]\n        test_df[f\"uniqcat_cnt_r{r}\"]  = res[r][\"uniq\"][ntr:]\n        for df in (train_df, test_df):\n            if f\"samecat_cnt_r{r}\" in df.columns:\n                df[f\"same_over_all_r{r}\"] = _safe_div(df[f\"samecat_cnt_r{r}\"], df[f\"allcat_cnt_r{r}\"])\nadd_local_market_structure(train, test, radii=(200, 300))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if \"metro_dist_min_m\" in train.columns and \"category\" in train.columns:\n    med = train.groupby(\"category\")[\"metro_dist_min_m\"].median()\n    for df in (train, test):\n        df[\"cat_metro_med\"] = df[\"category\"].map(med).astype(\"float32\")\n        df[\"metro_minus_cat_med\"] = (pd.to_numeric(df[\"metro_dist_min_m\"], errors=\"coerce\") - df[\"cat_metro_med\"]).astype(\"float32\")\n        df[\"metro_ratio_cat_med\"] = _safe_div(df[\"metro_dist_min_m\"], df[\"cat_metro_med\"]).astype(\"float32\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def oof_local_target_stats_any(train_df, test_df, radii_m=(200,300), n_splits=5, seed=42):\n    y = train_df[\"target\"].astype(float).values\n    lat_tr = pd.to_numeric(train_df[\"lat\"], errors=\"coerce\").values\n    lon_tr = pd.to_numeric(train_df[\"lon\"], errors=\"coerce\").values\n    lat_te = pd.to_numeric(test_df[\"lat\"],  errors=\"coerce\").values\n    lon_te = pd.to_numeric(test_df[\"lon\"],  errors=\"coerce\").values\n\n    bins = pd.qcut(y, q=min(10, max(2, len(y)//50)), duplicates=\"drop\")\n    y_bins = pd.factorize(bins)[0]\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    valid_tr = np.isfinite(lat_tr) & np.isfinite(lon_tr)\n    valid_te = np.isfinite(lat_te) & np.isfinite(lon_te)\n    X_tr_all = np.c_[np.deg2rad(np.where(valid_tr, lat_tr, 0.0)),\n                     np.deg2rad(np.where(valid_tr, lon_tr, 0.0))]\n    X_te_all = np.c_[np.deg2rad(np.where(valid_te, lat_te, 0.0)),\n                     np.deg2rad(np.where(valid_te, lon_te, 0.0))]\n\n    for r_m in radii_m:\n        r = r_m / R_EARTH\n        oof_mean = np.full(len(train_df), np.nan, dtype=np.float32)\n        oof_med  = np.full(len(train_df), np.nan, dtype=np.float32)\n        oof_cnt  = np.zeros(len(train_df), dtype=np.float32)\n\n        for tr_idx, va_idx in skf.split(train_df, y_bins):\n            tr_idx_v = tr_idx[valid_tr[tr_idx]]\n            va_idx_v = va_idx[valid_tr[va_idx]]\n            if tr_idx_v.size == 0 or va_idx_v.size == 0: continue\n            Xtr, ytr = X_tr_all[tr_idx_v], y[tr_idx_v]\n            tree = BallTree(Xtr, metric=\"haversine\")\n            neigh = tree.query_radius(X_tr_all[va_idx_v], r=r, return_distance=False)\n            for j, nb in enumerate(neigh):\n                if nb.size == 0: continue\n                vals = ytr[nb]\n                oof_mean[va_idx_v[j]] = float(np.mean(vals))\n                oof_med[va_idx_v[j]]  = float(np.median(vals))\n                oof_cnt[va_idx_v[j]]  = float(len(vals))\n\n        train_df[f\"loc_any_mean_r{r_m}\"]   = oof_mean\n        train_df[f\"loc_any_median_r{r_m}\"] = oof_med\n        train_df[f\"loc_any_count_r{r_m}\"]  = oof_cnt\n\n        # test на полном train\n        tr_all_v = np.where(valid_tr)[0]\n        if tr_all_v.size and np.any(valid_te):\n            Xtr, ytr = X_tr_all[tr_all_v], y[tr_all_v]\n            tree = BallTree(Xtr, metric=\"haversine\")\n            te_v = np.where(valid_te)[0]\n            neigh_te = tree.query_radius(X_te_all[te_v], r=r, return_distance=False)\n            te_mean = np.full(len(test_df), np.nan, dtype=np.float32)\n            te_med  = np.full(len(test_df), np.nan, dtype=np.float32)\n            te_cnt  = np.zeros(len(test_df), dtype=np.float32)\n            for j, nb in enumerate(neigh_te):\n                if nb.size == 0: continue\n                vals = ytr[nb]\n                idx = te_v[j]\n                te_mean[idx] = float(np.mean(vals))\n                te_med[idx]  = float(np.median(vals))\n                te_cnt[idx]  = float(len(vals))\n            test_df[f\"loc_any_mean_r{r_m}\"]   = te_mean\n            test_df[f\"loc_any_median_r{r_m}\"] = te_med\n            test_df[f\"loc_any_count_r{r_m}\"]  = te_cnt\n\noof_local_target_stats_any(train, test, radii_m=(200,300), n_splits=N_FOLDS, seed=RANDOM_STATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_same_name_distance(train_df, test_df, name_col=\"name_norm\", ks=(1,3)):\n    all_df = pd.concat([train_df[[\"lat\",\"lon\",name_col]],\n                        test_df[[\"lat\",\"lon\",name_col]]], ignore_index=True)\n    lat = pd.to_numeric(all_df[\"lat\"], errors=\"coerce\").values\n    lon = pd.to_numeric(all_df[\"lon\"], errors=\"coerce\").values\n    ok = np.isfinite(lat) & np.isfinite(lon)\n    lat_r = np.deg2rad(np.where(ok, lat, 0.0)); lon_r = np.deg2rad(np.where(ok, lon, 0.0))\n    names = all_df[name_col].astype(str).fillna(\"unknown\").values\n\n    out_min = np.full(len(all_df), np.nan, dtype=np.float32)\n    out_k3  = np.full(len(all_df), np.nan, dtype=np.float32)\n\n    for nm in pd.unique(names[ok]):\n        idx = np.where((names==nm) & ok)[0]\n        if idx.size <= 1: \n            out_min[idx] = np.nan; out_k3[idx] = np.nan\n            continue\n        X = np.c_[lat_r[idx], lon_r[idx]]\n        tree = BallTree(X, metric=\"haversine\")\n        kq = min(2, X.shape[0])\n        d2, _ = tree.query(X, k=kq)\n        if kq > 1:\n            out_min[idx] = (d2[:,1] * R_EARTH).astype(np.float32)\n        kq = min(4, X.shape[0])\n        d4, _ = tree.query(X, k=kq)\n        if kq > 1:\n            dnn = d4[:,1:] * R_EARTH\n            out_k3[idx] = dnn.mean(axis=1).astype(np.float32)\n\n    ntr = len(train_df)\n    train_df[\"same_name_dist_min_m\"]   = out_min[:ntr]\n    test_df[\"same_name_dist_min_m\"]    = out_min[ntr:]\n    train_df[\"same_name_dist_mean3_m\"] = out_k3[:ntr]\n    test_df[\"same_name_dist_mean3_m\"]  = out_k3[ntr:]\n\nadd_same_name_distance(train, test, name_col=\"name_norm\", ks=(1,3))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_category_diversity(train_df, test_df, radii=(200,300)):\n    all_df = pd.concat([train_df[[\"lat\",\"lon\",\"category\"]],\n                        test_df[[\"lat\",\"lon\",\"category\"]]], ignore_index=True)\n    lat = pd.to_numeric(all_df[\"lat\"], errors=\"coerce\").values\n    lon = pd.to_numeric(all_df[\"lon\"], errors=\"coerce\").values\n    cats = all_df[\"category\"].astype(str).fillna(\"Unknown\").values\n    ok = np.isfinite(lat) & np.isfinite(lon)\n    X = np.c_[np.deg2rad(np.where(ok, lat, 0.0)), np.deg2rad(np.where(ok, lon, 0.0))]\n    tree = BallTree(X[ok], metric=\"haversine\")\n\n    for r in radii:\n        ent = np.full(len(all_df), np.nan, dtype=np.float32)\n        hhi = np.full(len(all_df), np.nan, dtype=np.float32)\n        ind = tree.query_radius(X[ok], r=r/R_EARTH, return_distance=False)\n        ok_idx = np.where(ok)[0]\n        for i, nb_local in enumerate(ind):\n            gidx = ok_idx[i]\n            nb = ok_idx[nb_local]\n            nb = nb[nb != gidx]\n            if nb.size == 0: \n                ent[i] = np.nan; hhi[i] = np.nan; continue\n            vals, cnts = np.unique(cats[nb], return_counts=True)\n            p = cnts / cnts.sum()\n            ent[i] = float(-(p * np.log(p + 1e-12)).sum())\n            hhi[i] = float((p**2).sum())\n        ntr = len(train_df)\n        train_df[f\"cat_entropy_r{r}\"] = ent[:ntr]\n        test_df[f\"cat_entropy_r{r}\"]  = ent[ntr:]\n        train_df[f\"cat_hhi_r{r}\"]     = hhi[:ntr]\n        test_df[f\"cat_hhi_r{r}\"]      = hhi[ntr:]\n\nadd_category_diversity(train, test, radii=(200,300))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if \"dist_to_center_m\" in train.columns and \"category\" in train.columns:\n    med_c = train.groupby(\"category\")[\"dist_to_center_m\"].median()\n    for df in (train, test):\n        df[\"cat_center_med\"] = df[\"category\"].map(med_c).astype(\"float32\")\n        d = pd.to_numeric(df[\"dist_to_center_m\"], errors=\"coerce\")\n        cm = pd.to_numeric(df[\"cat_center_med\"], errors=\"coerce\")\n        df[\"center_minus_cat_med\"] = (d - cm).astype(\"float32\")\n        df[\"center_ratio_cat_med\"] = _safe_div(d, cm).astype(\"float32\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in [c for c in train.columns if c.endswith(\"_oof\")]:\n    pred = col.replace(\"_oof\",\"_pred\")\n    if pred in test.columns:\n        test[col] = test[pred]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_cols = {\"target\",\"address\",\"coordinates\",\"reviews_text\"}\nbase_cols = [c for c in train.columns if c not in drop_cols and c != \"id\"]\n\ncat_cols = []\nif \"category\" in train.columns: cat_cols.append(\"category\")\nif \"name\" in train.columns:     cat_cols.append(\"name\")\ncat_cols = [c for c in cat_cols if c in base_cols]\n\nnum_cols = [c for c in base_cols\n            if c not in cat_cols\n            and not c.startswith(\"kw_\")\n            and not c.startswith(\"rev_bert_\")\n            and not c.startswith(\"bert_svd_\")\n            and not c.startswith(\"bert_mean_\")\n            and not c.startswith(\"bert_raw_\")]\n\nfor c in cat_cols:\n    train[c] = train[c].astype(str).fillna(\"Unknown\")\n    test[c]  = test[c].astype(str).fillna(\"Unknown\")\nfor c in num_cols:\n    train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n    test[c]  = pd.to_numeric(test[c], errors=\"coerce\")\n\n_ = light_log1p_skewed(train, num_cols, skew_thr=1.0)\n_ = light_log1p_skewed(test,  num_cols, skew_thr=1.0)\n\nkw_cols = [c for c in train.columns if c.startswith(\"kw_\")]\nbert_cols = [c for c in train.columns if c.startswith(\"rev_bert_\") or c.startswith(\"bert_svd_\") or c.startswith(\"bert_raw_\")]\n\nOOF_HEADS = [\n    \"tfidf_pls16\",\n    \"bert_pls8\",\n    \"tfidf_ridge\",\n    \"tfidf_pls_cat\",\n    \"tokte\",\n    \"text_cb\",\n]\nOOF_HEADS += [\"chain_te\", \"chaincat_te\"]\nOOF_HEADS += [ \"kmeans60_te\" ]\n\n\nif \"category\" in train.columns:\n    add_oof_mean_by_key(train, test, [\"category\"], name=\"cat_te\",\n                        prior=10.0, n_splits=N_FOLDS, seed=RANDOM_STATE)\n    OOF_HEADS += [\"cat_te\"]\n\nif \"center_ring_id\" in train.columns and \"category\" in train.columns:\n    add_oof_mean_by_key(train, test, [\"category\",\"center_ring_id\"], name=\"cat_ring_te\",\n                        prior=12.0, n_splits=N_FOLDS, seed=RANDOM_STATE)\n    OOF_HEADS += [\"cat_ring_te\"]\n\n\nextra_oof_cols = [f\"{h}_oof\" for h in OOF_HEADS if f\"{h}_oof\" in train.columns]\n\nall_cols = cat_cols + num_cols + kw_cols + bert_cols + extra_oof_cols\n\nall_cols = pd.Index(all_cols).drop_duplicates().tolist()\n\nte_cols = [c.replace(\"_oof\", \"_pred\") for c in all_cols]\n\nfor c in te_cols:\n    if c not in test.columns:\n        test[c] = np.nan\n\nX = train[all_cols].copy()\nX = X.loc[:, ~X.columns.duplicated()]\n\nX_test = test[te_cols].copy()\nX_test.columns = X.columns\n\n\nbins = pd.qcut(train[\"target\"], q=min(10, train.shape[0]//50), duplicates=\"drop\")\ny_bins = pd.factorize(bins)[0]\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n\noof = np.zeros(len(train), dtype=np.float32)\ntest_pred = np.zeros(len(test), dtype=np.float32)\ntask_type = \"GPU\" if USE_GPU else \"CPU\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Начинаем {N_FOLDS}-fold обучение CatBoost (task_type={task_type})...\")\nfor fold, (tr_idx, va_idx) in enumerate(skf.split(X, y_bins), 1):\n    X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = X.iloc[va_idx], y[va_idx]\n\n    train_pool = Pool(X_tr, y_tr, cat_features=cat_cols)\n    valid_pool = Pool(X_va, y_va, cat_features=cat_cols)\n    test_pool  = Pool(X_test,      cat_features=cat_cols)\n\n    model = CatBoostRegressor(\n        loss_function=\"MAE\", eval_metric=\"MAE\",\n        iterations=20000, depth=8, learning_rate=0.03064123,\n        l2_leaf_reg=6.0, random_strength=1.5, bagging_temperature=0.5,\n        od_type=\"Iter\", od_wait=500, use_best_model=True,\n        task_type=task_type, random_seed=RANDOM_STATE\n    )\n    model.fit(train_pool, eval_set=valid_pool, verbose=200)\n\n    pred_va = model.predict(valid_pool).astype(np.float32)\n    oof[va_idx] = pred_va\n    fold_mae = mean_absolute_error(y_va, pred_va)\n    print(f\"[Fold {fold}] MAE: {fold_mae:.5f}, best_iter={model.get_best_iteration()}\")\n\n    test_pred += model.predict(test_pool).astype(np.float32) / N_FOLDS\n\noof_mae = mean_absolute_error(y, oof)\nprint(f\"OOF MAE = {oof_mae:.6f}\")\n\ntest_pred = np.clip(test_pred, 1.0, 5.0)\nsub = pd.DataFrame({\"id\": test[\"id\"], \"target\": test_pred})\nsub.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Сохранено: {SUBMISSION_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}