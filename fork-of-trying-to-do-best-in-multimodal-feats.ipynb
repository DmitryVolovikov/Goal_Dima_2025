{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13680650,"sourceType":"datasetVersion","datasetId":8699768}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Seeds","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nimport torch\nimport os\n\nseed=532\n\nos.environ['PYTHONHASHSEED']=str(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\ntorch.cuda.manual_seed_all(seed)\ntorch.manual_seed(seed)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# device","metadata":{}},{"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n#from sklearn.metrics import \nfrom torch.utils.data import Dataset, DataLoader\nimport timm\nimport PIL \nfrom PIL import Image\nfrom torchvision.transforms import v2\nfrom sklearn.neighbors import NearestNeighbors\nfrom catboost import CatBoostRegressor, Pool\n\nfrom transformers import CLIPModel, CLIPProcessor\nimport torch.nn.functional as F\nimport glob\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"code","source":"train=pd.read_parquet('/kaggle/input/auto-trying/train_dataset (2).parquet')\n\ntest=pd.read_parquet('/kaggle/input/auto-trying/test_dataset (2).parquet')\n\nsample=pd.read_csv('/kaggle/input/auto-trying/sample_submission (18).csv')\n\ntrain_img_dir='/kaggle/input/auto-trying/autoprice/АвтоПрайс/train_images'\ntest_img_dir='/kaggle/input/auto-trying/autoprice/АвтоПрайс/test_images'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_feats=train.select_dtypes(include='number').columns.tolist()\ncat_feats=train.select_dtypes(include='object').columns.tolist()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data, eval_data=train_test_split(train, test_size=0.15,  random_state=seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## num feats","metadata":{}},{"cell_type":"code","source":"for col in num_feats:\n    plt.hist(train[col])\n    plt.xlabel(col)\n    plt.ylabel('Распределение')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr=train[num_feats].corrwith(train[''])\nprint(corr.sort_values(ascending=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## cat_feats ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"#def featurize(df):\n    \n\n\n\n\n#    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data=featurize(train_data)\neval_data=featurize(eval_data)\ntest=featurize(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## useless feats","metadata":{}},{"cell_type":"code","source":"useless_feats=['', '']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data=train_data.drop(columns=useless_feats)\neval_data=eval_data.drop(columns=useless_feats)\ntest=test.drop(columns=useless_feats)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## NaN to string","metadata":{}},{"cell_type":"code","source":"def prep_cats(df, cats):\n    df = df.copy()\n    for c in cats:\n        df[c] = df[c].astype('object')              # ensure non-numeric type\n        df[c] = df[c].fillna('NaN')         # replace NaN\n        df[c] = df[c].astype(str)                   # strings only\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = prep_cats(train_data, cat_feats)\neval_data = prep_cats(eval_data, cat_feats)\ntest = prep_cats(test, cat_feats)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Just catboost over table data","metadata":{}},{"cell_type":"code","source":"X_train=train_data.drop(columns='')\ny_train=train_data['']\n\nX_eval=eval_data.drop(columns='')\ny_eval=eval_data['']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_features = [c for c in X_train.columns if X_train[c].dtype == 'object']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catboost_model = CatBoostRegressor(\n    iterations=1000,\n    depth=6,\n    learning_rate=0.05,\n    loss_function='RMSE',\n    eval_metric='RMSE',\n    l2_leaf_reg=3.0,\n    task_type='CPU',     # 'GPU' если есть\n    random_seed=seed,\n    verbose=200\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catboost_model.fit(X_train, y_train, eval_set=(X_eval, y_eval), cat_features=cat_features, early_stopping_rounds=200)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_pred = catboost_model.predict(test)\nsample[''] = test_pred\nsample.to_csv(\"submission1.csv\", index=False)\nprint(\"Saved submission1.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset for vision model","metadata":{}},{"cell_type":"code","source":"class GeneralDataset(Dataset):\n    def __init__(self, df, img_dir, transforms, is_train):\n        self.df=df\n        self.img_dir=img_dir\n        self.transforms=transforms\n        self.is_train=is_train\n\n        \n\n    def __len__(self):\n        return len(self.df)\n\n\n    def __getitem__(self, idx):\n        row=self.df.iloc[idx]\n        img_name=row['']\n        img_path=os.path.join(self.img_dir, f'{img_name}')\n        image=Image.open(img_path).convert('RGB')\n\n        image=self.transforms(image)\n\n        if self.is_train:\n            labels=torch.tensor(row[''], dtype=torch.float32)\n            return {\n                'image': image,\n                'label': labels\n            }\n        else:\n            return {\n                'image': image\n            }\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset for multi images","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## augmentations","metadata":{}},{"cell_type":"code","source":"IMG_SIZE=224","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transforms=v2.Compose([\n    v2.Resize((IMG_SIZE, IMG_SIZE)),\n    #v2.RandomHorizontalFlip(p=0.5),\n    #v2.RandomVerticalFlip(p=0.5),\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_transforms=v2.Compose([\n    v2.Resize((IMG_SIZE, IMG_SIZE)),\n\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_transforms=v2.Compose([\n    v2.Resize((IMG_SIZE, IMG_SIZE)),\n\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creation of dataset","metadata":{}},{"cell_type":"code","source":"train_dataset=GeneralDataset(train_data, train_img_dir, train_transforms, is_train=True)\neval_dataset=GeneralDataset(eval_data, train_img_dir, eval_transforms, is_train=True)\ntest_dataset=GeneralDataset(test, test_img_dir, test_transforms, is_train=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataloaders","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE=32","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_WORKERS=4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\neval_dataloader = DataLoader(\n    eval_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Отдельные даталоадеры без shuffle для извлечения эмбеддингов\ntrain_dataloader_emb = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\neval_dataloader_emb = DataLoader(\n    eval_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\ntest_dataloader_emb = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset for CLIP","metadata":{}},{"cell_type":"code","source":"class CLIPImageDataset(Dataset):\n    def __init__(self, df, img_dir, img_col, transforms):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.img_col = img_col\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_name = row[self.img_col]\n        img_path = os.path.join(self.img_dir, f'{img_name}_0.jpg')\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transforms(image)\n        return image   # (3, H, W) float32","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset for multi images","metadata":{}},{"cell_type":"code","source":"def build_images_df(df: pd.DataFrame, img_dir: str, img_col: str) -> pd.DataFrame:\n    \"\"\"\n    df      — таблица объектов (train_data / eval_data / test)\n    img_col — колонка с base-именем (например, item_id)\n    img_dir — папка с jpg\n    Возвращает таблицу с одной строкой на КАРТИНКУ:\n      obj_idx — индекс объекта в df\n      img_path — полный путь до конкретной картинки\n    \"\"\"\n    rows = []\n    df = df.reset_index(drop=True)\n\n    for obj_idx, row in df.iterrows():\n        base = row[img_col]\n\n        # Все картинки с суффиксом _k\n        pattern = os.path.join(img_dir, f\"{base}_*.jpg\")\n        files = sorted(glob.glob(pattern))\n\n        # Фоллбек: одиночный файл без суффикса\n        if not files:\n            alt = os.path.join(img_dir, f\"{base}.jpg\")\n            if os.path.exists(alt):\n                files = [alt]\n\n        for p in files:\n            rows.append({\"obj_idx\": obj_idx, \"img_path\": p})\n\n    return pd.DataFrame(rows)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_images_df = build_images_df(train_data, train_img_dir, IMG_COL)\neval_images_df  = build_images_df(eval_data,  train_img_dir, IMG_COL)\ntest_images_df  = build_images_df(test,       test_img_dir,  IMG_COL)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiImageDataset(Dataset):\n    def __init__(self, images_df: pd.DataFrame, df_objects: pd.DataFrame,\n                 transforms, is_train: bool):\n        \"\"\"\n        images_df: колонки ['obj_idx', 'img_path']\n        df_objects: исходный df с таргетом и табличкой (train_data / eval_data / test)\n        \"\"\"\n        self.images_df = images_df.reset_index(drop=True)\n        self.df_objects = df_objects.reset_index(drop=True)\n        self.transforms = transforms\n        self.is_train = is_train\n\n    def __len__(self):\n        return len(self.images_df)\n\n    def __getitem__(self, idx):\n        row_img = self.images_df.iloc[idx]\n        img_path = row_img[\"img_path\"]\n        obj_idx = int(row_img[\"obj_idx\"])\n\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transforms(image)\n\n        if self.is_train:\n            label = float(self.df_objects.loc[obj_idx, TARGET_COL])\n            label = torch.tensor(label, dtype=torch.float32)\n            return {\n                \"image\": image,\n                \"label\": label,\n                \"obj_idx\": torch.tensor(obj_idx, dtype=torch.long)\n            }\n        else:\n            return {\n                \"image\": image,\n                \"obj_idx\": torch.tensor(obj_idx, dtype=torch.long)\n            }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ntrain_dataset = MultiImageDataset(train_images_df, train_data, train_transforms, is_train=True)\neval_dataset  = MultiImageDataset(eval_images_df,  eval_data,  eval_transforms,  is_train=True)\ntest_dataset  = MultiImageDataset(test_images_df,  test,       test_transforms,  is_train=False)\n\nBATCH_SIZE = 32\nNUM_WORKERS = 4\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\neval_dataloader = DataLoader(\n    eval_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\n# для эмбеддингов/предиктов — без shuffle\ntrain_dataloader_emb = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\neval_dataloader_emb = DataLoader(\n    eval_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\ntest_dataloader_emb = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nCLIP_BATCH_SIZE = 32  # можно больше, если влазит в VRAM\n\nclip_train_dataset = MultiImageDataset(train_images_df, train_data, clip_transforms, is_train=False)\nclip_eval_dataset  = MultiImageDataset(eval_images_df,  eval_data,  clip_transforms, is_train=False)\nclip_test_dataset  = MultiImageDataset(test_images_df,  test,       clip_transforms, is_train=False)\n\nclip_train_loader = DataLoader(\n    clip_train_dataset,\n    batch_size=CLIP_BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\nclip_eval_loader = DataLoader(\n    clip_eval_dataset,\n    batch_size=CLIP_BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\nclip_test_loader = DataLoader(\n    clip_test_dataset,\n    batch_size=CLIP_BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vision model","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"MODEL_NAME='resnet50'\n\nmodel=timm.create_model(MODEL_NAME, pretrained=True, num_classes=1).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## NUM EPOCHS","metadata":{}},{"cell_type":"code","source":"NUM_EPOCHS=1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Criterion","metadata":{}},{"cell_type":"code","source":"#criterion=torch.nn.CrossEntropyLoss()\ncriterion = torch.nn.MSELoss()\n# or anything \n# add here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimizer","metadata":{}},{"cell_type":"code","source":"base_lr=0.01* (BATCH_SIZE/256)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer=torch.optim.SGD(model.parameters(), lr=base_lr)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Scheduler","metadata":{}},{"cell_type":"code","source":"NUM_STEPS=len(train_dataloader)* NUM_EPOCHS\nWARMUP_STEPS=NUM_STEPS*0.06","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scheduler1=torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1e-3, end_factor=1.00, total_iters=WARMUP_STEPS)\nscheduler2=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_STEPS - WARMUP_STEPS, eta_min=base_lr*0.06)\n\nscheduler=torch.optim.lr_scheduler.SequentialLR(\n    optimizer,\n    schedulers=[scheduler1, scheduler2],\n    milestones=[WARMUP_STEPS]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training loop","metadata":{}},{"cell_type":"code","source":"for epoch in range(NUM_EPOCHS):\n    model.train()\n    training_running_loss = 0.0\n    pbar_train=tqdm(train_dataloader, desc=f'Training {epoch+1} / {NUM_EPOCHS}: ', leave=True)\n    for step, batch in enumerate(pbar_train):\n        optimizer.zero_grad()\n        X=batch['image'].to(device)\n        y=batch['label'].to(device)\n        #logits=model(X)\n        logits = model(X).squeeze(-1)\n        loss=criterion(logits, y)\n        loss.backward()\n\n        training_running_loss += loss.item() * X.size(0)\n        optimizer.step()\n        scheduler.step()\n        avg_loss = training_running_loss / ((step + 1) * X.size(0))\n        pbar_train.set_postfix(loss=avg_loss)\n\n\n    model.eval()\n    eval_running_loss= 0.0\n    pbar_eval=tqdm(eval_dataloader, desc=f'Evaluating {epoch+1} / {NUM_EPOCHS}: ', leave=True)\n    with torch.no_grad():\n        for batch in pbar_eval:\n            X=batch['image'].to(device)\n            y=batch['label'].to(device)\n            #logits=model(X)\n            logits = model(X).squeeze(-1)\n            loss=criterion(logits, y)\n            eval_running_loss += loss.item() * X.size(0)\n    eval_loss = eval_running_loss / len(eval_dataset)\n    print(f\"Epoch {epoch+1}: eval loss = {eval_loss:.5f}\")\n            ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Getting features from vision model","metadata":{}},{"cell_type":"markdown","source":"## Preds of it","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_vision_predictions(model, dataloader):\n    model.eval()\n    all_preds = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Vision model predictions\"):\n            X = batch['image'].to(device)\n            preds = model(X).squeeze(-1)  # (B,)\n            all_preds.append(preds.cpu().numpy())\n    return np.concatenate(all_preds, axis=0)\n\n\ntrain_vision_pred = get_vision_predictions(model, train_dataloader_emb)\neval_vision_pred  = get_vision_predictions(model, eval_dataloader_emb)\ntest_vision_pred  = get_vision_predictions(model, test_dataloader_emb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[\"vision_pred\"] = train_vision_pred\neval_data[\"vision_pred\"]  = eval_vision_pred\ntest[\"vision_pred\"]       = test_vision_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## for multi image","metadata":{}},{"cell_type":"code","source":"def get_vision_predictions_multi(model, dataloader, n_objects: int):\n    model.eval()\n    pred_sum = np.zeros(n_objects, dtype=np.float32)\n    count = np.zeros(n_objects, dtype=np.int32)\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Vision model predictions (multi-image)\"):\n            X = batch['image'].to(device)\n            obj_idx = batch['obj_idx'].cpu().numpy()  # (B,)\n\n            preds = model(X).squeeze(-1).cpu().numpy()  # (B,)\n\n            for p, idx in zip(preds, obj_idx):\n                pred_sum[idx] += p\n                count[idx] += 1\n\n    count[count == 0] = 1\n    return pred_sum / count\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nn_train = len(train_data)\nn_eval  = len(eval_data)\nn_test  = len(test)\n\ntrain_vision_pred = get_vision_predictions_multi(model, train_dataloader_emb, n_train)\neval_vision_pred  = get_vision_predictions_multi(model, eval_dataloader_emb,  n_eval)\ntest_vision_pred  = get_vision_predictions_multi(model, test_dataloader_emb,  n_test)\n\ntrain_data[\"vision_pred\"] = train_vision_pred\neval_data[\"vision_pred\"]  = eval_vision_pred\ntest[\"vision_pred\"]       = test_vision_pred\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extract embeddings","metadata":{}},{"cell_type":"code","source":"def extract_embeddings(model, dataloader):\n    model.eval()\n    all_embeddings = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Extracting vision embeddings\"):\n            X = batch['image'].to(device)\n\n            # Для timm-моделей есть forward_features\n            if hasattr(model, 'forward_features'):\n                feats = model.forward_features(X)\n            else:\n                feats = model(X)\n\n            if isinstance(feats, (list, tuple)):\n                feats = feats[0]\n\n            feats = torch.flatten(feats, 1)  # (B, D)\n            all_embeddings.append(feats.cpu().numpy())\n\n    return np.concatenate(all_embeddings, axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_vision_emb = extract_embeddings(model, train_dataloader_emb)\neval_vision_emb = extract_embeddings(model, eval_dataloader_emb)\ntest_vision_emb = extract_embeddings(model, test_dataloader_emb)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vision_cols = [f\"vision_emb_{i}\" for i in range(train_vision_emb.shape[1])]\n\ntrain_vision_df = pd.DataFrame(train_vision_emb, columns=vision_cols, index=train_data.index)\neval_vision_df  = pd.DataFrame(eval_vision_emb,  columns=vision_cols, index=eval_data.index)\ntest_vision_df  = pd.DataFrame(test_vision_emb,  columns=vision_cols, index=test.index)\n\ntrain_data = pd.concat([train_data, train_vision_df], axis=1)\neval_data  = pd.concat([eval_data,  eval_vision_df],  axis=1)\ntest       = pd.concat([test,       test_vision_df],  axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### for multi image","metadata":{}},{"cell_type":"code","source":"def extract_embeddings_multi(model, dataloader, n_objects: int):\n    model.eval()\n    emb_sum = None\n    count = np.zeros(n_objects, dtype=np.int32)\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Extracting vision embeddings (multi-image)\"):\n            X = batch['image'].to(device)\n            obj_idx = batch['obj_idx'].cpu().numpy()\n\n            if hasattr(model, 'forward_features'):\n                feats = model.forward_features(X)\n            else:\n                feats = model(X)\n\n            if isinstance(feats, (list, tuple)):\n                feats = feats[0]\n\n            feats = torch.flatten(feats, 1)  # (B, D)\n            feats_np = feats.cpu().numpy()\n\n            if emb_sum is None:\n                emb_dim = feats_np.shape[1]\n                emb_sum = np.zeros((n_objects, emb_dim), dtype=np.float32)\n\n            for f, idx in zip(feats_np, obj_idx):\n                emb_sum[idx] += f\n                count[idx] += 1\n\n    count[count == 0] = 1\n    emb_avg = emb_sum / count[:, None]\n    return emb_avg\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ntrain_vision_emb = extract_embeddings_multi(model, train_dataloader_emb, n_train)\neval_vision_emb  = extract_embeddings_multi(model, eval_dataloader_emb,  n_eval)\ntest_vision_emb  = extract_embeddings_multi(model, test_dataloader_emb,  n_test)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Doing KNN over embeddings for tabular feats","metadata":{}},{"cell_type":"code","source":"TARGET_COL=''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_knn_features(\n    ref_emb: np.ndarray,\n    qry_emb: np.ndarray,\n    ref_df: pd.DataFrame,\n    qry_df: pd.DataFrame,\n    numeric_cols,\n    target_col: str,\n    cat_cols=None,\n    n_neighbors: int = 10,\n    prefix: str = \"knn\",\n    drop_self: bool = False\n) -> pd.DataFrame:\n    \"\"\"\n    ref_emb, ref_df — база (reference), по которой ищем соседей\n    qry_emb, qry_df — объекты, для которых считаем фичи\n    drop_self=True — для train, чтобы не использовать самого себя как соседа\n    \"\"\"\n    qry_df = qry_df.copy()\n    if len(ref_df) == 0:\n        return qry_df\n\n    # Реальное число соседей (учитываем drop_self и размер ref)\n    n_eff = n_neighbors + 1 if drop_self else n_neighbors\n    n_eff = min(n_eff, len(ref_df))\n\n    knn = NearestNeighbors(\n        n_neighbors=n_eff,\n        metric=\"cosine\"\n    )\n    knn.fit(ref_emb)\n\n    distances, indices = knn.kneighbors(qry_emb, return_distance=True)\n\n    # Убираем self-neighbor, если ref == train и qry == train\n    if drop_self:\n        distances = distances[:, 1:]\n        indices = indices[:, 1:]\n\n    # Если вдруг соседей не осталось\n    if distances.shape[1] == 0:\n        return qry_df\n\n    # ---- distance / similarity фичи ----\n    qry_df[f\"{prefix}_dist_mean\"] = distances.mean(axis=1)\n    qry_df[f\"{prefix}_dist_min\"] = distances.min(axis=1)\n    qry_df[f\"{prefix}_dist_max\"] = distances.max(axis=1)\n\n    qry_df[f\"{prefix}_sim_mean\"] = 1.0 - qry_df[f\"{prefix}_dist_mean\"]\n    qry_df[f\"{prefix}_sim_max\"] = 1.0 - qry_df[f\"{prefix}_dist_min\"]\n\n    # ---- таргет соседей ----\n    if target_col in ref_df.columns:\n        ref_targets = ref_df[target_col].values\n        neigh_targets = ref_targets[indices]  # (N, k)\n\n        qry_df[f\"{prefix}_target_mean\"] = neigh_targets.mean(axis=1)\n        qry_df[f\"{prefix}_target_std\"] = neigh_targets.std(axis=1)\n        qry_df[f\"{prefix}_target_min\"] = neigh_targets.min(axis=1)\n        qry_df[f\"{prefix}_target_max\"] = neigh_targets.max(axis=1)\n\n    # ---- агрегаты по числовым колонкам ----\n    for col in numeric_cols:\n        if col not in ref_df.columns:\n            continue\n        vals = ref_df[col].values\n        neigh_vals = vals[indices]  # (N, k)\n        qry_df[f\"{prefix}_{col}_mean\"] = np.nanmean(neigh_vals, axis=1)\n        # если хочешь более компактно, можно убрать std\n        # qry_df[f\"{prefix}_{col}_std\"] = np.nanstd(neigh_vals, axis=1)\n\n    # ---- \"насколько похожи\" по категориальным ----\n    if cat_cols is not None:\n        for col in cat_cols:\n            if col not in ref_df.columns or col not in qry_df.columns:\n                continue\n            ref_vals = ref_df[col].values\n            neigh_vals = ref_vals[indices]  # (N, k)\n            qry_vals = qry_df[col].values   # (N,)\n\n            same = (neigh_vals == qry_vals[:, None])\n            qry_df[f\"{prefix}_{col}_same_frac\"] = same.mean(axis=1)\n\n    return qry_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP ","metadata":{}},{"cell_type":"code","source":"IMG_COL=''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extracting features from CLIP","metadata":{}},{"cell_type":"code","source":"def extract_clip_image_embeddings(df, img_dir, img_col, clip_model, clip_processor, batch_size=32):\n    clip_model.eval()\n    all_emb = []\n    n = len(df)\n\n    for start in tqdm(range(0, n, batch_size), desc=\"Extracting CLIP embeddings\"):\n        end = min(start + batch_size, n)\n        batch_df = df.iloc[start:end]\n\n        images = []\n        for _, row in batch_df.iterrows():\n            img_name = row[img_col]\n            img_path = os.path.join(img_dir, str(img_name))\n            image = Image.open(img_path).convert(\"RGB\")\n            images.append(image)\n\n        inputs = clip_processor(images=images, return_tensors=\"pt\")\n        pixel_values = inputs[\"pixel_values\"].to(device)\n\n        with torch.no_grad():\n            outputs = clip_model.get_image_features(pixel_values=pixel_values)\n            # Нормализуем эмбеддинги (часто полезно)\n            outputs = F.normalize(outputs, p=2, dim=-1)\n\n        all_emb.append(outputs.cpu().numpy())\n\n    return np.concatenate(all_emb, axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_clip_emb = extract_clip_image_embeddings(train_data, train_img_dir, IMG_COL, clip_model, clip_processor, batch_size=BATCH_SIZE)\neval_clip_emb = extract_clip_image_embeddings(eval_data, train_img_dir, IMG_COL, clip_model, clip_processor, batch_size=BATCH_SIZE)\ntest_clip_emb = extract_clip_image_embeddings(test, test_img_dir, IMG_COL, clip_model, clip_processor, batch_size=BATCH_SIZE)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clip_cols = [f\"clip_emb_{i}\" for i in range(train_clip_emb.shape[1])]\n\ntrain_clip_df = pd.DataFrame(train_clip_emb, columns=clip_cols, index=train_data.index)\neval_clip_df  = pd.DataFrame(eval_clip_emb,  columns=clip_cols, index=eval_data.index)\ntest_clip_df  = pd.DataFrame(test_clip_emb,  columns=clip_cols, index=test.index)\n\ntrain_data = pd.concat([train_data, train_clip_df], axis=1)\neval_data  = pd.concat([eval_data,  eval_clip_df],  axis=1)\ntest       = pd.concat([test,       test_clip_df],  axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### for multi image","metadata":{}},{"cell_type":"code","source":"def extract_clip_embeddings_multi(clip_model, dataloader, n_objects: int):\n    clip_model.eval()\n    emb_sum = None\n    count = np.zeros(n_objects, dtype=np.int32)\n    device = next(clip_model.parameters()).device\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"CLIP embeddings (multi-image)\"):\n            X = batch['image'].to(device)\n            obj_idx = batch['obj_idx'].cpu().numpy()\n\n            use_amp = (device.type == \"cuda\")\n            with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=use_amp):\n                feats = clip_model.get_image_features(pixel_values=X)\n\n            feats = F.normalize(feats, p=2, dim=-1)\n            feats_np = feats.cpu().numpy()\n\n            if emb_sum is None:\n                emb_dim = feats_np.shape[1]\n                emb_sum = np.zeros((n_objects, emb_dim), dtype=np.float32)\n\n            for f, idx in zip(feats_np, obj_idx):\n                emb_sum[idx] += f\n                count[idx] += 1\n\n    count[count == 0] = 1\n    emb_avg = emb_sum / count[:, None]\n    return emb_avg\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ntrain_clip_emb = extract_clip_embeddings_multi(clip_model, clip_train_loader, n_train)\neval_clip_emb  = extract_clip_embeddings_multi(clip_model, clip_eval_loader,  n_eval)\ntest_clip_emb  = extract_clip_embeddings_multi(clip_model, clip_test_loader,  n_test)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Catboost over all feats ","metadata":{}},{"cell_type":"markdown","source":"## making all feats","metadata":{}},{"cell_type":"code","source":"K_NEIGHBORS = 10  # можно тюнить","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TARGET_COL in num_feats:\n    num_feats.remove(TARGET_COL)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ============================\n# kNN по vision-эмбеддингам\n# ============================\n\ntrain_data = add_knn_features(\n    ref_emb=train_vision_emb,\n    qry_emb=train_vision_emb,\n    ref_df=train_data,\n    qry_df=train_data,\n    numeric_cols=num_feats,      # числовые исходные фичи\n    target_col=TARGET_COL,\n    cat_cols=cat_feats,\n    n_neighbors=K_NEIGHBORS,\n    prefix=\"knn_vis\",\n    drop_self=True              # не считаем самого себя соседом\n)\n\neval_data = add_knn_features(\n    ref_emb=train_vision_emb,\n    qry_emb=eval_vision_emb,\n    ref_df=train_data,\n    qry_df=eval_data,\n    numeric_cols=num_feats,\n    target_col=TARGET_COL,\n    cat_cols=cat_feats,\n    n_neighbors=K_NEIGHBORS,\n    prefix=\"knn_vis\",\n    drop_self=False\n)\n\ntest = add_knn_features(\n    ref_emb=train_vision_emb,\n    qry_emb=test_vision_emb,\n    ref_df=train_data,\n    qry_df=test,\n    numeric_cols=num_feats,\n    target_col=TARGET_COL,\n    cat_cols=cat_feats,\n    n_neighbors=K_NEIGHBORS,\n    prefix=\"knn_vis\",\n    drop_self=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# kNN по CLIP-эмбеддингам\n# ============================\n\ntrain_data = add_knn_features(\n    ref_emb=train_clip_emb,\n    qry_emb=train_clip_emb,\n    ref_df=train_data,\n    qry_df=train_data,\n    numeric_cols=num_feats,\n    target_col=TARGET_COL,\n    cat_cols=cat_feats,\n    n_neighbors=K_NEIGHBORS,\n    prefix=\"knn_clip\",\n    drop_self=True\n)\n\neval_data = add_knn_features(\n    ref_emb=train_clip_emb,\n    qry_emb=eval_clip_emb,\n    ref_df=train_data,\n    qry_df=eval_data,\n    numeric_cols=num_feats,\n    target_col=TARGET_COL,\n    cat_cols=cat_feats,\n    n_neighbors=K_NEIGHBORS,\n    prefix=\"knn_clip\",\n    drop_self=False\n)\n\ntest = add_knn_features(\n    ref_emb=train_clip_emb,\n    qry_emb=test_clip_emb,\n    ref_df=train_data,\n    qry_df=test,\n    numeric_cols=num_feats,\n    target_col=TARGET_COL,\n    cat_cols=cat_feats,\n    n_neighbors=K_NEIGHBORS,\n    prefix=\"knn_clip\",\n    drop_self=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_train = pd.concat([train_data, eval_data], axis=0).reset_index(drop=True)\nfull_test = test.reset_index(drop=True)\n\nX_full = full_train.drop(columns=[TARGET_COL])\ny_full = full_train[TARGET_COL]\n\nX_test = full_test\n\ncat_features_full = [c for c in X_full.columns if X_full[c].dtype == 'object']\ncat_features_full_idx = [X_full.columns.get_loc(c) for c in cat_features_full]\n\nmodel_final = CatBoostRegressor(\n    iterations=1500,\n    depth=6,\n    learning_rate=0.05,\n    loss_function='RMSE',\n    eval_metric='RMSE',\n    l2_leaf_reg=3.0,\n    task_type='CPU',     # 'GPU' при наличии\n    random_seed=seed,\n    verbose=200\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_pool = Pool(X_full, y_full, cat_features=cat_features_full_idx)\ntest_pool = Pool(X_test, cat_features=cat_features_full_idx)\n\nmodel_final.fit(full_pool)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fi_pred = model_final.get_feature_importance(\n    full_pool,                    # Pool с X_full,y_full\n    type='PredictionValuesChange' # дефолт, но оставим явно\n)\n\nfi_df = pd.DataFrame({\n    \"feature\": X_full.columns,\n    \"importance\": fi_pred\n})\n\nfi_df = fi_df.sort_values(\"importance\", ascending=False).reset_index(drop=True)\nprint(fi_df.head(30))  # топ-30 в табличке\n\n# Визуализация топ-N признаков\nTOP_N = 30\n\nplt.figure(figsize=(8, 0.4 * TOP_N + 1))\nplt.barh(\n    y=fi_df[\"feature\"].head(TOP_N)[::-1],\n    width=fi_df[\"importance\"].head(TOP_N)[::-1]\n)\nplt.xlabel(\"Feature importance (PredictionValuesChange)\")\nplt.title(f\"Top {TOP_N} features: model_final\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# All submissions","metadata":{}},{"cell_type":"code","source":"test_pred = model_final.predict(test_pool)\nsample[TARGET_COL] = test_pred\nsample.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}